{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# magic to reload modules when they change\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initial Unoptimized code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import copy\n",
    "import torch\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "from tab_transformer_copy import TabTransformer_edit\n",
    "from sklearn.model_selection import train_test_split\n",
    "from pytorch_tabnet.tab_model import TabNetRegressor\n",
    "from tab_transformer_pytorch import TabTransformer, FTTransformer\n",
    "from helper_analysis import normalize_data, load_and_combine_data, format_array, load_and_normalize_data_tensor, denormalize_data, min_max_scale\n",
    "\n",
    "folder_path = \"your_output_folder_path\"  # Replace with your folder path with your data\n",
    "epochs_dl = 2500 # For deep learning models\n",
    "epochs_dl_tab = 20 # For TabNet model\n",
    "epochs_ml = 200 # For machine learning models\n",
    "max_files = 5  # Maximum number of files to process\n",
    "seed = 42\n",
    "scaling_type = \"minmax\"  # \"minmax\" or any other for z-score\n",
    "scheduler = 0  # 1 for step, 2 for plateau, 3 for cosine\n",
    "patience = 30\n",
    "\n",
    "# Define the ratio for the train-test split\n",
    "train_ratio = 0.9\n",
    "\n",
    "# Check if CUDA is available and set the device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n",
    "\n",
    "# Define your L1 regularization strength\n",
    "l1_lambda = 0.0001\n",
    "l2_lambda = 0.0001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([81922, 15]),\n",
       " torch.Size([81922, 15]),\n",
       " torch.Size([81922, 15]),\n",
       " torch.Size([81922, 15]),\n",
       " torch.Size([81922, 45]),\n",
       " torch.Size([81922, 45]))"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load and normalize the data\n",
    "E_tensor, E_return, E_return2 = load_and_normalize_data_tensor(folder_path, \"E_combo_output\", max_files=max_files)\n",
    "A_tensor, _, _ = load_and_normalize_data_tensor(folder_path, \"A_combo_output\", max_files=max_files)\n",
    "J_tensor, _, _ = load_and_normalize_data_tensor(folder_path, \"J_combo_output\", max_files=max_files)\n",
    "h_tensor, _, _ = load_and_normalize_data_tensor(folder_path, \"h_combo_output\", max_files=max_files)\n",
    "X_tensor, _, _ = load_and_normalize_data_tensor(folder_path, \"X_combo_output\", max_files=max_files)\n",
    "w_tensor, _, _ = load_and_normalize_data_tensor(folder_path, \"w_combo_output\", max_files=max_files)\n",
    "T_tensor, _, _ = load_and_normalize_data_tensor(folder_path, \"T_combo_output\", max_files=max_files)\n",
    "\n",
    "# After loading the data. Make a new data tensor of size (n_samples, n_features) taking A_tensor as reference\n",
    "# Starting from the 2nd index and moving by 3 beta (the angle) will be 0 and the rest will be 90 degrees (pi/2)\n",
    "\n",
    "\n",
    "E_tensor.shape, A_tensor.shape, J_tensor.shape, h_tensor.shape, X_tensor.shape, w_tensor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_idx = [0,1,3,4,6,7,9,10,12,13]\n",
    "beta_tensor = torch.zeros(A_tensor.shape)\n",
    "type_tensor = torch.zeros(A_tensor.shape)\n",
    "type_tensor[:, col_idx] = 1\n",
    "beta_tensor[:, col_idx] = np.pi/2\n",
    "# beta_tensor, _, _ = normalize_data(beta_tensor)\n",
    "# Reshape type_tensor to have a shape of (81922, 1)\n",
    "type_tensor = type_tensor[:, :1]\n",
    "\n",
    "# In case of missing values for non-damaged data\n",
    "# Create A_original_tensor of size (n_samples, n_features) with values [0.105, 0.16] with 0.16 for col_idx and 0.105 for the rest\n",
    "# A_original_tensor = torch.zeros(A_tensor.shape)\n",
    "# A_original_tensor[:, col_idx] = 0.16\n",
    "\n",
    "# Make an E tensor with the same shape as E_tensor but all the values are 21*10**7\n",
    "# E_original_tensor = torch.zeros(E_tensor.shape)\n",
    "# E_original_tensor[:, :] = 21*10**7\n",
    "# E_original_tensor, _, _ = normalize_data(E_original_tensor)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\kaido\\anaconda3\\envs\\ds\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 0.8042696118354797, Val Loss: 0.6857430338859558\n",
      "Loss amplifier: 0.0\n",
      "Epoch 10, Loss: 0.143502876162529, Val Loss: 0.1222248524427414\n",
      "Loss amplifier: 0.0\n",
      "Epoch 20, Loss: 0.10190340131521225, Val Loss: 0.10216639935970306\n",
      "Loss amplifier: 0.0\n",
      "Epoch 30, Loss: 0.09330277144908905, Val Loss: 0.09034019708633423\n",
      "Loss amplifier: 0.0\n",
      "Epoch 40, Loss: 0.08600609004497528, Val Loss: 0.08544498682022095\n",
      "Loss amplifier: 0.0\n",
      "Epoch 50, Loss: 0.0816267728805542, Val Loss: 0.0829775258898735\n",
      "Loss amplifier: 0.0\n",
      "Epoch 60, Loss: 0.07855284959077835, Val Loss: 0.0789579302072525\n",
      "Loss amplifier: 0.0\n",
      "Epoch 70, Loss: 0.07539109885692596, Val Loss: 0.0761089026927948\n",
      "Loss amplifier: 0.0\n",
      "Epoch 80, Loss: 0.07207562774419785, Val Loss: 0.07402638345956802\n",
      "Loss amplifier: 0.0\n",
      "Epoch 90, Loss: 0.06899531185626984, Val Loss: 0.07065395265817642\n",
      "Loss amplifier: 0.0\n",
      "[[ 22599472.  27885344.  10888944. ... -18686160.  32637440.  13456624.]\n",
      " [ 44503136.  -7371472.  18545680. ...  -7757824.  48815968.  10265424.]\n",
      " [ 89678872. -83456016.  -3645920. ...   6701812.   9075284.  11140144.]\n",
      " ...\n",
      " [-11573408.  27575568.   5187792. ...   7102996.   2740956.  16712656.]\n",
      " [-33274304.  66835616.  -4721632. ... -63590656.  46151872.  12776048.]\n",
      " [-85424528.  66522784.  12958544. ... -25549008.  35293168.   7711648.]]\n",
      "tensor([[ 0.8565,  0.8230,  0.9309,  ...,  0.7853,  0.7928,  0.9146],\n",
      "        [ 0.7174,  0.7135,  0.8822,  ...,  0.7159,  0.6901,  0.9348],\n",
      "        [ 0.4306,  0.5299,  1.0231,  ..., -0.0426, -0.0576,  0.9293],\n",
      "        ...,\n",
      "        [ 0.7401,  0.8249,  0.9671,  ..., -0.0451, -0.0174,  0.8939],\n",
      "        [ 0.5446,  0.5756,  1.0300,  ...,  0.7371,  0.7070,  0.9189],\n",
      "        [ 0.5424,  0.5776,  0.9177,  ...,  0.8289,  0.7759,  0.9510]]) 8193\n",
      "tensor([[1.8740e+08, 1.8211e+08, 1.9911e+08,  ..., 1.7619e+08, 1.7736e+08,\n",
      "         1.9654e+08],\n",
      "        [1.6550e+08, 1.6487e+08, 1.9145e+08,  ..., 1.6526e+08, 1.6118e+08,\n",
      "         1.9973e+08],\n",
      "        [1.2032e+08, 1.3596e+08, 2.1365e+08,  ..., 4.5798e+07, 4.3425e+07,\n",
      "         1.9886e+08],\n",
      "        ...,\n",
      "        [1.6907e+08, 1.8242e+08, 2.0481e+08,  ..., 4.5397e+07, 4.9759e+07,\n",
      "         1.9329e+08],\n",
      "        [1.3827e+08, 1.4316e+08, 2.1472e+08,  ..., 1.6859e+08, 1.6385e+08,\n",
      "         1.9722e+08],\n",
      "        [1.3792e+08, 1.4348e+08, 1.9704e+08,  ..., 1.8305e+08, 1.7471e+08,\n",
      "         2.0229e+08]]) 8193\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "\n",
    "model = TabTransformer_edit(\n",
    "    categories=(),\n",
    "    num_continuous=150,               # continuous features\n",
    "    dim=150,                          # dimension, paper set at 32\n",
    "    dim_head=16,                      # dimension of each head, paper set at 16\n",
    "    dim_out=15,                      # 15 due to the shape of E_data\n",
    "    depth=3,                         # depth, paper recommended 6 (9 worked better by .02%)\n",
    "    heads=15,                         # heads, paper recommends 8\n",
    "    attn_dropout=0.6,                # post-attention dropout\n",
    "    ff_dropout=0.6,                  # feed forward dropout\n",
    "    mlp_hidden_mults=(4,2),         # relative multiples of each hidden dimension of the last mlp to logits\n",
    "    mlp_act=nn.ReLU6(),               # activation for final mlp, defaults to relu, but could be anything else (selu, ELU, PReLU, Tanh, Sigmoid)\n",
    "    mlp_dropout=0.05,\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "# Split X_data and w_data into three separate sets of features\n",
    "X1_tensor = X_tensor[:, :15].clone().detach()\n",
    "X2_tensor = X_tensor[:, 15:30].clone().detach()\n",
    "X3_tensor = X_tensor[:, 30:].clone().detach()\n",
    "w1_tensor = w_tensor[:, :15].clone().detach()\n",
    "w2_tensor = w_tensor[:, 15:30].clone().detach()\n",
    "w3_tensor = w_tensor[:, 30:].clone().detach()\n",
    "\n",
    "# Concatenate the tensors along the second dimension (features) WITHOUT CATEGORY\n",
    "x_cont = torch.cat((A_tensor, J_tensor, h_tensor, beta_tensor, w1_tensor, X1_tensor, w2_tensor, X2_tensor, w3_tensor, X3_tensor), dim=1)\n",
    "x_categ = torch.empty((x_cont.shape[0], 0))\n",
    "\n",
    "# Split the data into a training set and a test set and get the indices\n",
    "x_cont_train, x_cont_test, E_train, E_test, train_indices, test_indices = train_test_split(\n",
    "    x_cont, E_tensor, range(E_tensor.shape[0]), train_size=train_ratio, random_state=seed\n",
    ")\n",
    "\n",
    "# Define a loss function and an optimizer\n",
    "loss_fn = nn.MSELoss()\n",
    "# loss_fn = nn.SmoothL1Loss()\n",
    "# loss_fn = nn.L1Loss()\n",
    "optimizer = torch.optim.Adam(model.parameters())\n",
    "\n",
    "# Define a scheduler\n",
    "if scheduler == 1:\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min')\n",
    "\n",
    "# Move the model to the device\n",
    "model = model.to(device)\n",
    "\n",
    "# Move the data to the device\n",
    "x_categ = x_categ.to(device) # This is the only tensor with categorical data\n",
    "x_cont_train = x_cont_train.to(device)\n",
    "x_cont_test = x_cont_test.to(device)\n",
    "E_train = E_train.to(device)\n",
    "E_test = E_test.to(device)\n",
    "\n",
    "# For plotting\n",
    "loss_values = []\n",
    "best_model = None\n",
    "best_val_loss = float('inf')\n",
    "loss_amplifier = .000\n",
    "epochs_since_improvement = 0\n",
    "past_losses = []\n",
    "\n",
    "for epoch in range(epochs_dl):  # Number of epochs\n",
    "    # Forward pass\n",
    "    pred_train = model(x_categ[:x_cont_train.shape[0]], x_cont_train)\n",
    "    loss = loss_fn(pred_train, E_train)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    (loss/2 + loss/2 * loss_amplifier).backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # Validation\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        pred_val = model(x_categ[:x_cont_test.shape[0]], x_cont_test)\n",
    "        val_loss = loss_fn(pred_val, E_test)\n",
    "\n",
    "    # Check if validation loss has improved\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        epochs_since_improvement = 0\n",
    "    else:\n",
    "        epochs_since_improvement += 1\n",
    "\n",
    "    # Amplify loss function based on validation loss trend\n",
    "    if epochs_since_improvement >= 10:\n",
    "        # If validation loss hasn't improved in 5 epochs, reward the model by decreasing the loss amplifier\n",
    "        loss_amplifier *= 0.99\n",
    "        epochs_since_improvement = 0  # Reset the counter\n",
    "    else:\n",
    "        if epochs_since_improvement >= patience:\n",
    "            print(f'Early stopping at epoch {epoch} due to no improvement in validation loss.')\n",
    "            break\n",
    "\n",
    "    # Add additional punishment if the past 10 losses are relatively the same\n",
    "    past_losses.append(val_loss.item())\n",
    "    if len(past_losses) > 10:\n",
    "        past_losses.pop(0)  # remove the oldest loss\n",
    "        if np.std(past_losses) < 0.0007:  # if the standard deviation of the past 10 losses is small\n",
    "            loss_amplifier *= 2  # increase the loss amplifier\n",
    "            past_losses = past_losses[1:]  # Give it a grace period by removing the oldest losses\n",
    "\n",
    "    # Update the scheduler\n",
    "    if scheduler:\n",
    "        scheduler.step(val_loss)\n",
    "\n",
    "    # If the validation loss is the best we've seen so far, save the model state\n",
    "    if val_loss <= best_val_loss:\n",
    "        best_model = copy.deepcopy(model.state_dict())\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    # Store the loss value\n",
    "    loss_values.append(loss.item())\n",
    "\n",
    "    # Print loss for every 10 epochs\n",
    "    if epoch % 10 == 0:\n",
    "        print(f'Epoch {epoch}, Loss: {loss.item()}, Val Loss: {val_loss.item()}')\n",
    "        print(f'Loss amplifier: {loss_amplifier}')\n",
    "\n",
    "# Load the best model state\n",
    "model.load_state_dict(best_model)\n",
    "\n",
    "# Test the model\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    pred_test = model(x_categ[:x_cont_test.shape[0]], x_cont_test)\n",
    "\n",
    "# Move the predictions back to the CPU for further processing\n",
    "pred_test = pred_test.to(\"cpu\")\n",
    "E_test = E_test.to(\"cpu\")\n",
    "\n",
    "# Denormalize the E_tensor\n",
    "E_test_denorm = denormalize_data(E_test, E_return, E_return2, scaling_type)\n",
    "E_test_denorm_np = E_test_denorm.numpy()\n",
    "# Denormalize the predictions\n",
    "pred_test_denorm = denormalize_data(pred_test, E_return, E_return2, scaling_type)\n",
    "pred_test_denorm_np = pred_test_denorm.numpy()\n",
    "\n",
    "# MAPE\n",
    "mape = np.mean(np.abs((E_test_denorm_np - pred_test_denorm_np) / np.abs(E_test_denorm_np))) * 100\n",
    "# MdAPE\n",
    "mdape = np.median(np.abs((E_test_denorm_np - pred_test_denorm_np) / np.abs(E_test_denorm_np))) * 100\n",
    "\n",
    "# Calculate the difference\n",
    "difference = E_test_denorm_np - pred_test_denorm_np\n",
    "\n",
    "\n",
    "print(difference)\n",
    "\n",
    "print(pred_test, len(pred_test))\n",
    "print(pred_test_denorm, len(pred_test_denorm))\n",
    "\n",
    "# Plot the loss values\n",
    "plt.plot(loss_values)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Difference:\n",
      "[23992736.0000, -12262768.0000, 1265728.0000, -877872.0000, 1215968.0000, 1284640.0000, 51806944.0000, -34119520.0000, 1314064.0000, 9868848.0000,\n",
      " -22643808.0000, 1538752.0000, 37335288.0000, -41384912.0000, 1502416.0000]\n",
      "\n",
      "E_test_denorm:\n",
      "[210000000.0000, 105000000.0000, 210000000.0000, 210000000.0000, 210000000.0000, 210000000.0000, 210000000.0000, 105000000.0000, 210000000.0000, 210000000.0000,\n",
      " 157500000.0000, 210000000.0000, 157500000.0000, 52500000.0000, 210000000.0000]\n",
      "\n",
      "Pred_test_denorm:\n",
      "[186007264.0000, 117262768.0000, 208734272.0000, 210877872.0000, 208784032.0000, 208715360.0000, 158193056.0000, 139119520.0000, 208685936.0000, 200131152.0000,\n",
      " 180143808.0000, 208461248.0000, 120164712.0000, 93884912.0000, 208497584.0000]\n",
      "\n",
      "\n",
      "MAPE: 10.4486%\n",
      "MdAPE: 4.031451418995857%\n"
     ]
    }
   ],
   "source": [
    "# All old results are based on 6 files\n",
    "print(\"Difference:\\n\" + format_array(difference[100], 4))\n",
    "print(\"\\nE_test_denorm:\\n\" + format_array(E_test_denorm_np[100], 4))\n",
    "print(\"\\nPred_test_denorm:\\n\" + format_array(pred_test_denorm_np[100], 4))\n",
    "print(f\"\\n\\nMAPE: {mape.item():.4f}%\")\n",
    "print(f'MdAPE: {mdape}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 0.00583114055916667, Val Loss: 0.22986441850662231\n",
      "Loss amplifier: 0.001\n",
      "Epoch 10, Loss: 0.04759063199162483, Val Loss: 0.056401368230581284\n",
      "Loss amplifier: 0.001\n",
      "Epoch 20, Loss: 0.03490660712122917, Val Loss: 0.049069005995988846\n",
      "Loss amplifier: 0.001\n",
      "Epoch 30, Loss: 0.026929644867777824, Val Loss: 0.041806843131780624\n",
      "Loss amplifier: 0.001\n",
      "Epoch 40, Loss: 0.02766173705458641, Val Loss: 0.036893486976623535\n",
      "Loss amplifier: 0.001\n",
      "Epoch 50, Loss: 0.0216047465801239, Val Loss: 0.03311355784535408\n",
      "Loss amplifier: 0.001\n",
      "Epoch 60, Loss: 0.01955840177834034, Val Loss: 0.030831433832645416\n",
      "Loss amplifier: 0.001\n",
      "Epoch 70, Loss: 0.018380824476480484, Val Loss: 0.025354374200105667\n",
      "Loss amplifier: 0.000995\n",
      "Epoch 80, Loss: 0.014692837372422218, Val Loss: 0.026153402402997017\n",
      "Loss amplifier: 0.000995\n",
      "Epoch 90, Loss: 0.013891144655644894, Val Loss: 0.0255893524736166\n",
      "Loss amplifier: 0.000990025\n",
      "Epoch 100, Loss: 0.014348939061164856, Val Loss: 0.03155336156487465\n",
      "Loss amplifier: 0.000985074875\n",
      "Epoch 110, Loss: 0.012903757393360138, Val Loss: 0.016065066680312157\n",
      "Loss amplifier: 0.000980149500625\n",
      "Epoch 120, Loss: 0.012924736365675926, Val Loss: 0.018890846520662308\n",
      "Loss amplifier: 0.000975248753121875\n",
      "Epoch 130, Loss: 0.011821243911981583, Val Loss: 0.02584567666053772\n",
      "Loss amplifier: 0.0009703725093562657\n",
      "Epoch 140, Loss: 0.011147061362862587, Val Loss: 0.019604645669460297\n",
      "Loss amplifier: 0.0009655206468094843\n",
      "Epoch 150, Loss: 0.009927111677825451, Val Loss: 0.016737613826990128\n",
      "Loss amplifier: 0.0009655206468094843\n",
      "Epoch 160, Loss: 0.009480206295847893, Val Loss: 0.017026562243700027\n",
      "Loss amplifier: 0.0009655206468094843\n",
      "Epoch 170, Loss: 0.009293239563703537, Val Loss: 0.013723968528211117\n",
      "Loss amplifier: 0.0009606930435754369\n",
      "Epoch 180, Loss: 0.00977393239736557, Val Loss: 0.01587490178644657\n",
      "Loss amplifier: 0.0009606930435754369\n",
      "Epoch 190, Loss: 0.008739778771996498, Val Loss: 0.017897605895996094\n",
      "Loss amplifier: 0.0009558895783575597\n",
      "Epoch 200, Loss: 0.007767901755869389, Val Loss: 0.00973123125731945\n",
      "Loss amplifier: 0.0009511101304657719\n",
      "Epoch 210, Loss: 0.009100370109081268, Val Loss: 0.013904969207942486\n",
      "Loss amplifier: 0.000946354579813443\n",
      "Epoch 220, Loss: 0.00803947914391756, Val Loss: 0.011044902727007866\n",
      "Loss amplifier: 0.0009416228069143757\n",
      "Epoch 230, Loss: 0.009429843164980412, Val Loss: 0.012897305190563202\n",
      "Loss amplifier: 0.0009369146928798038\n",
      "Epoch 240, Loss: 0.008886213414371014, Val Loss: 0.011546665802598\n",
      "Loss amplifier: 0.0009322301194154048\n",
      "Epoch 250, Loss: 0.00811510905623436, Val Loss: 0.011227067559957504\n",
      "Loss amplifier: 0.0009322301194154048\n",
      "Epoch 260, Loss: 0.00808772724121809, Val Loss: 0.015703730285167694\n",
      "Loss amplifier: 0.0009275689688183278\n",
      "Epoch 270, Loss: 0.007905729115009308, Val Loss: 0.009139198809862137\n",
      "Loss amplifier: 0.0009229311239742361\n",
      "Epoch 280, Loss: 0.007257066201418638, Val Loss: 0.0099830012768507\n",
      "Loss amplifier: 0.0009183164683543649\n",
      "Epoch 290, Loss: 0.008550375699996948, Val Loss: 0.011782941408455372\n",
      "Loss amplifier: 0.0009137248860125931\n",
      "Epoch 300, Loss: 0.007473060395568609, Val Loss: 0.010493520647287369\n",
      "Loss amplifier: 0.0009091562615825302\n",
      "Epoch 310, Loss: 0.008107059635221958, Val Loss: 0.009951431304216385\n",
      "Loss amplifier: 0.0009046104802746175\n",
      "Epoch 320, Loss: 0.007757101207971573, Val Loss: 0.007614850532263517\n",
      "Loss amplifier: 0.0009000874278732445\n",
      "Epoch 330, Loss: 0.008191686123609543, Val Loss: 0.011611024849116802\n",
      "Loss amplifier: 0.0008955869907338783\n",
      "Epoch 340, Loss: 0.0074485125951468945, Val Loss: 0.021579157561063766\n",
      "Loss amplifier: 0.0008911090557802089\n",
      "Epoch 350, Loss: 0.007722528651356697, Val Loss: 0.012477962300181389\n",
      "Loss amplifier: 0.0008866535105013078\n",
      "Epoch 360, Loss: 0.006692366674542427, Val Loss: 0.016625309363007545\n",
      "Loss amplifier: 0.0008822202429488013\n",
      "Epoch 370, Loss: 0.008735757321119308, Val Loss: 0.014479577541351318\n",
      "Loss amplifier: 0.0008778091417340573\n",
      "Epoch 380, Loss: 0.007557528093457222, Val Loss: 0.0137145621702075\n",
      "Loss amplifier: 0.000873420096025387\n",
      "Epoch 390, Loss: 0.007351686712354422, Val Loss: 0.010590271092951298\n",
      "Loss amplifier: 0.0008690529955452601\n",
      "Epoch 400, Loss: 0.00646617915481329, Val Loss: 0.008196092210710049\n",
      "Loss amplifier: 0.0008647077305675338\n",
      "Epoch 410, Loss: 0.007247596513479948, Val Loss: 0.00937683880329132\n",
      "Loss amplifier: 0.0008603841919146961\n",
      "Epoch 420, Loss: 0.007828094065189362, Val Loss: 0.009310806170105934\n",
      "Loss amplifier: 0.0008603841919146961\n",
      "Epoch 430, Loss: 0.007817047648131847, Val Loss: 0.02400587499141693\n",
      "Loss amplifier: 0.0008560822709551226\n",
      "Epoch 440, Loss: 0.007648745086044073, Val Loss: 0.020414277911186218\n",
      "Loss amplifier: 0.000851801859600347\n",
      "Epoch 450, Loss: 0.007305155973881483, Val Loss: 0.011715676635503769\n",
      "Loss amplifier: 0.0008475428503023452\n",
      "Epoch 460, Loss: 0.007423950359225273, Val Loss: 0.013633757829666138\n",
      "Loss amplifier: 0.0008433051360508335\n",
      "Epoch 470, Loss: 0.006922497879713774, Val Loss: 0.014200964011251926\n",
      "Loss amplifier: 0.0008390886103705794\n",
      "Epoch 480, Loss: 0.006537099834531546, Val Loss: 0.010924248956143856\n",
      "Loss amplifier: 0.0008348931673187264\n",
      "Epoch 490, Loss: 0.006998711731284857, Val Loss: 0.00920779723674059\n",
      "Loss amplifier: 0.0008307187014821328\n",
      "Epoch 500, Loss: 0.006815350614488125, Val Loss: 0.007602352183312178\n",
      "Loss amplifier: 0.0008265651079747222\n",
      "Epoch 510, Loss: 0.00658878730610013, Val Loss: 0.007691545877605677\n",
      "Loss amplifier: 0.0008224322824348485\n",
      "Epoch 520, Loss: 0.007255487609654665, Val Loss: 0.010212217457592487\n",
      "Loss amplifier: 0.0008183201210226743\n",
      "Epoch 530, Loss: 0.006399726960808039, Val Loss: 0.010784490033984184\n",
      "Loss amplifier: 0.0008142285204175609\n",
      "Epoch 540, Loss: 0.00795121118426323, Val Loss: 0.009653601795434952\n",
      "Loss amplifier: 0.0008101573778154731\n",
      "Epoch 550, Loss: 0.007381962612271309, Val Loss: 0.012178739532828331\n",
      "Loss amplifier: 0.0008061065909263957\n",
      "Epoch 560, Loss: 0.006801269482821226, Val Loss: 0.008316099643707275\n",
      "Loss amplifier: 0.0008020760579717638\n",
      "Epoch 570, Loss: 0.00680672749876976, Val Loss: 0.01275093574076891\n",
      "Loss amplifier: 0.000798065677681905\n",
      "Epoch 580, Loss: 0.008068891242146492, Val Loss: 0.011845163069665432\n",
      "Loss amplifier: 0.0007940753492934955\n",
      "Epoch 590, Loss: 0.006721182260662317, Val Loss: 0.009032249450683594\n",
      "Loss amplifier: 0.000790104972547028\n",
      "Epoch 600, Loss: 0.006312497891485691, Val Loss: 0.009890105575323105\n",
      "Loss amplifier: 0.0007861544476842928\n",
      "Epoch 610, Loss: 0.00895170122385025, Val Loss: 0.009034833870828152\n",
      "Loss amplifier: 0.0007822236754458713\n",
      "Epoch 620, Loss: 0.00630195951089263, Val Loss: 0.008262398652732372\n",
      "Loss amplifier: 0.0007783125570686419\n",
      "Epoch 630, Loss: 0.00618310272693634, Val Loss: 0.01138221938163042\n",
      "Loss amplifier: 0.0007744209942832988\n",
      "Epoch 640, Loss: 0.00769435940310359, Val Loss: 0.009894179180264473\n",
      "Loss amplifier: 0.0007705488893118823\n",
      "Epoch 650, Loss: 0.0061500221490859985, Val Loss: 0.009073461405932903\n",
      "Loss amplifier: 0.0007666961448653228\n",
      "Epoch 660, Loss: 0.006838880944997072, Val Loss: 0.01133927796036005\n",
      "Loss amplifier: 0.0007628626641409962\n",
      "Epoch 670, Loss: 0.0067055183462798595, Val Loss: 0.010091650299727917\n",
      "Loss amplifier: 0.0007590483508202912\n",
      "Epoch 680, Loss: 0.006445617880672216, Val Loss: 0.0099223917350173\n",
      "Loss amplifier: 0.0007552531090661898\n",
      "Epoch 690, Loss: 0.006915864069014788, Val Loss: 0.00928475707769394\n",
      "Loss amplifier: 0.0007514768435208588\n",
      "Epoch 700, Loss: 0.006159971002489328, Val Loss: 0.012001163326203823\n",
      "Loss amplifier: 0.0007477194593032545\n",
      "Epoch 710, Loss: 0.006236768793314695, Val Loss: 0.006896568462252617\n",
      "Loss amplifier: 0.0007439808620067382\n",
      "Epoch 720, Loss: 0.0064212726429104805, Val Loss: 0.00908819492906332\n",
      "Loss amplifier: 0.0007402609576967046\n",
      "Epoch 730, Loss: 0.006347146350890398, Val Loss: 0.010723608545958996\n",
      "Loss amplifier: 0.000736559652908221\n",
      "Epoch 740, Loss: 0.00641223369166255, Val Loss: 0.009087740443646908\n",
      "Loss amplifier: 0.0007328768546436799\n",
      "Epoch 750, Loss: 0.0066271210089325905, Val Loss: 0.009757008403539658\n",
      "Loss amplifier: 0.0007292124703704615\n",
      "Epoch 760, Loss: 0.007858356460928917, Val Loss: 0.007539934944361448\n",
      "Loss amplifier: 0.0007255664080186091\n",
      "Epoch 770, Loss: 0.007329849526286125, Val Loss: 0.00852314941585064\n",
      "Loss amplifier: 0.0007219385759785161\n",
      "Epoch 780, Loss: 0.00664457306265831, Val Loss: 0.013927634805440903\n",
      "Loss amplifier: 0.0007183288830986235\n",
      "Epoch 790, Loss: 0.006829041987657547, Val Loss: 0.011772220954298973\n",
      "Loss amplifier: 0.0007147372386831303\n",
      "Epoch 800, Loss: 0.006897206883877516, Val Loss: 0.010670594871044159\n",
      "Loss amplifier: 0.0007111635524897147\n",
      "Epoch 810, Loss: 0.00675703352317214, Val Loss: 0.008146543055772781\n",
      "Loss amplifier: 0.0007076077347272661\n",
      "Epoch 820, Loss: 0.005991602316498756, Val Loss: 0.009188054129481316\n",
      "Loss amplifier: 0.0007040696960536298\n",
      "Epoch 830, Loss: 0.008415134623646736, Val Loss: 0.011982259340584278\n",
      "Loss amplifier: 0.0007005493475733617\n",
      "Epoch 840, Loss: 0.006237109191715717, Val Loss: 0.009914368391036987\n",
      "Loss amplifier: 0.0006970466008354948\n",
      "Epoch 850, Loss: 0.006261378992348909, Val Loss: 0.0095486706122756\n",
      "Loss amplifier: 0.0006935613678313174\n",
      "Epoch 860, Loss: 0.006816065404564142, Val Loss: 0.007336682640016079\n",
      "Loss amplifier: 0.0006900935609921607\n",
      "Epoch 870, Loss: 0.007703103125095367, Val Loss: 0.007783128414303064\n",
      "Loss amplifier: 0.0006866430931872\n",
      "Epoch 880, Loss: 0.006297541316598654, Val Loss: 0.007604660000652075\n",
      "Loss amplifier: 0.000683209877721264\n",
      "Epoch 890, Loss: 0.00655567878857255, Val Loss: 0.0090944217517972\n",
      "Loss amplifier: 0.0006797938283326577\n",
      "Epoch 900, Loss: 0.006740217097103596, Val Loss: 0.009863735176622868\n",
      "Loss amplifier: 0.0006763948591909945\n",
      "Epoch 910, Loss: 0.006430715788155794, Val Loss: 0.007595312315970659\n",
      "Loss amplifier: 0.0006730128848950395\n",
      "Epoch 920, Loss: 0.0064759706147015095, Val Loss: 0.00992493610829115\n",
      "Loss amplifier: 0.0006696478204705643\n",
      "Epoch 930, Loss: 0.00694709038361907, Val Loss: 0.00993135292083025\n",
      "Loss amplifier: 0.0006662995813682115\n",
      "Epoch 940, Loss: 0.006851497106254101, Val Loss: 0.0077874548733234406\n",
      "Loss amplifier: 0.0006629680834613704\n",
      "Epoch 950, Loss: 0.007049164734780788, Val Loss: 0.012178584933280945\n",
      "Loss amplifier: 0.0006596532430440636\n",
      "Epoch 960, Loss: 0.006059639155864716, Val Loss: 0.00973867904394865\n",
      "Loss amplifier: 0.0006563549768288432\n",
      "Epoch 970, Loss: 0.007261858321726322, Val Loss: 0.011817524209618568\n",
      "Loss amplifier: 0.000653073201944699\n",
      "Epoch 980, Loss: 0.006154348142445087, Val Loss: 0.007294475566595793\n",
      "Loss amplifier: 0.0006498078359349755\n",
      "Epoch 990, Loss: 0.006107596214860678, Val Loss: 0.00807005725800991\n",
      "Loss amplifier: 0.0006465587967553006\n",
      "Epoch 1000, Loss: 0.00602710573002696, Val Loss: 0.010047487914562225\n",
      "Loss amplifier: 0.0006433260027715241\n",
      "Epoch 1010, Loss: 0.006698509678244591, Val Loss: 0.009929979220032692\n",
      "Loss amplifier: 0.0006401093727576665\n",
      "Epoch 1020, Loss: 0.0059612649492919445, Val Loss: 0.007558293640613556\n",
      "Loss amplifier: 0.0006369088258938781\n",
      "Epoch 1030, Loss: 0.005947215482592583, Val Loss: 0.008994507603347301\n",
      "Loss amplifier: 0.0006337242817644087\n",
      "Epoch 1040, Loss: 0.006408844608813524, Val Loss: 0.008136197924613953\n",
      "Loss amplifier: 0.0006305556603555866\n",
      "Epoch 1050, Loss: 0.006801160518079996, Val Loss: 0.008715584874153137\n",
      "Loss amplifier: 0.0006274028820538087\n",
      "Epoch 1060, Loss: 0.005848846398293972, Val Loss: 0.007073717657476664\n",
      "Loss amplifier: 0.0006242658676435396\n",
      "Epoch 1070, Loss: 0.006732073146849871, Val Loss: 0.009288963861763477\n",
      "Loss amplifier: 0.0006211445383053219\n",
      "Epoch 1080, Loss: 0.006778169889003038, Val Loss: 0.010498737916350365\n",
      "Loss amplifier: 0.0006180388156137953\n",
      "Epoch 1090, Loss: 0.005873335991054773, Val Loss: 0.012600552290678024\n",
      "Loss amplifier: 0.0006149486215357262\n",
      "Epoch 1100, Loss: 0.005794555880129337, Val Loss: 0.006670237518846989\n",
      "Loss amplifier: 0.0006118738784280476\n",
      "Epoch 1110, Loss: 0.006242028437554836, Val Loss: 0.008596380241215229\n",
      "Loss amplifier: 0.0006088145090359073\n",
      "Epoch 1120, Loss: 0.006172396708279848, Val Loss: 0.007333931978791952\n",
      "Loss amplifier: 0.0006057704364907278\n",
      "Epoch 1130, Loss: 0.0060153137892484665, Val Loss: 0.007639430928975344\n",
      "Loss amplifier: 0.0006027415843082742\n",
      "Epoch 1140, Loss: 0.007410061080008745, Val Loss: 0.008573910221457481\n",
      "Loss amplifier: 0.0005997278763867329\n",
      "Epoch 1150, Loss: 0.006244797259569168, Val Loss: 0.008350139483809471\n",
      "Loss amplifier: 0.0005967292370047993\n",
      "Epoch 1160, Loss: 0.006013378035277128, Val Loss: 0.007821407169103622\n",
      "Loss amplifier: 0.0005967292370047993\n",
      "Epoch 1170, Loss: 0.005868751090019941, Val Loss: 0.007833147421479225\n",
      "Loss amplifier: 0.0005937455908197753\n",
      "Epoch 1180, Loss: 0.0060129063203930855, Val Loss: 0.007776807993650436\n",
      "Loss amplifier: 0.0005907768628656764\n",
      "Epoch 1190, Loss: 0.006155033130198717, Val Loss: 0.012712419964373112\n",
      "Loss amplifier: 0.000587822978551348\n",
      "Epoch 1200, Loss: 0.006790874991565943, Val Loss: 0.009200566448271275\n",
      "Loss amplifier: 0.0005848838636585913\n",
      "Epoch 1210, Loss: 0.005945935845375061, Val Loss: 0.00846755038946867\n",
      "Loss amplifier: 0.0005848838636585913\n",
      "Epoch 1220, Loss: 0.0061234962195158005, Val Loss: 0.008468384854495525\n",
      "Loss amplifier: 0.0005848838636585913\n",
      "Epoch 1230, Loss: 0.005599829368293285, Val Loss: 0.008723641745746136\n",
      "Loss amplifier: 0.0005819594443402983\n",
      "Epoch 1240, Loss: 0.006215529516339302, Val Loss: 0.010959126986563206\n",
      "Loss amplifier: 0.0005790496471185969\n",
      "Epoch 1250, Loss: 0.007038017734885216, Val Loss: 0.02028227597475052\n",
      "Loss amplifier: 0.0005761543988830039\n",
      "Epoch 1260, Loss: 0.006629679352045059, Val Loss: 0.008233501575887203\n",
      "Loss amplifier: 0.0005732736268885889\n",
      "Epoch 1270, Loss: 0.006608650088310242, Val Loss: 0.00922574382275343\n",
      "Loss amplifier: 0.0005704072587541459\n",
      "Epoch 1280, Loss: 0.005861729849129915, Val Loss: 0.009192832745611668\n",
      "Loss amplifier: 0.0005675552224603752\n",
      "Epoch 1290, Loss: 0.006318145431578159, Val Loss: 0.008797070011496544\n",
      "Loss amplifier: 0.0005647174463480733\n",
      "Epoch 1300, Loss: 0.006537056528031826, Val Loss: 0.006642673164606094\n",
      "Loss amplifier: 0.0005618938591163329\n",
      "Epoch 1310, Loss: 0.005801997613161802, Val Loss: 0.00715087354183197\n",
      "Loss amplifier: 0.0005590843898207513\n",
      "Epoch 1320, Loss: 0.005959982052445412, Val Loss: 0.0073187523521482944\n",
      "Loss amplifier: 0.0005562889678716475\n",
      "Epoch 1330, Loss: 0.006345015484839678, Val Loss: 0.00816060695797205\n",
      "Loss amplifier: 0.0005535075230322892\n",
      "Epoch 1340, Loss: 0.005775050260126591, Val Loss: 0.008384759537875652\n",
      "Loss amplifier: 0.0005507399854171277\n",
      "Epoch 1350, Loss: 0.005737961735576391, Val Loss: 0.008854353800415993\n",
      "Loss amplifier: 0.0005479862854900421\n",
      "Epoch 1360, Loss: 0.007161653600633144, Val Loss: 0.011087195947766304\n",
      "Loss amplifier: 0.0005452463540625918\n",
      "Epoch 1370, Loss: 0.006573328282684088, Val Loss: 0.01172549370676279\n",
      "Loss amplifier: 0.0005425201222922788\n",
      "Epoch 1380, Loss: 0.005952679552137852, Val Loss: 0.012587814591825008\n",
      "Loss amplifier: 0.0005398075216808175\n",
      "Epoch 1390, Loss: 0.006052080076187849, Val Loss: 0.012624154798686504\n",
      "Loss amplifier: 0.0005371084840724133\n",
      "Epoch 1400, Loss: 0.006249167490750551, Val Loss: 0.01034831628203392\n",
      "Loss amplifier: 0.0005344229416520513\n",
      "Epoch 1410, Loss: 0.005842898972332478, Val Loss: 0.00772473867982626\n",
      "Loss amplifier: 0.000531750826943791\n",
      "Epoch 1420, Loss: 0.006005024071782827, Val Loss: 0.008326748386025429\n",
      "Loss amplifier: 0.000529092072809072\n",
      "Epoch 1430, Loss: 0.005411593243479729, Val Loss: 0.007531745359301567\n",
      "Loss amplifier: 0.0005264466124450266\n",
      "Epoch 1440, Loss: 0.007668179459869862, Val Loss: 0.008272772654891014\n",
      "Loss amplifier: 0.0005238143793828015\n",
      "Epoch 1450, Loss: 0.005766323301941156, Val Loss: 0.010858001187443733\n",
      "Loss amplifier: 0.0005211953074858875\n",
      "Epoch 1460, Loss: 0.005761335138231516, Val Loss: 0.008477341383695602\n",
      "Loss amplifier: 0.0005185893309484581\n",
      "Epoch 1470, Loss: 0.005513983778655529, Val Loss: 0.006257335189729929\n",
      "Loss amplifier: 0.0005159963842937158\n",
      "Epoch 1480, Loss: 0.00614407192915678, Val Loss: 0.010225759819149971\n",
      "Loss amplifier: 0.0005134164023722472\n",
      "Epoch 1490, Loss: 0.0062513831071555614, Val Loss: 0.009511703625321388\n",
      "Loss amplifier: 0.000510849320360386\n",
      "Epoch 1500, Loss: 0.006703468039631844, Val Loss: 0.010629897937178612\n",
      "Loss amplifier: 0.0005082950737585841\n",
      "Epoch 1510, Loss: 0.005386346951127052, Val Loss: 0.00956861861050129\n",
      "Loss amplifier: 0.0005057535983897911\n",
      "Epoch 1520, Loss: 0.006227112375199795, Val Loss: 0.00905687641352415\n",
      "Loss amplifier: 0.0005032248303978422\n",
      "Epoch 1530, Loss: 0.0064931707456707954, Val Loss: 0.008767613209784031\n",
      "Loss amplifier: 0.000500708706245853\n",
      "Epoch 1540, Loss: 0.006979560013860464, Val Loss: 0.009234096854925156\n",
      "Loss amplifier: 0.0004982051627146237\n",
      "Epoch 1550, Loss: 0.005949873942881823, Val Loss: 0.009842378087341785\n",
      "Loss amplifier: 0.0004957141369010506\n",
      "Epoch 1560, Loss: 0.005857400130480528, Val Loss: 0.019164888188242912\n",
      "Loss amplifier: 0.0004932355662165453\n",
      "Epoch 1570, Loss: 0.005394041538238525, Val Loss: 0.00804527010768652\n",
      "Loss amplifier: 0.0004907693883854625\n",
      "Epoch 1580, Loss: 0.006051583681255579, Val Loss: 0.012856057845056057\n",
      "Loss amplifier: 0.0004883155414435352\n",
      "Epoch 1590, Loss: 0.006016444880515337, Val Loss: 0.008881446905434132\n",
      "Loss amplifier: 0.00048587396373631753\n",
      "Epoch 1600, Loss: 0.005531666334718466, Val Loss: 0.009668879210948944\n",
      "Loss amplifier: 0.00048344459391763597\n",
      "Epoch 1610, Loss: 0.006217638496309519, Val Loss: 0.006865071598440409\n",
      "Loss amplifier: 0.0004810273709480478\n",
      "Epoch 1620, Loss: 0.006204270292073488, Val Loss: 0.009463358670473099\n",
      "Loss amplifier: 0.00047862223409330756\n",
      "Epoch 1630, Loss: 0.006118236109614372, Val Loss: 0.00622680876404047\n",
      "Loss amplifier: 0.000476229122922841\n",
      "Epoch 1640, Loss: 0.005397159606218338, Val Loss: 0.008602003566920757\n",
      "Loss amplifier: 0.0004738479773082268\n",
      "Epoch 1650, Loss: 0.005943526513874531, Val Loss: 0.008115514181554317\n",
      "Loss amplifier: 0.0004714787374216857\n",
      "Epoch 1660, Loss: 0.005778266116976738, Val Loss: 0.020572330802679062\n",
      "Loss amplifier: 0.00046912134373457723\n",
      "Epoch 1670, Loss: 0.0056862304918468, Val Loss: 0.0077363550662994385\n",
      "Loss amplifier: 0.00046677573701590436\n",
      "Epoch 1680, Loss: 0.005836023483425379, Val Loss: 0.008989442139863968\n",
      "Loss amplifier: 0.0004644418583308248\n",
      "Epoch 1690, Loss: 0.005328861065208912, Val Loss: 0.0068113901652395725\n",
      "Loss amplifier: 0.0004621196490391707\n",
      "Epoch 1700, Loss: 0.005715860519558191, Val Loss: 0.01121235266327858\n",
      "Loss amplifier: 0.00045980905079397486\n",
      "Epoch 1710, Loss: 0.006796735804527998, Val Loss: 0.00864084716886282\n",
      "Loss amplifier: 0.000457510005540005\n",
      "Epoch 1720, Loss: 0.005654746200889349, Val Loss: 0.007838084362447262\n",
      "Loss amplifier: 0.00045522245551230493\n",
      "Epoch 1730, Loss: 0.005480815656483173, Val Loss: 0.010844227857887745\n",
      "Loss amplifier: 0.0004529463432347434\n",
      "Epoch 1740, Loss: 0.005406285170465708, Val Loss: 0.006258002482354641\n",
      "Loss amplifier: 0.00045068161151856965\n",
      "Epoch 1750, Loss: 0.0056226905435323715, Val Loss: 0.008154724724590778\n",
      "Loss amplifier: 0.0004484282034609768\n",
      "Epoch 1760, Loss: 0.00506257638335228, Val Loss: 0.008139878511428833\n",
      "Loss amplifier: 0.0004461860624436719\n",
      "Epoch 1770, Loss: 0.005912823136895895, Val Loss: 0.008954828605055809\n",
      "Loss amplifier: 0.00044395513213145357\n",
      "Epoch 1780, Loss: 0.005702975671738386, Val Loss: 0.006843541748821735\n",
      "Loss amplifier: 0.0004417353564707963\n",
      "Epoch 1790, Loss: 0.005743978079408407, Val Loss: 0.007643911521881819\n",
      "Loss amplifier: 0.00043952667968844234\n",
      "Epoch 1800, Loss: 0.00569124473258853, Val Loss: 0.011254621669650078\n",
      "Loss amplifier: 0.0004373290462900001\n",
      "Epoch 1810, Loss: 0.005620336160063744, Val Loss: 0.009169193916022778\n",
      "Loss amplifier: 0.0004351424010585501\n",
      "Epoch 1820, Loss: 0.006189089268445969, Val Loss: 0.007723524700850248\n",
      "Loss amplifier: 0.00043296668905325734\n",
      "Epoch 1830, Loss: 0.0054114325903356075, Val Loss: 0.007155291270464659\n",
      "Loss amplifier: 0.00043080185560799106\n",
      "Epoch 1840, Loss: 0.006936151534318924, Val Loss: 0.012461148202419281\n",
      "Loss amplifier: 0.0004286478463299511\n",
      "Epoch 1850, Loss: 0.006371696945279837, Val Loss: 0.010098104365170002\n",
      "Loss amplifier: 0.00042650460709830134\n",
      "Epoch 1860, Loss: 0.006694241426885128, Val Loss: 0.009681684896349907\n",
      "Loss amplifier: 0.00042437208406280984\n",
      "Epoch 1870, Loss: 0.005236062221229076, Val Loss: 0.007021492812782526\n",
      "Loss amplifier: 0.0004222502236424958\n",
      "Epoch 1880, Loss: 0.005258191376924515, Val Loss: 0.008557884953916073\n",
      "Loss amplifier: 0.0004201389725242833\n",
      "Epoch 1890, Loss: 0.005512662697583437, Val Loss: 0.010562525130808353\n",
      "Loss amplifier: 0.00041803827766166186\n",
      "Epoch 1900, Loss: 0.006073256488889456, Val Loss: 0.011630572378635406\n",
      "Loss amplifier: 0.00041594808627335356\n",
      "Epoch 1910, Loss: 0.005434889812022448, Val Loss: 0.0069375550374388695\n",
      "Loss amplifier: 0.0004138683458419868\n",
      "Epoch 1920, Loss: 0.0057436744682490826, Val Loss: 0.00812512170523405\n",
      "Loss amplifier: 0.00041179900411277687\n",
      "Epoch 1930, Loss: 0.005425291135907173, Val Loss: 0.009576261974871159\n",
      "Loss amplifier: 0.000409740009092213\n",
      "Epoch 1940, Loss: 0.005745165515691042, Val Loss: 0.008731074631214142\n",
      "Loss amplifier: 0.00040769130904675196\n",
      "Epoch 1950, Loss: 0.005326887592673302, Val Loss: 0.00797631498426199\n",
      "Loss amplifier: 0.0004056528525015182\n",
      "Epoch 1960, Loss: 0.004930507857352495, Val Loss: 0.005990416277199984\n",
      "Loss amplifier: 0.0004036245882390106\n",
      "Epoch 1970, Loss: 0.006812248844653368, Val Loss: 0.012077763676643372\n",
      "Loss amplifier: 0.00040160646529781557\n",
      "Epoch 1980, Loss: 0.005653396714478731, Val Loss: 0.009623875841498375\n",
      "Loss amplifier: 0.00039959843297132647\n",
      "Epoch 1990, Loss: 0.006869876757264137, Val Loss: 0.008158124051988125\n",
      "Loss amplifier: 0.00039760044080646985\n",
      "Epoch 2000, Loss: 0.005654526874423027, Val Loss: 0.007513917051255703\n",
      "Loss amplifier: 0.0003956124386024375\n",
      "Epoch 2010, Loss: 0.006006020586937666, Val Loss: 0.007667483761906624\n",
      "Loss amplifier: 0.0003956124386024375\n",
      "Epoch 2020, Loss: 0.005210732575505972, Val Loss: 0.007619979325681925\n",
      "Loss amplifier: 0.0003936343764094253\n",
      "Epoch 2030, Loss: 0.005232714582234621, Val Loss: 0.007265241350978613\n",
      "Loss amplifier: 0.00039166620452737815\n",
      "Epoch 2040, Loss: 0.005837258882820606, Val Loss: 0.010121856816112995\n",
      "Loss amplifier: 0.00038970787350474124\n",
      "Epoch 2050, Loss: 0.005397498141974211, Val Loss: 0.009056748822331429\n",
      "Loss amplifier: 0.0003877593341372175\n",
      "Epoch 2060, Loss: 0.006774724926799536, Val Loss: 0.008566910400986671\n",
      "Loss amplifier: 0.00038582053746653145\n",
      "Epoch 2070, Loss: 0.005541427992284298, Val Loss: 0.009427772834897041\n",
      "Loss amplifier: 0.0003838914347791988\n",
      "Epoch 2080, Loss: 0.005357714369893074, Val Loss: 0.007923005148768425\n",
      "Loss amplifier: 0.0003819719776053028\n",
      "Epoch 2090, Loss: 0.005840375553816557, Val Loss: 0.012830497696995735\n",
      "Loss amplifier: 0.00038006211771727627\n",
      "Epoch 2100, Loss: 0.0055524250492453575, Val Loss: 0.007624438963830471\n",
      "Loss amplifier: 0.0003781618071286899\n",
      "Epoch 2110, Loss: 0.005822556093335152, Val Loss: 0.010476846247911453\n",
      "Loss amplifier: 0.00037627099809304647\n",
      "Epoch 2120, Loss: 0.005663120187819004, Val Loss: 0.009592827409505844\n",
      "Loss amplifier: 0.00037438964310258126\n",
      "Epoch 2130, Loss: 0.005236111581325531, Val Loss: 0.006044275593012571\n",
      "Loss amplifier: 0.00037251769488706835\n",
      "Epoch 2140, Loss: 0.005177284590899944, Val Loss: 0.0074182492680847645\n",
      "Loss amplifier: 0.000370655106412633\n",
      "Epoch 2150, Loss: 0.0055673192255198956, Val Loss: 0.007190694101154804\n",
      "Loss amplifier: 0.00036880183088056984\n",
      "Epoch 2160, Loss: 0.005975331645458937, Val Loss: 0.007175539154559374\n",
      "Loss amplifier: 0.000366957821726167\n",
      "Epoch 2170, Loss: 0.005740697495639324, Val Loss: 0.00889358576387167\n",
      "Loss amplifier: 0.00036512303261753613\n",
      "Epoch 2180, Loss: 0.0065628052689135075, Val Loss: 0.012791172601282597\n",
      "Loss amplifier: 0.00036329741745444845\n",
      "Epoch 2190, Loss: 0.005277963820844889, Val Loss: 0.007829941809177399\n",
      "Loss amplifier: 0.0003614809303671762\n",
      "Epoch 2200, Loss: 0.005951907020062208, Val Loss: 0.008779514580965042\n",
      "Loss amplifier: 0.0003596735257153403\n",
      "Epoch 2210, Loss: 0.0048310658894479275, Val Loss: 0.008356614969670773\n",
      "Loss amplifier: 0.00035787515808676363\n",
      "Epoch 2220, Loss: 0.005345780868083239, Val Loss: 0.006749211810529232\n",
      "Loss amplifier: 0.00035608578229632984\n",
      "Epoch 2230, Loss: 0.0052704173140227795, Val Loss: 0.006693897768855095\n",
      "Loss amplifier: 0.0003543053533848482\n",
      "Epoch 2240, Loss: 0.005241971928626299, Val Loss: 0.010030526667833328\n",
      "Loss amplifier: 0.00035253382661792394\n",
      "Epoch 2250, Loss: 0.005499295424669981, Val Loss: 0.007371818646788597\n",
      "Loss amplifier: 0.0003507711574848343\n",
      "Epoch 2260, Loss: 0.005388838704675436, Val Loss: 0.009006102569401264\n",
      "Loss amplifier: 0.00034901730169741013\n",
      "Epoch 2270, Loss: 0.004944723565131426, Val Loss: 0.008343694731593132\n",
      "Loss amplifier: 0.0003472722151889231\n",
      "Epoch 2280, Loss: 0.006245551630854607, Val Loss: 0.00936136208474636\n",
      "Loss amplifier: 0.0003455358541129785\n",
      "Epoch 2290, Loss: 0.005495443008840084, Val Loss: 0.00788712128996849\n",
      "Loss amplifier: 0.0003438081748424136\n",
      "Epoch 2300, Loss: 0.005210438743233681, Val Loss: 0.008112781681120396\n",
      "Loss amplifier: 0.0003420891339682015\n",
      "Epoch 2310, Loss: 0.005332624074071646, Val Loss: 0.007505844347178936\n",
      "Loss amplifier: 0.0003420891339682015\n",
      "Epoch 2320, Loss: 0.005002031568437815, Val Loss: 0.00749107776209712\n",
      "Loss amplifier: 0.0003403786882983605\n",
      "Epoch 2330, Loss: 0.005592642817646265, Val Loss: 0.043022532016038895\n",
      "Loss amplifier: 0.0003386767948568687\n",
      "Epoch 2340, Loss: 0.005072042346000671, Val Loss: 0.008108164183795452\n",
      "Loss amplifier: 0.00033698341088258437\n",
      "Epoch 2350, Loss: 0.005663648713380098, Val Loss: 0.010832993313670158\n",
      "Loss amplifier: 0.00033529849382817143\n",
      "Epoch 2360, Loss: 0.005154064856469631, Val Loss: 0.00979908648878336\n",
      "Loss amplifier: 0.00033362200135903056\n",
      "Epoch 2370, Loss: 0.005441918503493071, Val Loss: 0.011093764565885067\n",
      "Loss amplifier: 0.0003319538913522354\n",
      "Epoch 2380, Loss: 0.005630998406559229, Val Loss: 0.007529878988862038\n",
      "Loss amplifier: 0.00033029412189547426\n",
      "Epoch 2390, Loss: 0.005042691249400377, Val Loss: 0.008687998168170452\n",
      "Loss amplifier: 0.0003286426512859969\n",
      "Epoch 2400, Loss: 0.00567545834928751, Val Loss: 0.00707855261862278\n",
      "Loss amplifier: 0.0003269994380295669\n",
      "Epoch 2410, Loss: 0.005194529891014099, Val Loss: 0.010117006488144398\n",
      "Loss amplifier: 0.0003253644408394191\n",
      "Epoch 2420, Loss: 0.006082396954298019, Val Loss: 0.010434877127408981\n",
      "Loss amplifier: 0.000323737618635222\n",
      "Epoch 2430, Loss: 0.005386778153479099, Val Loss: 0.007715357933193445\n",
      "Loss amplifier: 0.00032211893054204585\n",
      "Epoch 2440, Loss: 0.005218280479311943, Val Loss: 0.00825129821896553\n",
      "Loss amplifier: 0.0003205083358893356\n",
      "Epoch 2450, Loss: 0.005142478737980127, Val Loss: 0.007095614913851023\n",
      "Loss amplifier: 0.0003189057942098889\n",
      "Epoch 2460, Loss: 0.005392690189182758, Val Loss: 0.007069094106554985\n",
      "Loss amplifier: 0.00031731126523883944\n",
      "Epoch 2470, Loss: 0.00541852368041873, Val Loss: 0.007676390465348959\n",
      "Loss amplifier: 0.00031572470891264525\n",
      "Epoch 2480, Loss: 0.0059018624015152454, Val Loss: 0.007356297690421343\n",
      "Loss amplifier: 0.000314146085368082\n",
      "Epoch 2490, Loss: 0.0054122768342494965, Val Loss: 0.01638624258339405\n",
      "Loss amplifier: 0.0003125753549412416\n",
      "[[-3.5617760e+06  7.9895840e+06  4.2720000e+03 ... -9.3188320e+06\n",
      "   2.2222128e+07 -3.4128000e+04]\n",
      " [ 1.3497232e+07 -3.7001120e+06  4.0800000e+03 ...  4.4806080e+06\n",
      "   2.4742176e+07 -3.0480000e+04]\n",
      " [-7.5017600e+06  6.6645760e+06 -1.1008000e+04 ...  4.3892800e+06\n",
      "  -7.3731600e+06 -4.7072000e+04]\n",
      " ...\n",
      " [-1.5820784e+07  1.4522192e+07  3.9680000e+03 ...  3.6838600e+06\n",
      "  -1.9888080e+06 -2.9744000e+04]\n",
      " [-2.3219200e+06 -4.9590560e+06  6.1440000e+03 ...  8.1996800e+05\n",
      "   5.7090720e+06 -4.8144000e+04]\n",
      " [-5.0451680e+06  6.8581600e+06  6.2400000e+03 ... -1.8671536e+07\n",
      "   2.9711904e+07 -1.7888000e+04]]\n",
      "tensor([[ 1.0226,  0.9493,  1.0000,  ...,  0.7258,  0.8589,  1.0002],\n",
      "        [ 0.9143,  0.6902,  1.0000,  ...,  0.6382,  0.8429,  1.0002],\n",
      "        [ 1.0476, -0.0423,  1.0001,  ..., -0.0279,  0.0468,  1.0003],\n",
      "        ...,\n",
      "        [ 0.7671,  0.9078,  1.0000,  ..., -0.0234,  0.0126,  1.0002],\n",
      "        [ 0.3481,  1.0315,  1.0000,  ...,  0.3281,  0.9638,  1.0003],\n",
      "        [ 0.0320,  0.9565,  1.0000,  ...,  0.7852,  0.8114,  1.0001]]) 8193\n",
      "tensor([[2.1356e+08, 2.0201e+08, 2.1000e+08,  ..., 1.6682e+08, 1.8778e+08,\n",
      "         2.1003e+08],\n",
      "        [1.9650e+08, 1.6120e+08, 2.1000e+08,  ..., 1.5302e+08, 1.8526e+08,\n",
      "         2.1003e+08],\n",
      "        [2.1750e+08, 4.5835e+07, 2.1001e+08,  ..., 4.8111e+07, 5.9873e+07,\n",
      "         2.1005e+08],\n",
      "        ...,\n",
      "        [1.7332e+08, 1.9548e+08, 2.1000e+08,  ..., 4.8816e+07, 5.4489e+07,\n",
      "         2.1003e+08],\n",
      "        [1.0732e+08, 2.1496e+08, 2.0999e+08,  ..., 1.0418e+08, 2.0429e+08,\n",
      "         2.1005e+08],\n",
      "        [5.7545e+07, 2.0314e+08, 2.0999e+08,  ..., 1.7617e+08, 1.8029e+08,\n",
      "         2.1002e+08]]) 8193\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create a new model\n",
    "new_model = TabTransformer_edit(\n",
    "    categories=(),\n",
    "    num_continuous=150,               # continuous features\n",
    "    dim=150,                          # dimension, paper set at 32\n",
    "    dim_out=15,                      # 15 due to the shape of E_data\n",
    "    depth=3,                         # depth, paper recommended 6 (9 worked better by .02%)\n",
    "    heads=15,                         # heads, paper recommends 8\n",
    "    attn_dropout=0.1,                # post-attention dropout\n",
    "    ff_dropout=0.1,                  # feed forward dropout\n",
    "    mlp_hidden_mults=(4,2),         # relative multiples of each hidden dimension of the last mlp to logits\n",
    "    mlp_act=nn.ReLU6(),               # activation for final mlp, defaults to relu, but could be anything else (selu, ELU, PReLU, Tanh, Sigmoid)\n",
    "    mlp_dropout=0.05,\n",
    ")\n",
    "\n",
    "# Load the saved state dictionary into the new model\n",
    "new_model.load_state_dict(torch.load('model_weights_withfreq_changedscales3.pth'))\n",
    "\n",
    "# Move the new model to the device\n",
    "new_model = new_model.to(device)\n",
    "\n",
    "# Define a new optimizer for the new model\n",
    "new_optimizer = torch.optim.Adam(new_model.parameters())\n",
    "\n",
    "# Define a new scheduler for the new optimizer\n",
    "if scheduler == 1:\n",
    "    new_scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(new_optimizer, 'min')\n",
    "\n",
    "# Replace the old model with the new model in your existing code\n",
    "model = new_model\n",
    "optimizer = new_optimizer\n",
    "scheduler = new_scheduler if scheduler == 1 else None\n",
    "\n",
    "# Move the data to the device\n",
    "x_categ = x_categ.to(device) # This is the only tensor with categorical data\n",
    "x_cont_train = x_cont_train.to(device)\n",
    "x_cont_test = x_cont_test.to(device)\n",
    "E_train = E_train.to(device)\n",
    "E_test = E_test.to(device)\n",
    "\n",
    "# Now you can retrain the new model using your existing training loop\n",
    "\n",
    "# For plotting\n",
    "loss_values = []\n",
    "best_model = None\n",
    "best_val_loss = float('inf')\n",
    "loss_amplifier = 0.001\n",
    "epochs_since_improvement = 0\n",
    "past_losses = []\n",
    "\n",
    "for epoch in range(epochs_dl):  # Number of epochs\n",
    "    # Forward pass\n",
    "    pred_train = model(x_categ[:x_cont_train.shape[0]], x_cont_train)\n",
    "    loss = loss_fn(pred_train, E_train)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    (loss/2 + loss/2 * loss_amplifier).backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # Validation\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        pred_val = model(x_categ[:x_cont_test.shape[0]], x_cont_test)\n",
    "        val_loss = loss_fn(pred_val, E_test)\n",
    "\n",
    "    # Check if validation loss has improved\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        epochs_since_improvement = 0\n",
    "    else:\n",
    "        epochs_since_improvement += 1\n",
    "\n",
    "    # Amplify loss function based on validation loss trend\n",
    "    if epochs_since_improvement >= 10:\n",
    "        # If validation loss hasn't improved in 5 epochs, reward the model by decreasing the loss amplifier\n",
    "        loss_amplifier *= 0.995\n",
    "        epochs_since_improvement = 0  # Reset the counter\n",
    "    else:\n",
    "        if epochs_since_improvement >= patience:\n",
    "            print(f'Early stopping at epoch {epoch} due to no improvement in validation loss.')\n",
    "            break\n",
    "\n",
    "    # Add additional punishment if the past 10 losses are relatively the same\n",
    "    past_losses.append(val_loss.item())\n",
    "    if len(past_losses) > 10:\n",
    "        past_losses.pop(0)  # remove the oldest loss\n",
    "        if np.std(past_losses) < 0.0007:  # if the standard deviation of the past 10 losses is small\n",
    "            # loss_amplifier *= 1.05  # increase the loss amplifier\n",
    "            past_losses = past_losses[1:]  # Give it a grace period by removing the oldest losses\n",
    "\n",
    "    # Update the scheduler\n",
    "    if scheduler:\n",
    "        scheduler.step(val_loss)\n",
    "\n",
    "    # If the validation loss is the best we've seen so far, save the model state\n",
    "    if val_loss <= best_val_loss:\n",
    "        best_model = copy.deepcopy(model.state_dict())\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    # Store the loss value\n",
    "    loss_values.append(loss.item())\n",
    "\n",
    "    # Print loss for every 10 epochs\n",
    "    if epoch % 10 == 0:\n",
    "        print(f'Epoch {epoch}, Loss: {loss.item()}, Val Loss: {val_loss.item()}')\n",
    "        print(f'Loss amplifier: {loss_amplifier}')\n",
    "\n",
    "# Load the best model state\n",
    "model.load_state_dict(best_model)\n",
    "\n",
    "# Test the model\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    pred_test = model(x_categ[:x_cont_test.shape[0]], x_cont_test)\n",
    "\n",
    "# Move the predictions back to the CPU for further processing\n",
    "pred_test = pred_test.to(\"cpu\")\n",
    "E_test = E_test.to(\"cpu\")\n",
    "\n",
    "# Denormalize the E_tensor\n",
    "E_test_denorm = denormalize_data(E_test, E_return, E_return2, scaling_type)\n",
    "E_test_denorm_np = E_test_denorm.numpy()\n",
    "# Denormalize the predictions\n",
    "pred_test_denorm = denormalize_data(pred_test, E_return, E_return2, scaling_type)\n",
    "pred_test_denorm_np = pred_test_denorm.numpy()\n",
    "\n",
    "# MAPE\n",
    "mape = np.mean(np.abs((E_test_denorm_np - pred_test_denorm_np) / np.abs(E_test_denorm_np))) * 100\n",
    "# MdAPE\n",
    "mdape = np.median(np.abs((E_test_denorm_np - pred_test_denorm_np) / np.abs(E_test_denorm_np))) * 100\n",
    "\n",
    "# Calculate the difference\n",
    "difference = E_test_denorm_np - pred_test_denorm_np\n",
    "\n",
    "\n",
    "print(difference)\n",
    "\n",
    "print(pred_test, len(pred_test))\n",
    "print(pred_test_denorm, len(pred_test_denorm))\n",
    "\n",
    "# Plot the loss values\n",
    "plt.plot(loss_values)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the best model state\n",
    "# model.load_state_dict(best_model)\n",
    "# Load model itself\n",
    "model = TabTransformer_edit(\n",
    "    categories=(),\n",
    "    num_continuous=105,               # continuous features\n",
    "    dim=105,                          # dimension, paper set at 32\n",
    "    dim_out=15,                      # 15 due to the shape of E_data\n",
    "    depth=3,                         # depth, paper recommended 6 (9 worked better by .02%)\n",
    "    heads=15,                         # heads, paper recommends 8\n",
    "    attn_dropout=0.6,                # post-attention dropout\n",
    "    ff_dropout=0.6,                  # feed forward dropout\n",
    "    mlp_hidden_mults=(4,2),         # relative multiples of each hidden dimension of the last mlp to logits\n",
    "    mlp_act=nn.ReLU6(),               # activation for final mlp, defaults to relu, but could be anything else (selu, ELU, PReLU, Tanh, Sigmoid)\n",
    "    mlp_dropout=0.05,\n",
    ")\n",
    "\n",
    "model.load_state_dict(torch.load('model_weights2.pth'))\n",
    "model = model.to(device)\n",
    "\n",
    "# Test the model\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    pred_test = model(x_categ[:x_cont_test.shape[0]], x_cont_test)\n",
    "\n",
    "# Move the predictions back to the CPU for further processing\n",
    "pred_test = pred_test.to(\"cpu\")\n",
    "E_test = E_test.to(\"cpu\")\n",
    "\n",
    "# Denormalize the E_tensor\n",
    "E_test_denorm = denormalize_data(E_test, E_return, E_return2, scaling_type)\n",
    "E_test_denorm_np = E_test_denorm.numpy()\n",
    "# Denormalize the predictions\n",
    "pred_test_denorm = denormalize_data(pred_test, E_return, E_return2, scaling_type)\n",
    "pred_test_denorm_np = pred_test_denorm.numpy()\n",
    "\n",
    "# MAPE\n",
    "mape = np.mean(np.abs((E_test_denorm_np - pred_test_denorm_np) / np.abs(E_test_denorm_np))) * 100\n",
    "# MdAPE\n",
    "mdape = np.median(np.abs((E_test_denorm_np - pred_test_denorm_np) / np.abs(E_test_denorm_np))) * 100\n",
    "\n",
    "# Calculate the difference\n",
    "difference = E_test_denorm_np - pred_test_denorm_np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Difference:\n",
      "[12037840.0000, -6652400.0000, 1744.0000, -5513280.0000, -14083088.0000, 1456.0000, 16182576.0000, 17486608.0000, 191680.0000, -15680384.0000,\n",
      " 4306816.0000, -80608.0000, 7245136.0000, 13044552.0000, 24880.0000]\n",
      "\n",
      "E_test_denorm:\n",
      "[210000000.0000, 105000000.0000, 210000000.0000, 157500000.0000, 105000000.0000, 210000000.0000, 210000000.0000, 210000000.0000, 210000000.0000, 157500000.0000,\n",
      " 210000000.0000, 210000000.0000, 210000000.0000, 52500000.0000, 210000000.0000]\n",
      "\n",
      "Pred_test_denorm:\n",
      "[197962160.0000, 111652400.0000, 209998256.0000, 163013280.0000, 119083088.0000, 209998544.0000, 193817424.0000, 192513392.0000, 209808320.0000, 173180384.0000,\n",
      " 205693184.0000, 210080608.0000, 202754864.0000, 39455448.0000, 209975120.0000]\n",
      "\n",
      "\n",
      "MAPE: 5.1511%\n",
      "MdAPE: 2.3696228861808777%\n"
     ]
    }
   ],
   "source": [
    "# 2nd run best 8.18% 3.09% (New model)\n",
    "# 3rd run best 6.8% 2.8% (New model)\n",
    "# With freq 8.3% 3.8%\n",
    "# scale3 5.5% 2.4%\n",
    "print(\"Difference:\\n\" + format_array(difference[123], 4))\n",
    "print(\"\\nE_test_denorm:\\n\" + format_array(E_test_denorm_np[123], 4))\n",
    "print(\"\\nPred_test_denorm:\\n\" + format_array(pred_test_denorm_np[123], 4))\n",
    "print(f\"\\n\\nMAPE: {mape.item():.4f}%\")\n",
    "print(f'MdAPE: {mdape}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Difference:\n",
      "[16033328.0000, -10273936.0000, 192.0000, -8481472.0000, -18208808.0000, 144.0000, 608416.0000, 21745264.0000, 48.0000, -17191392.0000,\n",
      " 15435920.0000, 64.0000, 26534384.0000, 2199532.0000, 64.0000]\n",
      "\n",
      "E_test_denorm:\n",
      "[210000000.0000, 105000000.0000, 210000000.0000, 157500000.0000, 105000000.0000, 210000000.0000, 210000000.0000, 210000000.0000, 210000000.0000, 157500000.0000,\n",
      " 210000000.0000, 210000000.0000, 210000000.0000, 52500000.0000, 210000000.0000]\n",
      "\n",
      "Pred_test_denorm:\n",
      "[193966672.0000, 115273936.0000, 209999808.0000, 165981472.0000, 123208808.0000, 209999856.0000, 209391584.0000, 188254736.0000, 209999952.0000, 174691392.0000,\n",
      " 194564080.0000, 209999936.0000, 183465616.0000, 50300468.0000, 209999936.0000]\n",
      "\n",
      "\n",
      "MAPE: 6.6393%\n",
      "MdAPE: 2.83391997218132%\n"
     ]
    }
   ],
   "source": [
    "# 2nd run best 8.18% 3.09% (New model)\n",
    "# 3rd run best 6.8% 2.8% (New model)\n",
    "# With freq 8.3% 3.8%\n",
    "print(\"Difference:\\n\" + format_array(difference[123], 4))\n",
    "print(\"\\nE_test_denorm:\\n\" + format_array(E_test_denorm_np[123], 4))\n",
    "print(\"\\nPred_test_denorm:\\n\" + format_array(pred_test_denorm_np[123], 4))\n",
    "print(f\"\\n\\nMAPE: {mape.item():.4f}%\")\n",
    "print(f'MdAPE: {mdape}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Below includes tabnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\kaido\\anaconda3\\envs\\ds\\lib\\site-packages\\pytorch_tabnet\\abstract_model.py:82: UserWarning: Device used : cuda\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 0.86676 | train_mse: 0.23352999985218048| valid_mse: 0.23389999568462372|  0:01:42s\n",
      "epoch 1  | loss: 0.09806 | train_mse: 0.10768000036478043| valid_mse: 0.10780999809503555|  0:03:25s\n",
      "epoch 2  | loss: 0.08856 | train_mse: 0.08031000196933746| valid_mse: 0.08034999668598175|  0:05:07s\n",
      "epoch 3  | loss: 0.0833  | train_mse: 0.08005999773740768| valid_mse: 0.07992000132799149|  0:06:52s\n",
      "epoch 4  | loss: 0.07821 | train_mse: 0.07490000128746033| valid_mse: 0.07475999742746353|  0:08:40s\n",
      "epoch 5  | loss: 0.07495 | train_mse: 0.07046999782323837| valid_mse: 0.07046999782323837|  0:10:24s\n",
      "epoch 6  | loss: 0.07245 | train_mse: 0.07334999740123749| valid_mse: 0.07322999835014343|  0:12:06s\n",
      "epoch 7  | loss: 0.07115 | train_mse: 0.0652799978852272| valid_mse: 0.06524000316858292|  0:13:47s\n",
      "epoch 8  | loss: 0.06986 | train_mse: 0.06668999791145325| valid_mse: 0.06666000187397003|  0:15:29s\n",
      "epoch 9  | loss: 0.06925 | train_mse: 0.06870999932289124| valid_mse: 0.06859000027179718|  0:17:08s\n",
      "epoch 10 | loss: 0.06853 | train_mse: 0.06446000188589096| valid_mse: 0.06435000151395798|  0:18:44s\n",
      "epoch 11 | loss: 0.06789 | train_mse: 0.06849999725818634| valid_mse: 0.06838999688625336|  0:20:20s\n",
      "epoch 12 | loss: 0.06737 | train_mse: 0.06565000116825104| valid_mse: 0.06565000116825104|  0:21:56s\n",
      "epoch 13 | loss: 0.06695 | train_mse: 0.06397999823093414| valid_mse: 0.06384000182151794|  0:23:33s\n",
      "epoch 14 | loss: 0.06667 | train_mse: 0.06702999770641327| valid_mse: 0.06690999865531921|  0:25:10s\n",
      "epoch 15 | loss: 0.06609 | train_mse: 0.07255999743938446| valid_mse: 0.07242000102996826|  0:26:47s\n",
      "epoch 16 | loss: 0.06563 | train_mse: 0.0632999986410141| valid_mse: 0.06328999996185303|  0:28:30s\n",
      "epoch 17 | loss: 0.06519 | train_mse: 0.06710000336170197| valid_mse: 0.06697999686002731|  0:30:20s\n",
      "epoch 18 | loss: 0.06473 | train_mse: 0.06362999975681305| valid_mse: 0.06357000023126602|  0:32:07s\n",
      "epoch 19 | loss: 0.06402 | train_mse: 0.0693499967455864| valid_mse: 0.0692799985408783|  0:33:54s\n",
      "epoch 20 | loss: 0.06364 | train_mse: 0.062199998646974564| valid_mse: 0.062130000442266464|  0:35:41s\n",
      "epoch 21 | loss: 0.0631  | train_mse: 0.06094999983906746| valid_mse: 0.06100999936461449|  0:37:28s\n",
      "epoch 22 | loss: 0.063   | train_mse: 0.06368999928236008| valid_mse: 0.06360000371932983|  0:39:15s\n",
      "epoch 23 | loss: 0.06153 | train_mse: 0.06729999929666519| valid_mse: 0.06722000241279602|  0:41:01s\n",
      "epoch 24 | loss: 0.06008 | train_mse: 0.0744599997997284| valid_mse: 0.07451000064611435|  0:42:48s\n",
      "epoch 25 | loss: 0.05686 | train_mse: 0.06840000301599503| valid_mse: 0.06830000132322311|  0:44:35s\n",
      "epoch 26 | loss: 0.05505 | train_mse: 0.06105000153183937| valid_mse: 0.06097999960184097|  0:46:21s\n",
      "epoch 27 | loss: 0.05378 | train_mse: 0.06283999979496002| valid_mse: 0.06272000074386597|  0:48:08s\n",
      "epoch 28 | loss: 0.05449 | train_mse: 0.060669999569654465| valid_mse: 0.06055999919772148|  0:49:55s\n",
      "epoch 29 | loss: 0.05235 | train_mse: 0.056120000779628754| valid_mse: 0.055969998240470886|  0:51:41s\n",
      "epoch 30 | loss: 0.05122 | train_mse: 0.06357000023126602| valid_mse: 0.06357000023126602|  0:53:27s\n",
      "epoch 31 | loss: 0.04994 | train_mse: 0.051270000636577606| valid_mse: 0.05121000111103058|  0:55:13s\n",
      "epoch 32 | loss: 0.04867 | train_mse: 0.054829999804496765| valid_mse: 0.05488999933004379|  0:57:00s\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[27], line 43\u001b[0m\n\u001b[0;32m     38\u001b[0m x_cont_train, x_cont_test, E_train, E_test, train_indices, test_indices \u001b[38;5;241m=\u001b[39m train_test_split(\n\u001b[0;32m     39\u001b[0m     x_cont, E_tensor\u001b[38;5;241m.\u001b[39mnumpy(), \u001b[38;5;28mrange\u001b[39m(E_tensor\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]), train_size\u001b[38;5;241m=\u001b[39mtrain_ratio, random_state\u001b[38;5;241m=\u001b[39mseed\n\u001b[0;32m     40\u001b[0m )\n\u001b[0;32m     42\u001b[0m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[1;32m---> 43\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     44\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX_train\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mx_cont_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mE_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     45\u001b[0m \u001b[43m    \u001b[49m\u001b[43meval_set\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_cont_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mE_train\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_cont_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mE_test\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     46\u001b[0m \u001b[43m    \u001b[49m\u001b[43meval_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtrain\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mvalid\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     47\u001b[0m \u001b[43m    \u001b[49m\u001b[43meval_metric\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmse\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     48\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepochs_dl\u001b[49m\u001b[43m \u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpatience\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m150\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     49\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m512\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvirtual_batch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m64\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     50\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdrop_last\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\n\u001b[0;32m     51\u001b[0m \u001b[43m)\u001b[49m\n\u001b[0;32m     53\u001b[0m \u001b[38;5;66;03m# Test the model\u001b[39;00m\n\u001b[0;32m     54\u001b[0m pred_test \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpredict(x_cont_test)\n",
      "File \u001b[1;32mc:\\Users\\kaido\\anaconda3\\envs\\ds\\lib\\site-packages\\pytorch_tabnet\\abstract_model.py:258\u001b[0m, in \u001b[0;36mTabModel.fit\u001b[1;34m(self, X_train, y_train, eval_set, eval_name, eval_metric, loss_fn, weights, max_epochs, patience, batch_size, virtual_batch_size, num_workers, drop_last, callbacks, pin_memory, from_unsupervised, warm_start, augmentations, compute_importance)\u001b[0m\n\u001b[0;32m    253\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch_idx \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_epochs):\n\u001b[0;32m    254\u001b[0m \n\u001b[0;32m    255\u001b[0m     \u001b[38;5;66;03m# Call method on_epoch_begin for all callbacks\u001b[39;00m\n\u001b[0;32m    256\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_callback_container\u001b[38;5;241m.\u001b[39mon_epoch_begin(epoch_idx)\n\u001b[1;32m--> 258\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_train_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    260\u001b[0m     \u001b[38;5;66;03m# Apply predict epoch to all eval sets\u001b[39;00m\n\u001b[0;32m    261\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m eval_name, valid_dataloader \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(eval_names, valid_dataloaders):\n",
      "File \u001b[1;32mc:\\Users\\kaido\\anaconda3\\envs\\ds\\lib\\site-packages\\pytorch_tabnet\\abstract_model.py:489\u001b[0m, in \u001b[0;36mTabModel._train_epoch\u001b[1;34m(self, train_loader)\u001b[0m\n\u001b[0;32m    486\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch_idx, (X, y) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(train_loader):\n\u001b[0;32m    487\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_callback_container\u001b[38;5;241m.\u001b[39mon_batch_begin(batch_idx)\n\u001b[1;32m--> 489\u001b[0m     batch_logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_train_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    491\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_callback_container\u001b[38;5;241m.\u001b[39mon_batch_end(batch_idx, batch_logs)\n\u001b[0;32m    493\u001b[0m epoch_logs \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlr\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_optimizer\u001b[38;5;241m.\u001b[39mparam_groups[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlr\u001b[39m\u001b[38;5;124m\"\u001b[39m]}\n",
      "File \u001b[1;32mc:\\Users\\kaido\\anaconda3\\envs\\ds\\lib\\site-packages\\pytorch_tabnet\\abstract_model.py:527\u001b[0m, in \u001b[0;36mTabModel._train_batch\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m    524\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m param \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnetwork\u001b[38;5;241m.\u001b[39mparameters():\n\u001b[0;32m    525\u001b[0m     param\u001b[38;5;241m.\u001b[39mgrad \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m--> 527\u001b[0m output, M_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnetwork\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    529\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompute_loss(output, y)\n\u001b[0;32m    530\u001b[0m \u001b[38;5;66;03m# Add the overall sparsity loss\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\kaido\\anaconda3\\envs\\ds\\lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\kaido\\anaconda3\\envs\\ds\\lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\kaido\\anaconda3\\envs\\ds\\lib\\site-packages\\pytorch_tabnet\\tab_network.py:616\u001b[0m, in \u001b[0;36mTabNet.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    614\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m    615\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membedder(x)\n\u001b[1;32m--> 616\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtabnet\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\kaido\\anaconda3\\envs\\ds\\lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\kaido\\anaconda3\\envs\\ds\\lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\kaido\\anaconda3\\envs\\ds\\lib\\site-packages\\pytorch_tabnet\\tab_network.py:492\u001b[0m, in \u001b[0;36mTabNetNoEmbeddings.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    490\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m    491\u001b[0m     res \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m--> 492\u001b[0m     steps_output, M_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    493\u001b[0m     res \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39msum(torch\u001b[38;5;241m.\u001b[39mstack(steps_output, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m), dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m    495\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_multi_task:\n\u001b[0;32m    496\u001b[0m         \u001b[38;5;66;03m# Result will be in list format\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\kaido\\anaconda3\\envs\\ds\\lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\kaido\\anaconda3\\envs\\ds\\lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\kaido\\anaconda3\\envs\\ds\\lib\\site-packages\\pytorch_tabnet\\tab_network.py:181\u001b[0m, in \u001b[0;36mTabNetEncoder.forward\u001b[1;34m(self, x, prior)\u001b[0m\n\u001b[0;32m    179\u001b[0m M_feature_level \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmatmul(M, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroup_attention_matrix)\n\u001b[0;32m    180\u001b[0m masked_x \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmul(M_feature_level, x)\n\u001b[1;32m--> 181\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfeat_transformers\u001b[49m\u001b[43m[\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmasked_x\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    182\u001b[0m d \u001b[38;5;241m=\u001b[39m ReLU()(out[:, : \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_d])\n\u001b[0;32m    183\u001b[0m steps_output\u001b[38;5;241m.\u001b[39mappend(d)\n",
      "File \u001b[1;32mc:\\Users\\kaido\\anaconda3\\envs\\ds\\lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\kaido\\anaconda3\\envs\\ds\\lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\kaido\\anaconda3\\envs\\ds\\lib\\site-packages\\pytorch_tabnet\\tab_network.py:738\u001b[0m, in \u001b[0;36mFeatTransformer.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    736\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m    737\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mshared(x)\n\u001b[1;32m--> 738\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mspecifics\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "File \u001b[1;32mc:\\Users\\kaido\\anaconda3\\envs\\ds\\lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\kaido\\anaconda3\\envs\\ds\\lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\kaido\\anaconda3\\envs\\ds\\lib\\site-packages\\pytorch_tabnet\\tab_network.py:780\u001b[0m, in \u001b[0;36mGLU_Block.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    777\u001b[0m     layers_left \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_glu)\n\u001b[0;32m    779\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m glu_id \u001b[38;5;129;01min\u001b[39;00m layers_left:\n\u001b[1;32m--> 780\u001b[0m     x \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39madd(x, \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mglu_layers\u001b[49m\u001b[43m[\u001b[49m\u001b[43mglu_id\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m    781\u001b[0m     x \u001b[38;5;241m=\u001b[39m x \u001b[38;5;241m*\u001b[39m scale\n\u001b[0;32m    782\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "File \u001b[1;32mc:\\Users\\kaido\\anaconda3\\envs\\ds\\lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\kaido\\anaconda3\\envs\\ds\\lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\kaido\\anaconda3\\envs\\ds\\lib\\site-packages\\pytorch_tabnet\\tab_network.py:804\u001b[0m, in \u001b[0;36mGLU_Layer.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    802\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m    803\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc(x)\n\u001b[1;32m--> 804\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    805\u001b[0m     out \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmul(x[:, : \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput_dim], torch\u001b[38;5;241m.\u001b[39msigmoid(x[:, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput_dim :]))\n\u001b[0;32m    806\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
      "File \u001b[1;32mc:\\Users\\kaido\\anaconda3\\envs\\ds\\lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\kaido\\anaconda3\\envs\\ds\\lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\kaido\\anaconda3\\envs\\ds\\lib\\site-packages\\pytorch_tabnet\\tab_network.py:36\u001b[0m, in \u001b[0;36mGBN.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     34\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m     35\u001b[0m     chunks \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mchunk(\u001b[38;5;28mint\u001b[39m(np\u001b[38;5;241m.\u001b[39mceil(x\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvirtual_batch_size)), \u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m---> 36\u001b[0m     res \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbn(x_) \u001b[38;5;28;01mfor\u001b[39;00m x_ \u001b[38;5;129;01min\u001b[39;00m chunks]\n\u001b[0;32m     38\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcat(res, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\kaido\\anaconda3\\envs\\ds\\lib\\site-packages\\pytorch_tabnet\\tab_network.py:36\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     34\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m     35\u001b[0m     chunks \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mchunk(\u001b[38;5;28mint\u001b[39m(np\u001b[38;5;241m.\u001b[39mceil(x\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvirtual_batch_size)), \u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m---> 36\u001b[0m     res \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m x_ \u001b[38;5;129;01min\u001b[39;00m chunks]\n\u001b[0;32m     38\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcat(res, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\kaido\\anaconda3\\envs\\ds\\lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\kaido\\anaconda3\\envs\\ds\\lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\kaido\\anaconda3\\envs\\ds\\lib\\site-packages\\torch\\nn\\modules\\batchnorm.py:155\u001b[0m, in \u001b[0;36m_BatchNorm.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    152\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrack_running_stats:\n\u001b[0;32m    153\u001b[0m     \u001b[38;5;66;03m# TODO: if statement only here to tell the jit to skip emitting this when it is None\u001b[39;00m\n\u001b[0;32m    154\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_batches_tracked \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:  \u001b[38;5;66;03m# type: ignore[has-type]\u001b[39;00m\n\u001b[1;32m--> 155\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_batches_tracked\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd_\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore[has-type]\u001b[39;00m\n\u001b[0;32m    156\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmomentum \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:  \u001b[38;5;66;03m# use cumulative moving average\u001b[39;00m\n\u001b[0;32m    157\u001b[0m             exponential_average_factor \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1.0\u001b[39m \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mfloat\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_batches_tracked)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "type_tensor = torch.zeros(A_tensor.shape)\n",
    "type_tensor[:, col_idx] = 1\n",
    "\n",
    "# Define the indices of categorical features\n",
    "cat_idxs = [0]\n",
    "\n",
    "# Define the number of unique values for each categorical feature\n",
    "cat_dims = [2]\n",
    "\n",
    "# Define the TabNet model\n",
    "model = TabNetRegressor(\n",
    "    n_d=64, n_a=64, n_steps=5,\n",
    "    gamma=1, n_independent=3, n_shared=3,\n",
    "    lambda_sparse=5e-4, momentum=0.35, clip_value=3.,\n",
    "    optimizer_fn=torch.optim.Adam,\n",
    "    optimizer_params=dict(lr=2e-3),\n",
    "    scheduler_params={\"step_size\":75, \"gamma\":0.8},\n",
    "    scheduler_fn=torch.optim.lr_scheduler.StepLR,\n",
    "    mask_type='entmax',\n",
    "    # cat_idxs=cat_idxs,\n",
    "    # cat_dims=cat_dims,\n",
    "    device_name='cuda' if torch.cuda.is_available() else 'cpu'\n",
    ")\n",
    "\n",
    "# Split X_data into three separate sets of features\n",
    "X1_tensor = X_tensor[:, :15].clone().detach()\n",
    "X2_tensor = X_tensor[:, 15:30].clone().detach()\n",
    "X3_tensor = X_tensor[:, 30:].clone().detach()\n",
    "\n",
    "# Concatenate the tensors along the second dimension (features) WITH CATEGORY\n",
    "x_cont = torch.cat((type_tensor, A_tensor, J_tensor, h_tensor, beta_tensor, X1_tensor, X2_tensor, X3_tensor), dim=1)\n",
    "x_cont = x_cont.numpy()  # Convert tensors to numpy arrays for TabNet\n",
    "# Concatenate the tensors along the second dimension (features)\n",
    "x_cont = torch.cat((A_tensor, J_tensor, h_tensor, beta_tensor, X1_tensor, X2_tensor, X3_tensor), dim=1)\n",
    "x_cont = x_cont.numpy()  # Convert tensors to numpy arrays for TabNet\n",
    "\n",
    "# Split the data into a training set and a test set and get the indices\n",
    "x_cont_train, x_cont_test, E_train, E_test, train_indices, test_indices = train_test_split(\n",
    "    x_cont, E_tensor.numpy(), range(E_tensor.shape[0]), train_size=train_ratio, random_state=seed\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "model.fit(\n",
    "    X_train=x_cont_train, y_train=E_train,\n",
    "    eval_set=[(x_cont_train, E_train), (x_cont_test, E_test)],\n",
    "    eval_name=['train', 'valid'],\n",
    "    eval_metric=['mse'],\n",
    "    max_epochs=epochs_dl_tab , patience=150,\n",
    "    batch_size=512, virtual_batch_size=64,\n",
    "    drop_last=False\n",
    ")\n",
    "\n",
    "# Test the model\n",
    "pred_test = model.predict(x_cont_test)\n",
    "\n",
    "# Denormalize the E_tensor\n",
    "E_test_denorm = E_test * E_std + E_mean\n",
    "E_test_denorm_np = E_test_denorm\n",
    "\n",
    "# Denormalize the predictions\n",
    "pred_test_denorm = pred_test * E_std + E_mean\n",
    "pred_test_denorm_np = pred_test_denorm\n",
    "\n",
    "# MAPE\n",
    "mape = np.mean(np.abs((E_test_denorm_np - pred_test_denorm_np) / np.abs(E_test_denorm_np))) * 100\n",
    "\n",
    "# Calculate the difference\n",
    "difference = E_test_denorm_np - pred_test_denorm_np\n",
    "\n",
    "print(difference)\n",
    "\n",
    "print(pred_test, len(pred_test))\n",
    "print(pred_test_denorm, len(pred_test_denorm))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1):\n",
    "    # Define the TabNet model (initial hyperparameters)\n",
    "    model = TabNetRegressor(\n",
    "        n_d=128,  # Feature embedding dimension\n",
    "        n_a=128,  # Attention embedding dimension\n",
    "        n_steps=8,  # Number of attention steps\n",
    "        gamma=1.0,   # Feature importance weighting factor\n",
    "        n_independent=2,  # Number of independent heads in attention\n",
    "        n_shared=2,   # Number of shared heads in attention\n",
    "        lambda_sparse=1.5e-4,  # Sparsity penalty for attention weights\n",
    "        momentum=0.3,  # Momentum for moving averages\n",
    "        clip_value=2.0,  # Clipping threshold for attention weights\n",
    "        optimizer_fn=torch.optim.Adam,\n",
    "        optimizer_params=dict(lr=2e-3),  # Learning rate (initial guess)\n",
    "        scheduler_params={\"step_size\": 75, \"gamma\": 0.8},\n",
    "        scheduler_fn=torch.optim.lr_scheduler.StepLR,\n",
    "        mask_type='entmax',\n",
    "        device_name='cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    )\n",
    "\n",
    "\n",
    "    # Split X_data into three separate sets of features\n",
    "    X1_tensor = X_tensor[:, :15].clone().detach()\n",
    "    X2_tensor = X_tensor[:, 15:30].clone().detach()\n",
    "    X3_tensor = X_tensor[:, 30:].clone().detach()\n",
    "\n",
    "    # Concatenate the tensors along the second dimension (features)\n",
    "    x_cont = torch.cat((A_tensor, J_tensor, h_tensor, X1_tensor, X2_tensor, X3_tensor), dim=1)\n",
    "    x_cont = x_cont.numpy()  # Convert tensors to numpy arrays for TabNet\n",
    "\n",
    "    # Split the data into a training set and a test set and get the indices\n",
    "    x_cont_train, x_cont_test, E_train, E_test, train_indices, test_indices = train_test_split(\n",
    "        x_cont, E_tensor.numpy(), range(E_tensor.shape[0]), train_size=train_ratio, random_state=seed\n",
    "    )\n",
    "\n",
    "    # Option 1: Manual Grid Search\n",
    "    best_model = None\n",
    "    best_mse = float('inf')\n",
    "    for gamma in [0.25,0.35,0.4]:  # Experiment with learning rates\n",
    "        for steps in [1e-4, 1.25e-4, 1.5e-4, 2e-4]:  # Experiment with attention steps\n",
    "            # Create a new model with the current learning rate\n",
    "            model = TabNetRegressor(\n",
    "                n_d=64, n_a=64, n_steps=3,\n",
    "                gamma=1, n_independent=2, n_shared=2,\n",
    "                lambda_sparse=steps, momentum=gamma, clip_value=2.0,\n",
    "                optimizer_fn=torch.optim.Adam,\n",
    "                optimizer_params=dict(lr=5e-3),  # Set learning rate for this iteration\n",
    "                scheduler_params={\"step_size\":75, \"gamma\":0.8},\n",
    "                scheduler_fn=torch.optim.lr_scheduler.StepLR,\n",
    "                mask_type='entmax',\n",
    "                device_name='cuda' if torch.cuda.is_available() else 'cpu'\n",
    "            )\n",
    "            model.fit(\n",
    "                X_train=x_cont_train, y_train=E_train,\n",
    "                eval_set=[(x_cont_train, E_train), (x_cont_test, E_test)],\n",
    "                eval_name=['train', 'valid'],\n",
    "                eval_metric=['mse'],\n",
    "                max_epochs=epochs_dl,\n",
    "                patience=50,  # Early stopping after 50 epochs without improvement\n",
    "                batch_size=512, virtual_batch_size=64,\n",
    "                drop_last=False\n",
    "            )\n",
    "            valid_mse = model.history['valid_mse'][-1]  # Get last validation MSE\n",
    "            if valid_mse < best_mse:\n",
    "                best_model = model\n",
    "                best_mse = valid_mse\n",
    "\n",
    "\n",
    "    if best_params:\n",
    "        print(\"Best hyperparameters found:\", best_params)\n",
    "        # Update model with best hyperparameters\n",
    "        model.n_steps = best_params[1]\n",
    "        model.optimizer_params = dict(lr=best_params[0])\n",
    "\n",
    "    # Test the model\n",
    "    pred_test = model.predict(x_cont_test)\n",
    "\n",
    "    # Denormalize the E_tensor\n",
    "    E_test_denorm = E_test * E_std + E_mean\n",
    "    E_test_denorm_np = E_test_denorm\n",
    "\n",
    "    # Denormalize the predictions\n",
    "    pred_test_denorm = pred_test * E_std + E_mean\n",
    "    pred_test_denorm_np = pred_test_denorm\n",
    "\n",
    "    # MAPE\n",
    "    mape = np.mean(np.abs((E_test_denorm_np - pred_test_denorm_np) / np.abs(E_test_denorm_np))) * 100\n",
    "\n",
    "    # Calculate the difference\n",
    "    difference = E_test_denorm_np - pred_test_denorm_np\n",
    "\n",
    "    print(difference)\n",
    "\n",
    "    print(pred_test, len(pred_test))\n",
    "    print(pred_test_denorm, len(pred_test_denorm))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Difference:\n",
      "[51101360.0000, -54166288.0000, 271664.0000, 45799744.0000, -6060112.0000, 577616.0000, -60078592.0000, 44737152.0000, 589040.0000, -85990416.0000,\n",
      " 22953376.0000, 711024.0000, 18914320.0000, 18138160.0000, 651664.0000]\n",
      "\n",
      "E_test_denorm:\n",
      "[157500000.0000, 52500000.0000, 210000000.0000, 210000000.0000, 157500000.0000, 210000000.0000, 105000000.0000, 210000000.0000, 210000000.0000, 52500000.0000,\n",
      " 157500000.0000, 210000000.0000, 210000000.0000, 210000000.0000, 210000000.0000]\n",
      "\n",
      "Pred_test_denorm:\n",
      "[106398640.0000, 106666288.0000, 209728336.0000, 164200256.0000, 163560112.0000, 209422384.0000, 165078592.0000, 165262848.0000, 209410960.0000, 138490416.0000,\n",
      " 134546624.0000, 209288976.0000, 191085680.0000, 191861840.0000, 209348336.0000]\n",
      "\n",
      "\n",
      "MAPE: 26.5393%\n"
     ]
    }
   ],
   "source": [
    "print(\"Difference:\\n\" + format_array(difference[123], 4))\n",
    "print(\"\\nE_test_denorm:\\n\" + format_array(E_test_denorm_np[123], 4))\n",
    "print(\"\\nPred_test_denorm:\\n\" + format_array(pred_test_denorm_np[123], 4))\n",
    "print(f\"\\n\\nMAPE: {mape.item():.4f}%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ds",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
