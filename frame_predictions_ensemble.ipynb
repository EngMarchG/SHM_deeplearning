{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import copy\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "import xgboost as xgb\n",
    "import matplotlib.pyplot as plt\n",
    "from xgboost import XGBRFRegressor\n",
    "from tab_transformer_copy import TabTransformer_edit\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.multioutput import MultiOutputRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "from helper_ml import train_xgb_models, load_models, generate_predictions_and_train_second_level_model\n",
    "from helper_analysis import normalize_data, load_and_combine_data, format_array, load_and_normalize_data_tensor, denormalize_data\n",
    "\n",
    "folder_path = \"your_output_folder_path\"  # Replace with your actual folder path\n",
    "epochs_dl = 10 # For deep learning models\n",
    "epochs_dl_tab = 20 # For TabNet model\n",
    "epochs_ml = 200 # For machine learning models\n",
    "max_files = 6 \n",
    "seed = 42\n",
    "scaling_type = \"minmax\"  # \"minmax\" or any other for z-score\n",
    "scheduler = 0  # 1 for step, 2 for plateau, 3 for cosine\n",
    "patience = 30\n",
    "\n",
    "# Define the ratio for the train-test split\n",
    "train_ratio = 0.8\n",
    "\n",
    "# Check if CUDA is available and set the device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n",
    "\n",
    "# Define your L1 regularization strength\n",
    "l1_lambda = 0.0001\n",
    "l2_lambda = 0.0001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([81922, 15]),\n",
       " torch.Size([81922, 15]),\n",
       " torch.Size([81922, 15]),\n",
       " torch.Size([81922, 15]),\n",
       " torch.Size([81922, 45]),\n",
       " torch.Size([81922, 45]))"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load and normalize the data\n",
    "E_tensor, E_return, E_return2 = load_and_normalize_data_tensor(folder_path, \"E_combo_output\", max_files=max_files)\n",
    "A_tensor, _, _ = load_and_normalize_data_tensor(folder_path, \"A_combo_output\", max_files=max_files)\n",
    "J_tensor, _, _ = load_and_normalize_data_tensor(folder_path, \"J_combo_output\", max_files=max_files)\n",
    "h_tensor, _, _ = load_and_normalize_data_tensor(folder_path, \"h_combo_output\", max_files=max_files)\n",
    "X_tensor, _, _ = load_and_normalize_data_tensor(folder_path, \"X_combo_output\", max_files=max_files, scaling_type=scaling_type)\n",
    "w_tensor, _, _ = load_and_normalize_data_tensor(folder_path, \"w_combo_output\", max_files=max_files)\n",
    "\n",
    "# After loading the data. Make a new data tensor of size (n_samples, n_features) taking A_tensor as reference\n",
    "# Starting from the 2nd index and moving by 3 beta (the angle) will be 0 and the rest will be 90 degrees (pi/2)\n",
    "\n",
    "\n",
    "E_tensor.shape, A_tensor.shape, J_tensor.shape, h_tensor.shape, X_tensor.shape, w_tensor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([81922, 15]),\n",
       " tensor([1.5708, 1.5708, 0.0000, 1.5708, 1.5708, 0.0000, 1.5708, 1.5708, 0.0000,\n",
       "         1.5708, 1.5708, 0.0000, 1.5708, 1.5708, 0.0000]),\n",
       " tensor([1.]))"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "col_idx = [0,1,3,4,6,7,9,10,12,13]\n",
    "beta_tensor = torch.zeros(A_tensor.shape)\n",
    "type_tensor = torch.zeros(A_tensor.shape)\n",
    "type_tensor[:, col_idx] = 1\n",
    "beta_tensor[:, col_idx] = np.pi/2\n",
    "# Reshape type_tensor to have a shape of (81922, 1)\n",
    "type_tensor = type_tensor[:, :1]\n",
    "# x_cont = torch.stack((A_tensor, J_tensor, h_tensor, beta_tensor, w1_tensor, X1_tensor, w2_tensor, X2_tensor, w3_tensor, X3_tensor), dim=1)\n",
    "beta_tensor.shape, beta_tensor[0], type_tensor[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "\n",
    "# Split X_data and w_data into three separate sets of features\n",
    "X1_tensor = X_tensor[:, :15].clone().detach()\n",
    "X2_tensor = X_tensor[:, 15:30].clone().detach()\n",
    "X3_tensor = X_tensor[:, 30:].clone().detach()\n",
    "w1_tensor = w_tensor[:, :15].clone().detach()\n",
    "w2_tensor = w_tensor[:, 15:30].clone().detach()\n",
    "w3_tensor = w_tensor[:, 30:].clone().detach()\n",
    "\n",
    "\n",
    "\n",
    "x_cont = torch.cat((A_tensor, J_tensor, h_tensor, beta_tensor, w1_tensor, X1_tensor, w2_tensor, X2_tensor, w3_tensor, X3_tensor), dim=1)\n",
    "x_categ = torch.empty((x_cont.shape[0], 0))\n",
    "\n",
    "x_cont_np = x_cont.numpy()\n",
    "# Flatten your input tensors and concatenate them into a 2D array\n",
    "X = x_cont_np\n",
    "y = E_tensor\n",
    "\n",
    "# Split your data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Make a tensor version of the splitted data\n",
    "X_train_tensor = torch.tensor(X_train, dtype=torch.float32).clone().detach().to(device)\n",
    "X_test_tensor = torch.tensor(X_test, dtype=torch.float32).clone().detach().to(device)\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.float32).clone().detach().to(device)\n",
    "y_test_tensor = torch.tensor(y_test, dtype=torch.float32).clone().detach().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([47003, 15]), torch.Size([47003, 15]))"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test.shape, y_test_tensor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.decomposition import PCA\n",
    "# from sklearn.decomposition import KernelPCA\n",
    "\n",
    "\n",
    "# # Assuming x_cont is your feature matrix\n",
    "# x_cont_np = x_cont.numpy()  # Convert to numpy array if it's a tensor\n",
    "\n",
    "# # Initialize PCA\n",
    "# # You can specify the number of components you want to keep, e.g., PCA(n_components=10)\n",
    "# pca = PCA()\n",
    "\n",
    "# # Fit PCA to your data\n",
    "# # pca.fit(x_cont_np)\n",
    "\n",
    "# # Transform your data to the first N principal components\n",
    "# # x_cont_pca = pca.transform(x_cont_np)\n",
    "\n",
    "# # Kernel PCA\n",
    "# kpca = KernelPCA(n_components=2, kernel='rbf', degree=2)  # You can adjust the number of components and the kernel\n",
    "# x_cont_kpca = kpca.fit_transform(x_cont_np)\n",
    "\n",
    "# # The explained variance ratio tells you how much information (variance) can be attributed to each principal component\n",
    "# # print(pca.explained_variance_ratio_)\n",
    "\n",
    "# # Convert the transformed data to a DataFrame\n",
    "# df = pd.DataFrame(x_cont_kpca, columns=[f'PC{i+1}' for i in range(x_cont_kpca.shape[1])])\n",
    "\n",
    "# # Create a scatter plot matrix\n",
    "# sns.pairplot(df, vars=['PC1', 'PC2', 'PC3', 'PC4'])  # You can adjust the components you want to plot\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize your XGBoost regressors\n",
    "models = [xgb.XGBRegressor(objective ='reg:squarederror', colsample_bytree = 0.5, learning_rate = 0.1,\n",
    "                max_depth = 15, alpha = 5, n_estimators = 200) for _ in range(y.shape[1])]\n",
    "\n",
    "# Fit the models to the training data and make predictions\n",
    "y_pred = np.zeros_like(y_test)\n",
    "for i, model in enumerate(models):\n",
    "    model.fit(X_train, y_train[:, i])\n",
    "    y_pred[:, i] = model.predict(X_test)\n",
    "\n",
    "    # Save the trained model\n",
    "    model.save_model(f'xgb_model_{i}.json')\n",
    "\n",
    "# Compute the RMSE of the predictions\n",
    "rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "print(\"RMSE: %f\" % (rmse))\n",
    "\n",
    "# Initialize an empty list to hold the models\n",
    "models = []\n",
    "\n",
    "# Load the models\n",
    "for i in range(y.shape[1]):\n",
    "    model = xgb.XGBRegressor()\n",
    "    model.load_model(f'xgb_model_{i}.json')\n",
    "    models.append(model)\n",
    "\n",
    "\n",
    "# Convert the predictions to a tensor\n",
    "y_pred_tensor = torch.tensor(y_pred)\n",
    "\n",
    "# Denormalize the predictions\n",
    "y_pred_denorm = denormalize_data(y_pred_tensor, E_return, E_return2, scaling_type)\n",
    "y_pred_denorm_np = y_pred_denorm.numpy()\n",
    "\n",
    "# Denormalize the actual values\n",
    "y_test_tensor = torch.tensor(y_test)\n",
    "y_test_denorm = denormalize_data(y_test_tensor, E_return, E_return2, scaling_type)\n",
    "y_test_denorm_np = y_test_denorm.numpy()\n",
    "\n",
    "# Calculate MAPE\n",
    "mape = np.mean(np.abs((y_test_denorm_np - y_pred_denorm_np) / np.abs(y_test_denorm_np))) * 100\n",
    "\n",
    "# Calculate MdAPE\n",
    "mdape = np.median(np.abs((y_test_denorm_np - y_pred_denorm_np) / np.abs(y_test_denorm_np))) * 100\n",
    "\n",
    "# Calculate the difference\n",
    "difference = y_test_denorm_np - y_pred_denorm_np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Training the models\n",
    "\n",
    "# Train the models\n",
    "y_pred = train_xgb_models(X_train, y_train, X_test, y_test)\n",
    "\n",
    "# Load the models\n",
    "models = load_models(y.shape[1], use_gpu=True)\n",
    "\n",
    "# Predict the test set (if you have not trained the models in this session)\n",
    "y_pred = np.zeros_like(y_test)\n",
    "for i, model in enumerate(models):\n",
    "    y_pred[:, i] = model.predict(X_test)\n",
    "\n",
    "# Convert the predictions to a tensor\n",
    "y_pred_tensor = torch.tensor(y_pred)\n",
    "\n",
    "# Denormalize the predictions\n",
    "y_pred_denorm = denormalize_data(y_pred_tensor, E_return, E_return2, scaling_type)\n",
    "y_pred_denorm_np = y_pred_denorm.numpy()\n",
    "\n",
    "# Denormalize the actual values\n",
    "y_test_tensor = torch.tensor(y_test)\n",
    "y_test_denorm = denormalize_data(y_test_tensor, E_return, E_return2, scaling_type)\n",
    "y_test_denorm_np = y_test_denorm.numpy()\n",
    "\n",
    "# Calculate MAPE\n",
    "mape = np.mean(np.abs((y_test_denorm_np - y_pred_denorm_np) / np.abs(y_test_denorm_np))) * 100\n",
    "\n",
    "# Calculate MdAPE\n",
    "mdape = np.median(np.abs((y_test_denorm_np - y_pred_denorm_np) / np.abs(y_test_denorm_np))) * 100\n",
    "\n",
    "# Calculate the difference\n",
    "difference = y_test_denorm_np - y_pred_denorm_np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAPE: 7.362738996744156\n",
      "MdAPE: 2.2229790687561035\n",
      "Difference: [  5604208.  -4466640.         0.  -2811184.   -506384.         0.\n",
      "   -112224.  -3357280.         0. -20054432.   3808960.         0.\n",
      "  14474864.   4909956.         0.]\n",
      "Original: [2.100e+08 1.050e+08 2.100e+08 1.575e+08 1.050e+08 2.100e+08 2.100e+08\n",
      " 2.100e+08 2.100e+08 1.575e+08 2.100e+08 2.100e+08 2.100e+08 5.250e+07\n",
      " 2.100e+08] 16385\n",
      "Denormalized predictions: [2.0439579e+08 1.0946664e+08 2.1000000e+08 1.6031118e+08 1.0550638e+08\n",
      " 2.1000000e+08 2.1011222e+08 2.1335728e+08 2.1000000e+08 1.7755443e+08\n",
      " 2.0619104e+08 2.1000000e+08 1.9552514e+08 4.7590044e+07 2.1000000e+08] 16385\n"
     ]
    }
   ],
   "source": [
    "# Print the results\n",
    "print(\"MAPE:\", mape)\n",
    "print(\"MdAPE:\", mdape)\n",
    "print(\"Difference:\", difference[123])\n",
    "print(\"Original:\", y_test_denorm[123].numpy(), len(y_pred))\n",
    "print(\"Denormalized predictions:\", y_pred_denorm[123].numpy(), len(y_pred_denorm))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model itself\n",
    "model = TabTransformer_edit(\n",
    "    categories=(),\n",
    "    num_continuous=150,               # continuous features\n",
    "    dim=150,                          # dimension, paper set at 32\n",
    "    dim_out=15,                      # 15 due to the shape of E_data\n",
    "    depth=3,                         # depth, paper recommended 6 (9 worked better by .02%)\n",
    "    heads=15,                         # heads, paper recommends 8\n",
    "    attn_dropout=0.6,                # post-attention dropout\n",
    "    ff_dropout=0.6,                  # feed forward dropout\n",
    "    mlp_hidden_mults=(4,2),         # relative multiples of each hidden dimension of the last mlp to logits\n",
    "    mlp_act=nn.ReLU6(),               # activation for final mlp, defaults to relu, but could be anything else (selu, ELU, PReLU, Tanh, Sigmoid)\n",
    "    mlp_dropout=0.05,\n",
    ")\n",
    "\n",
    "model.load_state_dict(torch.load('model_weights_withfreq4.pth'))\n",
    "model = model.to(device)\n",
    "\n",
    "# Test the model\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    pred_test_dl = model(x_categ[:X_test.shape[0]], X_test_tensor)\n",
    "\n",
    "# Move the predictions back to the CPU for further processing\n",
    "pred_test_dl = pred_test_dl.to(\"cpu\")\n",
    "y_test = y_test.to(\"cpu\")\n",
    "\n",
    "# Denormalize the E_tensor\n",
    "E_test_denorm = denormalize_data(y_test, E_return, E_return2, scaling_type)\n",
    "E_test_denorm_np = E_test_denorm.numpy()\n",
    "# Denormalize the predictions\n",
    "pred_test_denorm_dl = denormalize_data(pred_test_dl, E_return, E_return2, scaling_type)\n",
    "pred_test_denorm_np_dl = pred_test_denorm_dl.numpy()\n",
    "\n",
    "# MAPE\n",
    "mape_dl = np.mean(np.abs((E_test_denorm_np - pred_test_denorm_np_dl) / np.abs(E_test_denorm_np))) * 100\n",
    "# MdAPE\n",
    "mdape_dl = np.median(np.abs((E_test_denorm_np - pred_test_denorm_np_dl) / np.abs(E_test_denorm_np))) * 100\n",
    "\n",
    "# Calculate the difference\n",
    "difference_dl = E_test_denorm_np - pred_test_denorm_np_dl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a copy of the model\n",
    "model2 = copy.deepcopy(model)\n",
    "\n",
    "# Load different weights into model2\n",
    "model2.load_state_dict(torch.load('model_weights_withfreq4.pth'))\n",
    "\n",
    "# Move model2 to the device\n",
    "model2 = model2.to(device)\n",
    "\n",
    "# Test model2\n",
    "model2.eval()\n",
    "with torch.no_grad():\n",
    "    pred_test_dl2 = model2(x_categ[:X_test.shape[0]], X_test_tensor)\n",
    "\n",
    "# Move the predictions back to the CPU for further processing\n",
    "pred_test_dl2 = pred_test_dl2.to(\"cpu\")\n",
    "\n",
    "# Denormalize the predictions\n",
    "pred_test_denorm_dl2 = denormalize_data(pred_test_dl2, E_return, E_return2, scaling_type)\n",
    "pred_test_denorm_np_dl2 = pred_test_denorm_dl2.numpy()\n",
    "\n",
    "# MAPE\n",
    "mape_dl2 = np.mean(np.abs((E_test_denorm_np - pred_test_denorm_np_dl2) / np.abs(E_test_denorm_np))) * 100\n",
    "# MdAPE\n",
    "mdape_dl2 = np.median(np.abs((E_test_denorm_np - pred_test_denorm_np_dl2) / np.abs(E_test_denorm_np))) * 100\n",
    "\n",
    "# Calculate the difference\n",
    "difference_dl2 = E_test_denorm_np - pred_test_denorm_np_dl2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAPE: 5.106492340564728 4.091095924377441\n",
      "MdAPE: 2.2805027663707733 1.7494704574346542\n",
      "Difference: [ 6.8607200e+06 -5.1106880e+06  7.0400000e+02 -1.0403296e+07\n",
      " -7.0272400e+06  1.1840000e+03  6.4405920e+06  2.7732000e+06\n",
      "  9.4400000e+02  5.1756800e+06  8.6895360e+06  1.7600000e+02\n",
      "  2.2118432e+07 -2.9237120e+06  6.8800000e+02]\n",
      "Original: [2.100e+08 1.050e+08 2.100e+08 1.575e+08 1.050e+08 2.100e+08 2.100e+08\n",
      " 2.100e+08 2.100e+08 1.575e+08 2.100e+08 2.100e+08 2.100e+08 5.250e+07\n",
      " 2.100e+08]\n"
     ]
    }
   ],
   "source": [
    "# Print the results\n",
    "sample = 123\n",
    "print(\"MAPE:\", mape_dl, mape_dl2)\n",
    "print(\"MdAPE:\", mdape_dl, mdape_dl2)\n",
    "print(\"Difference:\", difference_dl2[sample])\n",
    "print(\"Original:\", E_test_denorm[sample].numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### ENSEMBLE MODEL 1 - Naive Method ###\n",
    "# Calculate the element-wise MAPE\n",
    "mape_lgbm = np.abs((y_test_denorm_np - y_pred_denorm_np) / np.abs(y_test_denorm_np)) + 1e-10\n",
    "mape_dl = np.abs((y_test_denorm_np - pred_test_denorm_np_dl) / np.abs(y_test_denorm_np)) + 1e-10\n",
    "mape_dl_2 = np.abs((y_test_denorm_np - pred_test_denorm_np_dl2) / np.abs(y_test_denorm_np)) + 1e-10\n",
    "\n",
    "# Calculate the variance of the MAPE element-wise\n",
    "var_lgbm = np.var(mape_lgbm)\n",
    "var_dl = np.var(mape_dl)\n",
    "var_dl_2 = np.var(mape_dl_2)\n",
    "\n",
    "# Calculate the weights based on the inverse of the variance\n",
    "weight_lgbm = 1 / var_lgbm\n",
    "weight_dl = 1 / var_dl\n",
    "weight_dl_2 = 1 / var_dl_2\n",
    "\n",
    "# Normalize the weights so they sum to 1 for each element\n",
    "total_weight = weight_lgbm + weight_dl + weight_dl_2\n",
    "weight_lgbm /= total_weight\n",
    "weight_dl /= total_weight\n",
    "weight_dl_2 /= total_weight\n",
    "\n",
    "# Perform the ensemble\n",
    "ensemble_prediction = weight_lgbm * y_pred_denorm_np + weight_dl * pred_test_denorm_np_dl + weight_dl_2 * pred_test_denorm_np_dl2\n",
    "mape_ensemble = np.mean(np.abs((y_test_denorm_np - ensemble_prediction) / np.abs(y_test_denorm_np))) * 100\n",
    "mdape_ensemble = np.median(np.abs((y_test_denorm_np - ensemble_prediction) / np.abs(y_test_denorm_np))) * 100\n",
    "\n",
    "# Calculate the difference\n",
    "difference_ensemble = y_test_denorm_np - ensemble_prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAPE: 3.849160298705101\n",
      "MdAPE: 1.6662705689668655\n",
      "Difference: [ 1.7260960e+06  5.0040800e+05 -9.4400000e+02 -1.1279296e+07\n",
      "  2.5038400e+05 -1.0240000e+03  5.0848160e+06  4.7190400e+06\n",
      "  5.2800000e+02 -3.2168000e+06  8.1463680e+06 -9.8080000e+03\n",
      "  2.1986560e+07 -6.6765600e+05 -1.0304000e+04]\n",
      "Prediction: [2.0439579e+08 1.0946664e+08 2.1000000e+08 1.6031118e+08 1.0550638e+08\n",
      " 2.1000000e+08 2.1011222e+08 2.1335728e+08 2.1000000e+08 1.7755443e+08\n",
      " 2.0619104e+08 2.1000000e+08 1.9552514e+08 4.7590044e+07 2.1000000e+08]\n",
      "Original: [2.100e+08 1.050e+08 2.100e+08 1.575e+08 1.050e+08 2.100e+08 2.100e+08\n",
      " 2.100e+08 2.100e+08 1.575e+08 2.100e+08 2.100e+08 2.100e+08 5.250e+07\n",
      " 2.100e+08] 16385\n"
     ]
    }
   ],
   "source": [
    "sample = 123\n",
    "print(\"MAPE:\", mape_ensemble)\n",
    "print(\"MdAPE:\", mdape_ensemble)\n",
    "print(\"Difference:\", difference_ensemble[sample])\n",
    "print(\"Prediction:\", y_pred_denorm[sample].numpy())\n",
    "print(\"Original:\", y_test_denorm[sample].numpy(), len(y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensemble model\n",
    "# Generate predictions from the LightGBM models on the training data\n",
    "y_pred_train_lgbm = np.column_stack([model.predict(X_train) for model in models])\n",
    "\n",
    "# Generate predictions from the Deep Learning model on the training data\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    y_pred_train_dl = model(x_categ[:X_test.shape[0]],X_train_tensor).cpu().numpy()\n",
    "\n",
    "# Stack the predictions together to form a new feature set for the second-level model\n",
    "X_train_level2 = np.column_stack([y_pred_train_lgbm, y_pred_train_dl])\n",
    "\n",
    "# Convert the labels to a numpy array\n",
    "y_train_np = y_train_tensor.cpu().numpy()\n",
    "\n",
    "# Train the second-level model\n",
    "dtrain = xgb.DMatrix(X_train_level2, label=y_train_np)\n",
    "params = {'objective': 'reg:squarederror', 'eval_metric': 'mae', 'eta': 0.1, 'max_depth': 6}\n",
    "model_level2 = xgb.train(params, dtrain, num_boost_round=1000)\n",
    "\n",
    "# Generate predictions from the LightGBM models on the test data\n",
    "y_pred_test_lgbm = np.column_stack([model.predict(X_test) for model in models])\n",
    "\n",
    "# Generate predictions from the Deep Learning model on the test data\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    y_pred_test_dl = model(x_categ[:X_test.shape[0]],X_test_tensor).cpu().numpy()\n",
    "\n",
    "# Stack the predictions together to form a new feature set for the second-level model\n",
    "X_test_level2 = np.column_stack([y_pred_test_lgbm, y_pred_test_dl])\n",
    "\n",
    "# Generate the final prediction from the second-level model\n",
    "dtest = xgb.DMatrix(X_test_level2)\n",
    "y_pred_final = model_level2.predict(dtest)\n",
    "\n",
    "# Denormalize the final prediction\n",
    "y_pred_final_denorm = denormalize_data(torch.tensor(y_pred_final), E_return, E_return2, scaling_type).numpy()\n",
    "\n",
    "# MAPE\n",
    "mape_ensemble = np.mean(np.abs((y_test_denorm_np - y_pred_final_denorm) / np.abs(y_test_denorm_np))) * 100\n",
    "# MdAPE\n",
    "mdape_ensemble = np.median(np.abs((y_test_denorm_np - y_pred_final_denorm) / np.abs(y_test_denorm_np))) * 100\n",
    "\n",
    "# Calculate the difference\n",
    "difference_ensemble = y_test_denorm_np - y_pred_final_denorm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call the function for the first time and save a pickle\n",
    "y_pred_final = generate_predictions_and_train_second_level_model(X_train, y_train, X_test, y_test, num_models=len(models), model=model, x_categ=x_categ, \n",
    "                                                                 X_train_tensor=X_train_tensor, X_test_tensor=X_test_tensor, E_return=E_return, \n",
    "                                                                 E_return2=E_return2, scaling_type=scaling_type, name=\"xgb_model\", train_second_level_model=True,\n",
    "                                                                 models=models)\n",
    "\n",
    "# Denormalize the final prediction\n",
    "y_pred_final_denorm = denormalize_data(torch.tensor(y_pred_final), E_return, E_return2, scaling_type).numpy()\n",
    "\n",
    "# MAPE\n",
    "mape_ensemble = np.mean(np.abs((E_test_denorm_np - y_pred_final_denorm) / np.abs(E_test_denorm_np))) * 100\n",
    "# MdAPE\n",
    "mdape_ensemble = np.median(np.abs((E_test_denorm_np - y_pred_final_denorm) / np.abs(E_test_denorm_np))) * 100\n",
    "\n",
    "# Calculate the difference\n",
    "difference_ensemble = E_test_denorm_np - y_pred_final_denorm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAPE: 1.5224144794046879\n",
      "MdAPE: 0.35021714866161346\n",
      "Difference: [ 2.2896000e+04  9.8304000e+04  4.8000000e+01 -5.7658240e+06\n",
      "  1.4878688e+07  4.8000000e+01  5.6284800e+05 -2.4715840e+06\n",
      "  4.8000000e+01  5.2822400e+05 -2.4139200e+05  4.8000000e+01\n",
      " -2.3846400e+06 -8.9351200e+05  4.8000000e+01]\n",
      "Prediction from ensemble: [2.0997710e+08 1.0490170e+08 2.0999995e+08 1.6326582e+08 9.0121312e+07\n",
      " 2.0999995e+08 2.0943715e+08 2.1247158e+08 2.0999995e+08 1.5697178e+08\n",
      " 2.1024139e+08 2.0999995e+08 2.1238464e+08 5.3393512e+07 2.0999995e+08] 16385\n",
      "Original: [2.100e+08 1.050e+08 2.100e+08 1.575e+08 1.050e+08 2.100e+08 2.100e+08\n",
      " 2.100e+08 2.100e+08 1.575e+08 2.100e+08 2.100e+08 2.100e+08 5.250e+07\n",
      " 2.100e+08] 16385\n"
     ]
    }
   ],
   "source": [
    "# Call the function\n",
    "y_pred_final = generate_predictions_and_train_second_level_model(X_train, y_train, X_test, y_test, num_models=len(models), model=model, x_categ=x_categ, \n",
    "                                                                 X_train_tensor=X_train_tensor, X_test_tensor=X_test_tensor, E_return=E_return, \n",
    "                                                                 E_return2=E_return2, scaling_type=scaling_type, name=\"xgb_model\", train_second_level_model=False, \n",
    "                                                                 models=models, second_name=\"xgb_model_level2_8files\")\n",
    "\n",
    "# Denormalize the final prediction\n",
    "y_pred_final_denorm = denormalize_data(torch.tensor(y_pred_final), E_return, E_return2, scaling_type).numpy()\n",
    "\n",
    "# MAPE\n",
    "mape_ensemble = np.mean(np.abs((E_test_denorm_np - y_pred_final_denorm) / np.abs(E_test_denorm_np))) * 100\n",
    "# MdAPE\n",
    "mdape_ensemble = np.median(np.abs((E_test_denorm_np - y_pred_final_denorm) / np.abs(E_test_denorm_np))) * 100\n",
    "\n",
    "# Calculate the difference\n",
    "difference_ensemble = E_test_denorm_np - y_pred_final_denorm\n",
    "\n",
    "print(\"MAPE:\", mape_ensemble)\n",
    "print(\"MdAPE:\", mdape_ensemble)\n",
    "print(\"Difference:\", difference_ensemble[123])\n",
    "print(\"Prediction from ensemble:\", y_pred_final_denorm[123], len(y_pred_final_denorm))\n",
    "print(\"Original:\", y_test_denorm[123].numpy(), len(y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAPE: 1.1730022728443146\n",
      "MdAPE: 0.0678399985190481\n",
      "Difference: [-2.5177280e+06  4.4444000e+06 -4.1616000e+04 -1.1686656e+07\n",
      "  2.1532800e+05 -6.5600000e+04  5.8907200e+06  1.1599616e+07\n",
      "  6.4960000e+03 -1.1539632e+07  5.7415040e+06  4.6320000e+04\n",
      "  1.6516192e+07  5.4013000e+06  1.4304000e+04]\n",
      "Prediction from ensemble: [2.1003094e+08 1.0501678e+08 2.0999995e+08 1.5844226e+08 1.0523671e+08\n",
      " 2.0999995e+08 2.0976867e+08 2.1017341e+08 2.0999995e+08 1.5790074e+08\n",
      " 2.1009451e+08 2.0999995e+08 2.0946392e+08 5.2994408e+07 2.0999995e+08] 16385\n",
      "Original: [2.100e+08 1.050e+08 2.100e+08 1.575e+08 1.050e+08 2.100e+08 2.100e+08\n",
      " 2.100e+08 2.100e+08 1.575e+08 2.100e+08 2.100e+08 2.100e+08 5.250e+07\n",
      " 2.100e+08] 16385\n"
     ]
    }
   ],
   "source": [
    "# Using a different DL model\n",
    "print(\"MAPE:\", mape_ensemble)\n",
    "print(\"MdAPE:\", mdape_ensemble)\n",
    "print(\"Difference:\", difference_ensemble[123])\n",
    "print(\"Prediction from ensemble:\", y_pred_final_denorm[123], len(y_pred_final_denorm))\n",
    "print(\"Original:\", y_test_denorm[123].numpy(), len(y_pred))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ds",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
