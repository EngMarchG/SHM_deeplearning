{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-08T12:34:47.998272Z",
     "iopub.status.busy": "2025-02-08T12:34:47.997954Z",
     "iopub.status.idle": "2025-02-08T12:35:05.966146Z",
     "shell.execute_reply": "2025-02-08T12:35:05.964769Z",
     "shell.execute_reply.started": "2025-02-08T12:34:47.998248Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install einops\n",
    "!pip install tab-transformer-pytorch\n",
    "!pip install pytorch-tabnet\n",
    "!pip install hyper-connections\n",
    "!pip install ipyfilechooser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-08T12:35:06.439656Z",
     "iopub.status.busy": "2025-02-08T12:35:06.439354Z",
     "iopub.status.idle": "2025-02-08T12:35:06.442844Z",
     "shell.execute_reply": "2025-02-08T12:35:06.442045Z",
     "shell.execute_reply.started": "2025-02-08T12:35:06.439637Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "import shutil\n",
    "import matplotlib.pyplot as plt\n",
    "from huggingface_hub import snapshot_download\n",
    "# from google.colab import userdata\n",
    "from kaggle_secrets import UserSecretsClient\n",
    "\n",
    "user_secrets = UserSecretsClient()\n",
    "\n",
    "\n",
    "\n",
    "repo_id = \"your_repo_id\"\n",
    "repo_type = \"dataset\"  # Specify the repository type (model or dataset)\n",
    "destination_path = \"SMH\"\n",
    "# token = \"\"  # Replace with your actual Hugging Face token\n",
    "# token = userdata.get(\"HF_TOKEN\") # Token as a secret on colab\n",
    "token = user_secrets.get_secret(\"HF_token\") # Token for kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "\n",
    "# # DOWNLOAD THE FOLDERS FILE FROM KAGGLE (1st run)\n",
    "# if not os.path.exists(destination_path):\n",
    "#     os.makedirs(destination_path)\n",
    "\n",
    "# # Download the entire dataset to the specified destination path\n",
    "# snapshot_download(repo_id, repo_type=repo_type, revision=\"main\", token=token, local_dir=destination_path)\n",
    "\n",
    "# # Move all files and folders in the 'SMH' directory to the current working directory\n",
    "# for item in os.listdir(destination_path):\n",
    "#     item_path = os.path.join(destination_path, item)\n",
    "#     if os.path.isdir(item_path):\n",
    "#         shutil.move(item_path, os.getcwd())\n",
    "#         print(f\"Moved '{item}' folder to the current working directory.\")\n",
    "#     else:\n",
    "#         shutil.move(item_path, os.getcwd())\n",
    "#         print(f\"Moved '{item}' file to the current working directory.\")\n",
    "\n",
    "# print(\"Finished moving files and folders.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bridge Geometry for later plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-11T21:41:18.211232Z",
     "iopub.status.busy": "2025-02-11T21:41:18.210872Z",
     "iopub.status.idle": "2025-02-11T21:41:18.220632Z",
     "shell.execute_reply": "2025-02-11T21:41:18.219719Z",
     "shell.execute_reply.started": "2025-02-11T21:41:18.211202Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from utils.truss_geometric import *\n",
    "from utils.truss_constraints import *\n",
    "from utils.truss_helpers import *\n",
    "\n",
    "span = 25\n",
    "angle = 45\n",
    "beam_length = 2.5\n",
    "skip_rod = []\n",
    "truss_mode = \"warren\" # warren, pratt, howe\n",
    "\n",
    "# Calculation of bridge geometry\n",
    "height, spacing, diag = calculate_bridge(span, angle, beam_length, truss_mode)\n",
    "n_columns, n_nod_tot, n_rods, n_beams, n_ele_tot, n_bot_beams = calculate_essential_elements(span, spacing, truss_mode, skip_rod)\n",
    "\n",
    "# Recalculates the skip_rod to match the truss design\n",
    "skip_rod = truss_design(n_bot_beams, n_rods, truss_mode)\n",
    "n_columns, n_nod_tot, n_rods, n_beams, n_ele_tot, n_bot_beams = calculate_essential_elements(span, spacing, truss_mode, skip_rod)\n",
    "\n",
    "\n",
    "# Calculation of nodal coordinates and construction of par, pel matrices and element node indices\n",
    "# Building priorty: Beams -> Columns -> Rods\n",
    "nodal_coord, par, pel, ele_nod, n_par_tot = calculate_element_node(span, spacing, height, n_dim, n_par_nod, truss_mode, skip_rod)\n",
    "\n",
    "\n",
    "# Seperating nodal coordinates into x and y coordinates\n",
    "X = np.zeros(n_nod_tot, dtype=float)\n",
    "Y = np.zeros(n_nod_tot, dtype=float)\n",
    "X[:] = nodal_coord[:, 0]\n",
    "Y[:] = nodal_coord[:, 1]\n",
    "\n",
    "# # -------- ELEMENT CHARACTERIZING DATA --------\n",
    "h = np.zeros(n_ele_tot, dtype=np.float32)\n",
    "J = np.zeros(n_ele_tot, dtype=np.float32)\n",
    "A = np.zeros(n_ele_tot, dtype=np.float32)\n",
    "beta = np.zeros(n_ele_tot,dtype=np.float32)\n",
    "ro = np.zeros(n_ele_tot,dtype=np.float32)\n",
    "E = np.zeros(n_ele_tot, dtype=np.float32)\n",
    "\n",
    "\n",
    "# # -------- ELEMENT PROPERTIES --------\n",
    "J, A, h, beta, ro, E, G = calculate_element_properties(n_columns, n_beams, diag, spacing, height, J, A, h, beta, ro, E, X, Y, ele_nod, shear_mod,\n",
    "                                                 width_properties, height_properties, unit_weight_properties, elastic_mod_properties)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DL Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-11T23:00:47.674948Z",
     "iopub.status.busy": "2025-02-11T23:00:47.674655Z",
     "iopub.status.idle": "2025-02-11T23:00:47.681646Z",
     "shell.execute_reply": "2025-02-11T23:00:47.680488Z",
     "shell.execute_reply.started": "2025-02-11T23:00:47.674926Z"
    },
    "id": "BdXMBSz6o6Bn",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import copy\n",
    "import time\n",
    "import torch\n",
    "import random\n",
    "import importlib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from collections import deque\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau, CosineAnnealingWarmRestarts, OneCycleLR\n",
    "\n",
    "import tab_transformer_copy\n",
    "import matplotlib.pyplot as plt\n",
    "from tab_transformer_copy import TabTransformer_edit\n",
    "from sklearn.model_selection import train_test_split\n",
    "from pytorch_tabnet.tab_model import TabNetRegressor\n",
    "from tab_transformer_pytorch import TabTransformer, FTTransformer\n",
    "from helper_analysis import normalize_data, load_and_combine_data, format_array, load_and_normalize_data_tensor, denormalize_data, min_max_scale\n",
    "\n",
    "\n",
    "folder_path = \"/kaggle/working/combinedOutputs\" #combinedOutputs or Outputs\n",
    "epochs_dl = 6000 # For deep learning models\n",
    "epochs_ml = 200 # For machine learning models\n",
    "max_files = 1  # Combined datasets are now used (Previously concat 6 = 5 damaged, 8 = 6 damaged)\n",
    "seed = 42\n",
    "scaling_type = \"minmax\"  # \"minmax\" or any other for z-score\n",
    "scaling_type2 = \"min_max\" # \"z-score\"\n",
    "\n",
    "\n",
    "# Define the ratio for the train-test split\n",
    "train_ratio = 0.8\n",
    "\n",
    "# Check if CUDA is available and set the device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n",
    "\n",
    "# Define your L1 regularization strength\n",
    "l1 = 0.0001\n",
    "l2 = 1e-5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-11T23:00:48.471756Z",
     "iopub.status.busy": "2025-02-11T23:00:48.471498Z",
     "iopub.status.idle": "2025-02-11T23:01:07.364871Z",
     "shell.execute_reply": "2025-02-11T23:01:07.363842Z",
     "shell.execute_reply.started": "2025-02-11T23:00:48.471734Z"
    },
    "id": "wCeLIgQ0rlOu",
    "outputId": "f3857fb6-b895-4e5d-bfdc-e8a305acce9e",
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([128764, 39]),\n",
       " torch.Size([128764, 39]),\n",
       " torch.Size([128764, 39]),\n",
       " torch.Size([128764, 39]),\n",
       " torch.Size([128764, 156]),\n",
       " torch.Size([128764, 156]))"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#@title Loading the data and normalizing\n",
    "# Load and normalize the data\n",
    "file_outputs = \"_combo_output1234\" # All possible values to choose1234, 12345, 4567, 67\n",
    "E_tensor, E_return, E_return2 = load_and_normalize_data_tensor(folder_path, f\"E{file_outputs}\", max_files=max_files, scaling_type=scaling_type2)\n",
    "A_tensor, _, _ = load_and_normalize_data_tensor(folder_path, f\"A{file_outputs}\", max_files=max_files, scaling_type=scaling_type2)\n",
    "J_tensor, _, _ = load_and_normalize_data_tensor(folder_path, f\"J{file_outputs}\", max_files=max_files, scaling_type=scaling_type2)\n",
    "h_tensor, h_return, h_return2 = load_and_normalize_data_tensor(folder_path, f\"h{file_outputs}\", max_files=max_files, scaling_type=scaling_type2)\n",
    "X_tensor, _, _ = load_and_normalize_data_tensor(folder_path, f\"X{file_outputs}\", max_files=max_files, scaling_type=scaling_type2)\n",
    "Y_tensor, _, _ = load_and_normalize_data_tensor(folder_path, f\"Y{file_outputs}\", max_files=max_files, scaling_type=scaling_type2)\n",
    "w_tensor, _, _ = load_and_normalize_data_tensor(folder_path, f\"w{file_outputs}\", max_files=max_files, scaling_type=scaling_type2)\n",
    "T_tensor, _, _ = load_and_normalize_data_tensor(folder_path, f\"T{file_outputs}\", max_files=max_files, scaling_type=scaling_type2)\n",
    "\n",
    "# After loading the data. Make a new data tensor of size (n_samples, n_features) taking A_tensor as reference\n",
    "# Starting from the 2nd index and moving by 3 beta (the angle) will be 0 and the rest will be 90 degrees (pi/2)\n",
    "E_tensor.shape, A_tensor.shape, J_tensor.shape, h_tensor.shape, X_tensor.shape, w_tensor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-11T23:01:07.366406Z",
     "iopub.status.busy": "2025-02-11T23:01:07.366090Z",
     "iopub.status.idle": "2025-02-11T23:01:07.395006Z",
     "shell.execute_reply": "2025-02-11T23:01:07.394203Z",
     "shell.execute_reply.started": "2025-02-11T23:01:07.366377Z"
    },
    "id": "_OodA1urJFzp",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "beta_tensor = torch.zeros(A_tensor.shape)\n",
    "\n",
    "for i in range(A_tensor.shape[1]):\n",
    "    if i <= 18:\n",
    "        beta_tensor[:, i] = 0\n",
    "    else:\n",
    "        beta_tensor[:, i] = np.pi/4\n",
    "\n",
    "beta_tensor, _, _ = min_max_scale(beta_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-08T09:00:40.640073Z",
     "iopub.status.busy": "2025-02-08T09:00:40.639777Z",
     "iopub.status.idle": "2025-02-08T09:00:40.644193Z",
     "shell.execute_reply": "2025-02-08T09:00:40.643397Z",
     "shell.execute_reply.started": "2025-02-08T09:00:40.640046Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# # Take only 60k sample if you train a subset for faster inference\n",
    "# E_tensor = E_tensor[:60000]\n",
    "# A_tensor = A_tensor[:60000]\n",
    "# J_tensor = J_tensor[:60000]\n",
    "# h_tensor = h_tensor[:60000]\n",
    "# X_tensor = X_tensor[:60000]\n",
    "# Y_tensor = Y_tensor[:60000]\n",
    "# w_tensor = w_tensor[:60000]\n",
    "# T_tensor = T_tensor[:60000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialize the data and model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-11T23:01:07.397485Z",
     "iopub.status.busy": "2025-02-11T23:01:07.397199Z",
     "iopub.status.idle": "2025-02-11T23:01:08.116420Z",
     "shell.execute_reply": "2025-02-11T23:01:08.115574Z",
     "shell.execute_reply.started": "2025-02-11T23:01:07.397459Z"
    },
    "id": "9xSoL-m4c3Ez",
    "outputId": "86e1de5a-be5f-4053-e236-1053c4cd4980",
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([128764, 39])"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "\n",
    "n_ele = A_tensor.shape[1]\n",
    "n_ele2 = n_ele*2\n",
    "n_ele3 = n_ele*3\n",
    "\n",
    "# Split X_data and w_data into three separate sets of features\n",
    "X1_tensor = X_tensor[:, :n_ele].clone().detach()\n",
    "X2_tensor = X_tensor[:, n_ele:n_ele2].clone().detach()\n",
    "X3_tensor = X_tensor[:, n_ele2:n_ele3].clone().detach()\n",
    "Y1_tensor = Y_tensor[:, :n_ele].clone().detach()\n",
    "Y2_tensor = Y_tensor[:, n_ele:n_ele2].clone().detach()\n",
    "Y3_tensor = Y_tensor[:, n_ele2:n_ele3].clone().detach()\n",
    "w1_tensor = w_tensor[:, :n_ele].clone().detach()\n",
    "w2_tensor = w_tensor[:, n_ele:n_ele2].clone().detach()\n",
    "w3_tensor = w_tensor[:, n_ele2:n_ele3].clone().detach()\n",
    "\n",
    "# Extra 4th mode due to added complexity\n",
    "X4_tensor = X_tensor[:, n_ele3:].clone().detach()\n",
    "Y4_tensor = Y_tensor[:, n_ele3:].clone().detach()\n",
    "w4_tensor = w_tensor[:, n_ele3:].clone().detach()\n",
    "\n",
    "# Concatenate all the data to be fed to the model\n",
    "x_cont = torch.cat((A_tensor, J_tensor, h_tensor, w1_tensor, w2_tensor, w3_tensor, w4_tensor, X1_tensor, \n",
    "                    X2_tensor, X3_tensor, X4_tensor, Y1_tensor, Y2_tensor, Y3_tensor, Y4_tensor), dim=1)\n",
    "# Not used (categorical data)\n",
    "x_categ = torch.empty((x_cont.shape[0], 0))\n",
    "\n",
    "# To normalize in the TabTransformer instead\n",
    "cont_mean_std = torch.randn(x_cont.shape[1], 2)\n",
    "\n",
    "# Split the data into a training set and a test set and get the indices\n",
    "x_cont_train, x_cont_test, E_train, E_test, train_indices, test_indices = train_test_split(\n",
    "    x_cont, E_tensor, range(E_tensor.shape[0]), train_size=train_ratio, random_state=seed\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "# Move the data to the device\n",
    "x_categ = x_categ.to(device) # This is the only tensor with categorical data\n",
    "x_cont_train = x_cont_train.to(device)\n",
    "x_cont_test = x_cont_test.to(device)\n",
    "E_train = E_train.to(device)\n",
    "E_test = E_test.to(device)\n",
    "Y3_tensor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-11T23:01:08.117731Z",
     "iopub.status.busy": "2025-02-11T23:01:08.117403Z",
     "iopub.status.idle": "2025-02-11T23:01:08.727191Z",
     "shell.execute_reply": "2025-02-11T23:01:08.726327Z",
     "shell.execute_reply.started": "2025-02-11T23:01:08.117701Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Original\n",
    "model = TabTransformer_edit(\n",
    "    categories=(),\n",
    "    num_continuous=x_cont.shape[1],   # continuous features\n",
    "    dim=x_cont.shape[1],              # dimension, paper set at 32\n",
    "    dim_head=16,                      # dimension of each head, paper set at 16\n",
    "    dim_out=E_tensor.shape[1],        # 15 due to the shape of E_data\n",
    "    depth=3,                          # depth, paper recommended 6 (9 worked better by .02%)\n",
    "    heads=15,                         # heads, paper recommends 8\n",
    "    attn_dropout=0.4,                 # post-attention dropout\n",
    "    ff_dropout=0.4,                   # feed forward dropout\n",
    "    mlp_hidden_mults=(4,2),           # relative multiples of each hidden dimension of the last mlp to logits\n",
    "    mlp_act=nn.ReLU6(),               # activation for final mlp, defaults to relu, but could be anything else (selu, ELU, PReLU, Tanh, Sigmoid)\n",
    "    mlp_dropout=0.05,\n",
    "    #mlp_use_residual=True,\n",
    "    #continuous_mean_std=cont_mean_std # normalize the continuous values before layer norm\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "_kg_hide-input": true,
    "execution": {
     "iopub.execute_input": "2025-02-11T23:01:08.728536Z",
     "iopub.status.busy": "2025-02-11T23:01:08.728292Z",
     "iopub.status.idle": "2025-02-11T23:01:08.741202Z",
     "shell.execute_reply": "2025-02-11T23:01:08.740368Z",
     "shell.execute_reply.started": "2025-02-11T23:01:08.728517Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class BalancedLoss(nn.Module):\n",
    "    def forward(self, pred, target):\n",
    "        abs_error = torch.abs(pred - target)\n",
    "        rel_error = abs_error / (torch.abs(target) + 1e-8)\n",
    "        return torch.mean(abs_error + rel_error)\n",
    "\n",
    "class FocalRegressionLoss(nn.Module):\n",
    "    def __init__(self, alpha=2.0, gamma=2.0):\n",
    "        super().__init__()\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "\n",
    "    def forward(self, pred, target):\n",
    "        diff = torch.abs(pred - target)\n",
    "        loss = self.alpha * torch.pow(diff, self.gamma)\n",
    "        return loss.mean()\n",
    "    \n",
    "class LogCoshLoss(nn.Module):\n",
    "    def forward(self, pred, target):\n",
    "        return torch.mean(torch.log(torch.cosh(pred - target)))\n",
    "    \n",
    "class QuantileLoss(nn.Module):\n",
    "    def __init__(self, quantile=0.5):\n",
    "        super().__init__()\n",
    "        self.quantile = quantile\n",
    "\n",
    "    def forward(self, pred, target):\n",
    "        diff = target - pred\n",
    "        return torch.mean(torch.max(self.quantile * diff, (self.quantile - 1) * diff)), None\n",
    "\n",
    "class QuantileLoss_modified(nn.Module):\n",
    "    def __init__(self, quantile=0.5, threshold_weight=2):\n",
    "        super().__init__()\n",
    "        self.quantile = quantile\n",
    "        self.threshold_weight = threshold_weight\n",
    "    \n",
    "    def forward(self, pred, target):\n",
    "        diff = target - pred\n",
    "        loss = torch.max(self.quantile * diff, (self.quantile - 1) * diff)\n",
    "        \n",
    "        # Calculate element-wise quantile threshold\n",
    "        ele_quantile = target * self.quantile\n",
    "        \n",
    "        # Determine how many diffs are above the quantile threshold\n",
    "        above_threshold = (diff.abs() > ele_quantile).float()\n",
    "        \n",
    "        # Apply additional weight to losses above the threshold\n",
    "        weighted_loss = loss * torch.min((1 + (self.threshold_weight - 1) * above_threshold), torch.tensor(self.threshold_weight * 4))\n",
    "        \n",
    "        return torch.mean(weighted_loss), above_threshold.mean()\n",
    "\n",
    "\n",
    "class NormalizedCombinedRegressionLoss(nn.Module):\n",
    "    def __init__(self, alpha=0.3, beta=0.1, gamma=0.1, delta=0.3, zeta=0.2, epsilon=1e-11, start_epoch=20, ramp_epochs=800, quantile=0.1, modify=False):\n",
    "        super().__init__()\n",
    "        self.alpha = alpha\n",
    "        self.beta = beta\n",
    "        self.gamma = gamma\n",
    "        self.delta = delta\n",
    "        self.zeta = zeta\n",
    "        self.epsilon = epsilon\n",
    "        self.start_epoch = start_epoch\n",
    "        self.ramp_epochs = ramp_epochs\n",
    "        self.huber_loss = nn.HuberLoss(delta=1.0)\n",
    "        self.mse_loss = nn.MSELoss()\n",
    "        self.mae_loss = nn.L1Loss()\n",
    "        self.logcosh_loss = LogCoshLoss()\n",
    "        self.quantile_loss = QuantileLoss(quantile=quantile)\n",
    "        if modify:\n",
    "            self.quantile_loss = QuantileLoss_modified(quantile=quantile)\n",
    "        self.current_epoch = 0\n",
    "\n",
    "    def forward(self, pred, target):\n",
    "        huber = self.huber_loss(pred, target)\n",
    "        mse = self.mse_loss(pred, target)\n",
    "        mae = self.mae_loss(pred, target)\n",
    "        logcosh = self.logcosh_loss(pred, target)\n",
    "        quantile, _ = self.quantile_loss(pred, target)\n",
    "        \n",
    "        # Normalize losses\n",
    "        norm_factor = torch.mean(torch.abs(target) + self.epsilon)\n",
    "        normalized_huber = huber / norm_factor\n",
    "        normalized_mse = torch.sqrt(mse) / norm_factor\n",
    "        normalized_mae = mae / norm_factor\n",
    "        normalized_logcosh = logcosh / norm_factor\n",
    "        normalized_quantile = quantile / norm_factor\n",
    "\n",
    "        total_loss = (\n",
    "            self.alpha * normalized_huber + \n",
    "            self.beta * normalized_mse + \n",
    "            self.gamma * normalized_mae + \n",
    "            self.zeta * normalized_quantile\n",
    "        )\n",
    "\n",
    "        # Apply LogCosh loss after start_epoch\n",
    "        if self.current_epoch >= self.start_epoch:\n",
    "            ramp_factor = min(1.0, (self.current_epoch - self.start_epoch + 1) / self.ramp_epochs)\n",
    "            total_loss += ramp_factor * self.delta * normalized_logcosh\n",
    "        \n",
    "        return total_loss + self.epsilon\n",
    "\n",
    "    def update_epoch(self, epoch):\n",
    "        self.current_epoch = epoch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary of findings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-02-06T16:44:21.307537Z",
     "iopub.status.idle": "2025-02-06T16:44:21.307932Z",
     "shell.execute_reply": "2025-02-06T16:44:21.307735Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Original run during optimizing parameters 1->\n",
    "Results: Test -> Unseen test \n",
    "Original: 4.0 -> 8.5% 50 mins \n",
    "Original_edit: 4.2 -> 7.8% 90 mins\n",
    "Original_edit_combined: 2.6 - > 7.1% 120 mins\n",
    "\n",
    "Original_edit_combined_train: 2.0 -> 6.7% 130 mins 9.1k epoch # 0.7 split\n",
    "Original_edit_combined_train_elepunish: 1.8 -> 6.1% 110 mins 8.3k epoch # 0.7 split and new loss function\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Dataset 1->4 damaged full data ~= 130k combinations\n",
    "run 1 original loss function: 4.74% MAPE and 2.52% MDAPE\n",
    "run 2 combined loss function: 2.436% Mape and 1.25% MDAPE\n",
    "run 3 combined loss function optimized: 1.94% Mape and 1.067% MDAPE\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-07T22:18:18.301323Z",
     "iopub.status.busy": "2025-02-07T22:18:18.301042Z",
     "iopub.status.idle": "2025-02-08T00:01:16.800861Z",
     "shell.execute_reply": "2025-02-08T00:01:16.799905Z",
     "shell.execute_reply.started": "2025-02-07T22:18:18.301301Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Train Loss: 0.4633, Val Loss: 0.4365\n",
      "Learning rate: 0.000100\n",
      "Epoch 10, Train Loss: 0.1496, Val Loss: 0.3321\n",
      "Learning rate: 0.000100\n",
      "Epoch 20, Train Loss: 0.0446, Val Loss: 0.1951\n",
      "Learning rate: 0.000100\n",
      "Epoch 30, Train Loss: 0.0374, Val Loss: 0.1298\n",
      "Learning rate: 0.000100\n",
      "Epoch 40, Train Loss: 0.0297, Val Loss: 0.1101\n",
      "Learning rate: 0.000100\n",
      "Epoch 50, Train Loss: 0.0267, Val Loss: 0.1116\n",
      "Learning rate: 0.000100\n",
      "Epoch 60, Train Loss: 0.0245, Val Loss: 0.0938\n",
      "Learning rate: 0.000100\n",
      "Epoch 70, Train Loss: 0.0227, Val Loss: 0.0704\n",
      "Learning rate: 0.000100\n",
      "Epoch 80, Train Loss: 0.0210, Val Loss: 0.0507\n",
      "Learning rate: 0.000100\n",
      "Epoch 90, Train Loss: 0.0197, Val Loss: 0.0379\n",
      "Learning rate: 0.000100\n",
      "Epoch 100, Train Loss: 0.0186, Val Loss: 0.0310\n",
      "Learning rate: 0.000100\n",
      "Epoch 110, Train Loss: 0.0176, Val Loss: 0.0265\n",
      "Learning rate: 0.000100\n",
      "Epoch 120, Train Loss: 0.0167, Val Loss: 0.0236\n",
      "Learning rate: 0.000100\n",
      "Epoch 130, Train Loss: 0.0159, Val Loss: 0.0221\n",
      "Learning rate: 0.000100\n",
      "Epoch 140, Train Loss: 0.0151, Val Loss: 0.0242\n",
      "Learning rate: 0.000100\n",
      "Epoch 150, Train Loss: 0.0144, Val Loss: 0.0203\n",
      "Learning rate: 0.000100\n",
      "Epoch 160, Train Loss: 0.0138, Val Loss: 0.0218\n",
      "Learning rate: 0.000100\n",
      "Epoch 170, Train Loss: 0.0132, Val Loss: 0.0197\n",
      "Learning rate: 0.000100\n",
      "Epoch 180, Train Loss: 0.0127, Val Loss: 0.0180\n",
      "Learning rate: 0.000100\n",
      "Epoch 190, Train Loss: 0.0122, Val Loss: 0.0223\n",
      "Learning rate: 0.000100\n",
      "Epoch 200, Train Loss: 0.0118, Val Loss: 0.0228\n",
      "Learning rate: 0.000100\n",
      "Epoch 210, Train Loss: 0.0114, Val Loss: 0.0194\n",
      "Learning rate: 0.000098\n",
      "Epoch 220, Train Loss: 0.0109, Val Loss: 0.0162\n",
      "Learning rate: 0.000098\n",
      "Epoch 230, Train Loss: 0.0106, Val Loss: 0.0175\n",
      "Learning rate: 0.000098\n",
      "Epoch 240, Train Loss: 0.0103, Val Loss: 0.0189\n",
      "Learning rate: 0.000098\n",
      "Epoch 250, Train Loss: 0.0099, Val Loss: 0.0143\n",
      "Learning rate: 0.000098\n",
      "Epoch 260, Train Loss: 0.0096, Val Loss: 0.0142\n",
      "Learning rate: 0.000098\n",
      "Epoch 270, Train Loss: 0.0094, Val Loss: 0.0204\n",
      "Learning rate: 0.000098\n",
      "Epoch 280, Train Loss: 0.0092, Val Loss: 0.0167\n",
      "Learning rate: 0.000096\n",
      "Epoch 290, Train Loss: 0.0089, Val Loss: 0.0178\n",
      "Learning rate: 0.000096\n",
      "Epoch 300, Train Loss: 0.0087, Val Loss: 0.0147\n",
      "Learning rate: 0.000094\n",
      "Epoch 310, Train Loss: 0.0085, Val Loss: 0.0134\n",
      "Learning rate: 0.000094\n",
      "Epoch 320, Train Loss: 0.0083, Val Loss: 0.0136\n",
      "Learning rate: 0.000092\n",
      "Epoch 330, Train Loss: 0.0081, Val Loss: 0.0137\n",
      "Learning rate: 0.000092\n",
      "Epoch 340, Train Loss: 0.0079, Val Loss: 0.0132\n",
      "Learning rate: 0.000092\n",
      "Epoch 350, Train Loss: 0.0077, Val Loss: 0.0117\n",
      "Learning rate: 0.000090\n",
      "Epoch 360, Train Loss: 0.0076, Val Loss: 0.0101\n",
      "Learning rate: 0.000090\n",
      "Epoch 370, Train Loss: 0.0074, Val Loss: 0.0125\n",
      "Learning rate: 0.000090\n",
      "Epoch 380, Train Loss: 0.0073, Val Loss: 0.0145\n",
      "Learning rate: 0.000089\n",
      "Epoch 390, Train Loss: 0.0071, Val Loss: 0.0089\n",
      "Learning rate: 0.000089\n",
      "Epoch 400, Train Loss: 0.0070, Val Loss: 0.0098\n",
      "Learning rate: 0.000089\n",
      "Epoch 410, Train Loss: 0.0069, Val Loss: 0.0087\n",
      "Learning rate: 0.000089\n",
      "Epoch 420, Train Loss: 0.0068, Val Loss: 0.0094\n",
      "Learning rate: 0.000089\n",
      "Epoch 430, Train Loss: 0.0067, Val Loss: 0.0099\n",
      "Learning rate: 0.000089\n",
      "Epoch 440, Train Loss: 0.0066, Val Loss: 0.0104\n",
      "Learning rate: 0.000089\n",
      "Epoch 450, Train Loss: 0.0065, Val Loss: 0.0096\n",
      "Learning rate: 0.000087\n",
      "Epoch 460, Train Loss: 0.0064, Val Loss: 0.0077\n",
      "Learning rate: 0.000087\n",
      "Epoch 470, Train Loss: 0.0063, Val Loss: 0.0088\n",
      "Learning rate: 0.000087\n",
      "Epoch 480, Train Loss: 0.0062, Val Loss: 0.0077\n",
      "Learning rate: 0.000085\n",
      "Epoch 490, Train Loss: 0.0061, Val Loss: 0.0068\n",
      "Learning rate: 0.000085\n",
      "Epoch 500, Train Loss: 0.0060, Val Loss: 0.0089\n",
      "Learning rate: 0.000085\n",
      "Epoch 510, Train Loss: 0.0060, Val Loss: 0.0084\n",
      "Learning rate: 0.000085\n",
      "Epoch 520, Train Loss: 0.0059, Val Loss: 0.0084\n",
      "Learning rate: 0.000083\n",
      "Epoch 530, Train Loss: 0.0058, Val Loss: 0.0066\n",
      "Learning rate: 0.000083\n",
      "Epoch 540, Train Loss: 0.0057, Val Loss: 0.0062\n",
      "Learning rate: 0.000083\n",
      "Epoch 550, Train Loss: 0.0057, Val Loss: 0.0072\n",
      "Learning rate: 0.000083\n",
      "Epoch 560, Train Loss: 0.0056, Val Loss: 0.0072\n",
      "Learning rate: 0.000083\n",
      "Epoch 570, Train Loss: 0.0056, Val Loss: 0.0077\n",
      "Learning rate: 0.000082\n",
      "Epoch 580, Train Loss: 0.0055, Val Loss: 0.0061\n",
      "Learning rate: 0.000082\n",
      "Epoch 590, Train Loss: 0.0055, Val Loss: 0.0070\n",
      "Learning rate: 0.000082\n",
      "Epoch 600, Train Loss: 0.0054, Val Loss: 0.0069\n",
      "Learning rate: 0.000080\n",
      "Epoch 610, Train Loss: 0.0053, Val Loss: 0.0096\n",
      "Learning rate: 0.000080\n",
      "Epoch 620, Train Loss: 0.0053, Val Loss: 0.0111\n",
      "Learning rate: 0.000078\n",
      "Epoch 630, Train Loss: 0.0053, Val Loss: 0.0081\n",
      "Learning rate: 0.000078\n",
      "Epoch 640, Train Loss: 0.0052, Val Loss: 0.0070\n",
      "Learning rate: 0.000078\n",
      "Epoch 650, Train Loss: 0.0051, Val Loss: 0.0074\n",
      "Learning rate: 0.000077\n",
      "Epoch 660, Train Loss: 0.0051, Val Loss: 0.0075\n",
      "Learning rate: 0.000077\n",
      "Epoch 670, Train Loss: 0.0051, Val Loss: 0.0066\n",
      "Learning rate: 0.000075\n",
      "Epoch 680, Train Loss: 0.0050, Val Loss: 0.0052\n",
      "Learning rate: 0.000075\n",
      "Epoch 690, Train Loss: 0.0050, Val Loss: 0.0079\n",
      "Learning rate: 0.000075\n",
      "Epoch 700, Train Loss: 0.0049, Val Loss: 0.0064\n",
      "Learning rate: 0.000075\n",
      "Epoch 710, Train Loss: 0.0049, Val Loss: 0.0058\n",
      "Learning rate: 0.000074\n",
      "Epoch 720, Train Loss: 0.0048, Val Loss: 0.0058\n",
      "Learning rate: 0.000074\n",
      "Epoch 730, Train Loss: 0.0048, Val Loss: 0.0052\n",
      "Learning rate: 0.000074\n",
      "Epoch 740, Train Loss: 0.0048, Val Loss: 0.0052\n",
      "Learning rate: 0.000072\n",
      "Epoch 750, Train Loss: 0.0047, Val Loss: 0.0064\n",
      "Learning rate: 0.000072\n",
      "Epoch 760, Train Loss: 0.0047, Val Loss: 0.0050\n",
      "Learning rate: 0.000072\n",
      "Epoch 770, Train Loss: 0.0047, Val Loss: 0.0053\n",
      "Learning rate: 0.000072\n",
      "Epoch 780, Train Loss: 0.0046, Val Loss: 0.0070\n",
      "Learning rate: 0.000071\n",
      "Epoch 790, Train Loss: 0.0046, Val Loss: 0.0050\n",
      "Learning rate: 0.000071\n",
      "Epoch 800, Train Loss: 0.0046, Val Loss: 0.0051\n",
      "Learning rate: 0.000070\n",
      "Epoch 810, Train Loss: 0.0045, Val Loss: 0.0052\n",
      "Learning rate: 0.000070\n",
      "Epoch 820, Train Loss: 0.0045, Val Loss: 0.0050\n",
      "Learning rate: 0.000068\n",
      "Epoch 830, Train Loss: 0.0045, Val Loss: 0.0062\n",
      "Learning rate: 0.000068\n",
      "Epoch 840, Train Loss: 0.0045, Val Loss: 0.0059\n",
      "Learning rate: 0.000068\n",
      "Epoch 850, Train Loss: 0.0044, Val Loss: 0.0049\n",
      "Learning rate: 0.000067\n",
      "Epoch 860, Train Loss: 0.0044, Val Loss: 0.0047\n",
      "Learning rate: 0.000067\n",
      "Epoch 870, Train Loss: 0.0044, Val Loss: 0.0066\n",
      "Learning rate: 0.000067\n",
      "Epoch 880, Train Loss: 0.0044, Val Loss: 0.0074\n",
      "Learning rate: 0.000065\n",
      "Epoch 890, Train Loss: 0.0043, Val Loss: 0.0049\n",
      "Learning rate: 0.000065\n",
      "Epoch 900, Train Loss: 0.0043, Val Loss: 0.0044\n",
      "Learning rate: 0.000065\n",
      "Epoch 910, Train Loss: 0.0043, Val Loss: 0.0049\n",
      "Learning rate: 0.000065\n",
      "Epoch 920, Train Loss: 0.0042, Val Loss: 0.0047\n",
      "Learning rate: 0.000065\n",
      "Epoch 930, Train Loss: 0.0042, Val Loss: 0.0050\n",
      "Learning rate: 0.000065\n",
      "Epoch 940, Train Loss: 0.0042, Val Loss: 0.0053\n",
      "Learning rate: 0.000064\n",
      "Epoch 950, Train Loss: 0.0042, Val Loss: 0.0044\n",
      "Learning rate: 0.000064\n",
      "Epoch 960, Train Loss: 0.0042, Val Loss: 0.0045\n",
      "Learning rate: 0.000064\n",
      "Epoch 970, Train Loss: 0.0041, Val Loss: 0.0048\n",
      "Learning rate: 0.000064\n",
      "Epoch 980, Train Loss: 0.0041, Val Loss: 0.0065\n",
      "Learning rate: 0.000063\n",
      "Epoch 990, Train Loss: 0.0041, Val Loss: 0.0043\n",
      "Learning rate: 0.000063\n",
      "Epoch 1000, Train Loss: 0.0041, Val Loss: 0.0043\n",
      "Learning rate: 0.000062\n",
      "Epoch 1010, Train Loss: 0.0041, Val Loss: 0.0041\n",
      "Learning rate: 0.000062\n",
      "Epoch 1020, Train Loss: 0.0040, Val Loss: 0.0044\n",
      "Learning rate: 0.000062\n",
      "Epoch 1030, Train Loss: 0.0040, Val Loss: 0.0052\n",
      "Learning rate: 0.000062\n",
      "Epoch 1040, Train Loss: 0.0040, Val Loss: 0.0040\n",
      "Learning rate: 0.000060\n",
      "Epoch 1050, Train Loss: 0.0040, Val Loss: 0.0043\n",
      "Learning rate: 0.000060\n",
      "Epoch 1060, Train Loss: 0.0039, Val Loss: 0.0043\n",
      "Learning rate: 0.000059\n",
      "Epoch 1070, Train Loss: 0.0039, Val Loss: 0.0045\n",
      "Learning rate: 0.000059\n",
      "Epoch 1080, Train Loss: 0.0039, Val Loss: 0.0040\n",
      "Learning rate: 0.000059\n",
      "Epoch 1090, Train Loss: 0.0039, Val Loss: 0.0042\n",
      "Learning rate: 0.000059\n",
      "Epoch 1100, Train Loss: 0.0039, Val Loss: 0.0047\n",
      "Learning rate: 0.000059\n",
      "Epoch 1110, Train Loss: 0.0039, Val Loss: 0.0045\n",
      "Learning rate: 0.000058\n",
      "Epoch 1120, Train Loss: 0.0038, Val Loss: 0.0043\n",
      "Learning rate: 0.000058\n",
      "Epoch 1130, Train Loss: 0.0038, Val Loss: 0.0055\n",
      "Learning rate: 0.000057\n",
      "Epoch 1140, Train Loss: 0.0038, Val Loss: 0.0041\n",
      "Learning rate: 0.000057\n",
      "Epoch 1150, Train Loss: 0.0038, Val Loss: 0.0047\n",
      "Learning rate: 0.000056\n",
      "Epoch 1160, Train Loss: 0.0038, Val Loss: 0.0041\n",
      "Learning rate: 0.000056\n",
      "Epoch 1170, Train Loss: 0.0038, Val Loss: 0.0038\n",
      "Learning rate: 0.000056\n",
      "Epoch 1180, Train Loss: 0.0037, Val Loss: 0.0040\n",
      "Learning rate: 0.000056\n",
      "Epoch 1190, Train Loss: 0.0037, Val Loss: 0.0039\n",
      "Learning rate: 0.000055\n",
      "Epoch 1200, Train Loss: 0.0037, Val Loss: 0.0041\n",
      "Learning rate: 0.000055\n",
      "Epoch 1210, Train Loss: 0.0037, Val Loss: 0.0038\n",
      "Learning rate: 0.000053\n",
      "Epoch 1220, Train Loss: 0.0037, Val Loss: 0.0043\n",
      "Learning rate: 0.000053\n",
      "Epoch 1230, Train Loss: 0.0037, Val Loss: 0.0050\n",
      "Learning rate: 0.000052\n",
      "Epoch 1240, Train Loss: 0.0037, Val Loss: 0.0040\n",
      "Learning rate: 0.000052\n",
      "Epoch 1250, Train Loss: 0.0036, Val Loss: 0.0038\n",
      "Learning rate: 0.000052\n",
      "Epoch 1260, Train Loss: 0.0036, Val Loss: 0.0050\n",
      "Learning rate: 0.000052\n",
      "Epoch 1270, Train Loss: 0.0036, Val Loss: 0.0038\n",
      "Learning rate: 0.000051\n",
      "Epoch 1280, Train Loss: 0.0036, Val Loss: 0.0036\n",
      "Learning rate: 0.000051\n",
      "Epoch 1290, Train Loss: 0.0036, Val Loss: 0.0036\n",
      "Learning rate: 0.000051\n",
      "Epoch 1300, Train Loss: 0.0036, Val Loss: 0.0035\n",
      "Learning rate: 0.000051\n",
      "Epoch 1310, Train Loss: 0.0036, Val Loss: 0.0039\n",
      "Learning rate: 0.000050\n",
      "Epoch 1320, Train Loss: 0.0036, Val Loss: 0.0037\n",
      "Learning rate: 0.000050\n",
      "Epoch 1330, Train Loss: 0.0035, Val Loss: 0.0035\n",
      "Learning rate: 0.000050\n",
      "Epoch 1340, Train Loss: 0.0035, Val Loss: 0.0038\n",
      "Learning rate: 0.000050\n",
      "Epoch 1350, Train Loss: 0.0035, Val Loss: 0.0038\n",
      "Learning rate: 0.000049\n",
      "Epoch 1360, Train Loss: 0.0035, Val Loss: 0.0042\n",
      "Learning rate: 0.000049\n",
      "Epoch 1370, Train Loss: 0.0035, Val Loss: 0.0037\n",
      "Learning rate: 0.000048\n",
      "Epoch 1380, Train Loss: 0.0035, Val Loss: 0.0036\n",
      "Learning rate: 0.000048\n",
      "Epoch 1390, Train Loss: 0.0035, Val Loss: 0.0036\n",
      "Learning rate: 0.000048\n",
      "Epoch 1400, Train Loss: 0.0035, Val Loss: 0.0037\n",
      "Learning rate: 0.000048\n",
      "Epoch 1410, Train Loss: 0.0034, Val Loss: 0.0036\n",
      "Learning rate: 0.000048\n",
      "Epoch 1420, Train Loss: 0.0034, Val Loss: 0.0045\n",
      "Learning rate: 0.000047\n",
      "Epoch 1430, Train Loss: 0.0034, Val Loss: 0.0037\n",
      "Learning rate: 0.000047\n",
      "Epoch 1440, Train Loss: 0.0034, Val Loss: 0.0040\n",
      "Learning rate: 0.000046\n",
      "Epoch 1450, Train Loss: 0.0034, Val Loss: 0.0035\n",
      "Learning rate: 0.000046\n",
      "Epoch 1460, Train Loss: 0.0034, Val Loss: 0.0037\n",
      "Learning rate: 0.000045\n",
      "Epoch 1470, Train Loss: 0.0034, Val Loss: 0.0051\n",
      "Learning rate: 0.000045\n",
      "Epoch 1480, Train Loss: 0.0034, Val Loss: 0.0035\n",
      "Learning rate: 0.000045\n",
      "Epoch 1490, Train Loss: 0.0034, Val Loss: 0.0041\n",
      "Learning rate: 0.000045\n",
      "Epoch 1500, Train Loss: 0.0033, Val Loss: 0.0036\n",
      "Learning rate: 0.000044\n",
      "Epoch 1510, Train Loss: 0.0033, Val Loss: 0.0033\n",
      "Learning rate: 0.000044\n",
      "Epoch 1520, Train Loss: 0.0033, Val Loss: 0.0041\n",
      "Learning rate: 0.000044\n",
      "Epoch 1530, Train Loss: 0.0033, Val Loss: 0.0035\n",
      "Learning rate: 0.000043\n",
      "Epoch 1540, Train Loss: 0.0033, Val Loss: 0.0033\n",
      "Learning rate: 0.000043\n",
      "Epoch 1550, Train Loss: 0.0033, Val Loss: 0.0043\n",
      "Learning rate: 0.000042\n",
      "Epoch 1560, Train Loss: 0.0033, Val Loss: 0.0039\n",
      "Learning rate: 0.000042\n",
      "Epoch 1570, Train Loss: 0.0033, Val Loss: 0.0033\n",
      "Learning rate: 0.000042\n",
      "Epoch 1580, Train Loss: 0.0033, Val Loss: 0.0033\n",
      "Learning rate: 0.000042\n",
      "Epoch 1590, Train Loss: 0.0033, Val Loss: 0.0033\n",
      "Learning rate: 0.000041\n",
      "Epoch 1600, Train Loss: 0.0033, Val Loss: 0.0039\n",
      "Learning rate: 0.000041\n",
      "Epoch 1610, Train Loss: 0.0033, Val Loss: 0.0034\n",
      "Learning rate: 0.000040\n",
      "Epoch 1620, Train Loss: 0.0032, Val Loss: 0.0052\n",
      "Learning rate: 0.000040\n",
      "Epoch 1630, Train Loss: 0.0032, Val Loss: 0.0041\n",
      "Learning rate: 0.000039\n",
      "Epoch 1640, Train Loss: 0.0032, Val Loss: 0.0034\n",
      "Learning rate: 0.000039\n",
      "Epoch 1650, Train Loss: 0.0032, Val Loss: 0.0033\n",
      "Learning rate: 0.000039\n",
      "Epoch 1660, Train Loss: 0.0032, Val Loss: 0.0030\n",
      "Learning rate: 0.000039\n",
      "Epoch 1670, Train Loss: 0.0032, Val Loss: 0.0032\n",
      "Learning rate: 0.000039\n",
      "Epoch 1680, Train Loss: 0.0032, Val Loss: 0.0033\n",
      "Learning rate: 0.000039\n",
      "Epoch 1690, Train Loss: 0.0032, Val Loss: 0.0032\n",
      "Learning rate: 0.000038\n",
      "Epoch 1700, Train Loss: 0.0032, Val Loss: 0.0032\n",
      "Learning rate: 0.000038\n",
      "Epoch 1710, Train Loss: 0.0032, Val Loss: 0.0032\n",
      "Learning rate: 0.000037\n",
      "Epoch 1720, Train Loss: 0.0032, Val Loss: 0.0040\n",
      "Learning rate: 0.000037\n",
      "Epoch 1730, Train Loss: 0.0032, Val Loss: 0.0040\n",
      "Learning rate: 0.000036\n",
      "Epoch 1740, Train Loss: 0.0031, Val Loss: 0.0033\n",
      "Learning rate: 0.000036\n",
      "Epoch 1750, Train Loss: 0.0031, Val Loss: 0.0044\n",
      "Learning rate: 0.000036\n",
      "Epoch 1760, Train Loss: 0.0031, Val Loss: 0.0036\n",
      "Learning rate: 0.000036\n",
      "Epoch 1770, Train Loss: 0.0031, Val Loss: 0.0030\n",
      "Learning rate: 0.000035\n",
      "Epoch 1780, Train Loss: 0.0031, Val Loss: 0.0032\n",
      "Learning rate: 0.000035\n",
      "Epoch 1790, Train Loss: 0.0031, Val Loss: 0.0035\n",
      "Learning rate: 0.000035\n",
      "Epoch 1800, Train Loss: 0.0031, Val Loss: 0.0039\n",
      "Learning rate: 0.000034\n",
      "Epoch 1810, Train Loss: 0.0031, Val Loss: 0.0040\n",
      "Learning rate: 0.000034\n",
      "Epoch 1820, Train Loss: 0.0031, Val Loss: 0.0033\n",
      "Learning rate: 0.000034\n",
      "Epoch 1830, Train Loss: 0.0031, Val Loss: 0.0038\n",
      "Learning rate: 0.000034\n",
      "Epoch 1840, Train Loss: 0.0031, Val Loss: 0.0059\n",
      "Learning rate: 0.000033\n",
      "Epoch 1850, Train Loss: 0.0031, Val Loss: 0.0035\n",
      "Learning rate: 0.000033\n",
      "Epoch 1860, Train Loss: 0.0031, Val Loss: 0.0043\n",
      "Learning rate: 0.000032\n",
      "Epoch 1870, Train Loss: 0.0031, Val Loss: 0.0036\n",
      "Learning rate: 0.000032\n",
      "Epoch 1880, Train Loss: 0.0030, Val Loss: 0.0031\n",
      "Learning rate: 0.000032\n",
      "Epoch 1890, Train Loss: 0.0030, Val Loss: 0.0047\n",
      "Learning rate: 0.000032\n",
      "Epoch 1900, Train Loss: 0.0030, Val Loss: 0.0033\n",
      "Learning rate: 0.000032\n",
      "Epoch 1910, Train Loss: 0.0030, Val Loss: 0.0029\n",
      "Learning rate: 0.000032\n",
      "Epoch 1920, Train Loss: 0.0030, Val Loss: 0.0035\n",
      "Learning rate: 0.000031\n",
      "Epoch 1930, Train Loss: 0.0030, Val Loss: 0.0033\n",
      "Learning rate: 0.000031\n",
      "Epoch 1940, Train Loss: 0.0030, Val Loss: 0.0038\n",
      "Learning rate: 0.000030\n",
      "Epoch 1950, Train Loss: 0.0030, Val Loss: 0.0038\n",
      "Learning rate: 0.000030\n",
      "Epoch 1960, Train Loss: 0.0030, Val Loss: 0.0029\n",
      "Learning rate: 0.000030\n",
      "Epoch 1970, Train Loss: 0.0030, Val Loss: 0.0029\n",
      "Learning rate: 0.000030\n",
      "Epoch 1980, Train Loss: 0.0030, Val Loss: 0.0034\n",
      "Learning rate: 0.000030\n",
      "Epoch 1990, Train Loss: 0.0030, Val Loss: 0.0031\n",
      "Learning rate: 0.000029\n",
      "Epoch 2000, Train Loss: 0.0030, Val Loss: 0.0032\n",
      "Learning rate: 0.000029\n",
      "Epoch 2010, Train Loss: 0.0030, Val Loss: 0.0031\n",
      "Learning rate: 0.000029\n",
      "Epoch 2020, Train Loss: 0.0030, Val Loss: 0.0031\n",
      "Learning rate: 0.000029\n",
      "Epoch 2030, Train Loss: 0.0030, Val Loss: 0.0029\n",
      "Learning rate: 0.000029\n",
      "Epoch 2040, Train Loss: 0.0029, Val Loss: 0.0032\n",
      "Learning rate: 0.000028\n",
      "Epoch 2050, Train Loss: 0.0029, Val Loss: 0.0029\n",
      "Learning rate: 0.000028\n",
      "Epoch 2060, Train Loss: 0.0029, Val Loss: 0.0030\n",
      "Learning rate: 0.000027\n",
      "Epoch 2070, Train Loss: 0.0029, Val Loss: 0.0029\n",
      "Learning rate: 0.000027\n",
      "Epoch 2080, Train Loss: 0.0029, Val Loss: 0.0032\n",
      "Learning rate: 0.000027\n",
      "Epoch 2090, Train Loss: 0.0029, Val Loss: 0.0030\n",
      "Learning rate: 0.000027\n",
      "Epoch 2100, Train Loss: 0.0029, Val Loss: 0.0029\n",
      "Learning rate: 0.000026\n",
      "Epoch 2110, Train Loss: 0.0029, Val Loss: 0.0028\n",
      "Learning rate: 0.000026\n",
      "Epoch 2120, Train Loss: 0.0029, Val Loss: 0.0037\n",
      "Learning rate: 0.000026\n",
      "Epoch 2130, Train Loss: 0.0029, Val Loss: 0.0029\n",
      "Learning rate: 0.000026\n",
      "Epoch 2140, Train Loss: 0.0029, Val Loss: 0.0029\n",
      "Learning rate: 0.000025\n",
      "Epoch 2150, Train Loss: 0.0029, Val Loss: 0.0029\n",
      "Learning rate: 0.000025\n",
      "Epoch 2160, Train Loss: 0.0029, Val Loss: 0.0028\n",
      "Learning rate: 0.000025\n",
      "Epoch 2170, Train Loss: 0.0029, Val Loss: 0.0030\n",
      "Learning rate: 0.000025\n",
      "Epoch 2180, Train Loss: 0.0029, Val Loss: 0.0028\n",
      "Learning rate: 0.000024\n",
      "Epoch 2190, Train Loss: 0.0029, Val Loss: 0.0029\n",
      "Learning rate: 0.000024\n",
      "Epoch 2200, Train Loss: 0.0029, Val Loss: 0.0029\n",
      "Learning rate: 0.000024\n",
      "Epoch 2210, Train Loss: 0.0029, Val Loss: 0.0029\n",
      "Learning rate: 0.000024\n",
      "Epoch 2220, Train Loss: 0.0029, Val Loss: 0.0032\n",
      "Learning rate: 0.000024\n",
      "Epoch 2230, Train Loss: 0.0028, Val Loss: 0.0031\n",
      "Learning rate: 0.000023\n",
      "Epoch 2240, Train Loss: 0.0028, Val Loss: 0.0029\n",
      "Learning rate: 0.000023\n",
      "Epoch 2250, Train Loss: 0.0028, Val Loss: 0.0034\n",
      "Learning rate: 0.000023\n",
      "Epoch 2260, Train Loss: 0.0028, Val Loss: 0.0036\n",
      "Learning rate: 0.000023\n",
      "Epoch 2270, Train Loss: 0.0028, Val Loss: 0.0028\n",
      "Learning rate: 0.000022\n",
      "Epoch 2280, Train Loss: 0.0028, Val Loss: 0.0029\n",
      "Learning rate: 0.000022\n",
      "Epoch 2290, Train Loss: 0.0028, Val Loss: 0.0027\n",
      "Learning rate: 0.000022\n",
      "Epoch 2300, Train Loss: 0.0028, Val Loss: 0.0027\n",
      "Learning rate: 0.000022\n",
      "Epoch 2310, Train Loss: 0.0028, Val Loss: 0.0029\n",
      "Learning rate: 0.000022\n",
      "Epoch 2320, Train Loss: 0.0028, Val Loss: 0.0028\n",
      "Learning rate: 0.000022\n",
      "Epoch 2330, Train Loss: 0.0028, Val Loss: 0.0028\n",
      "Learning rate: 0.000022\n",
      "Epoch 2340, Train Loss: 0.0028, Val Loss: 0.0028\n",
      "Learning rate: 0.000022\n",
      "Epoch 2350, Train Loss: 0.0028, Val Loss: 0.0038\n",
      "Learning rate: 0.000022\n",
      "Epoch 2360, Train Loss: 0.0028, Val Loss: 0.0031\n",
      "Learning rate: 0.000022\n",
      "Epoch 2370, Train Loss: 0.0028, Val Loss: 0.0027\n",
      "Learning rate: 0.000021\n",
      "Epoch 2380, Train Loss: 0.0028, Val Loss: 0.0028\n",
      "Learning rate: 0.000021\n",
      "Epoch 2390, Train Loss: 0.0028, Val Loss: 0.0027\n",
      "Learning rate: 0.000021\n",
      "Epoch 2400, Train Loss: 0.0028, Val Loss: 0.0027\n",
      "Learning rate: 0.000021\n",
      "Epoch 2410, Train Loss: 0.0028, Val Loss: 0.0027\n",
      "Learning rate: 0.000021\n",
      "Epoch 2420, Train Loss: 0.0028, Val Loss: 0.0028\n",
      "Learning rate: 0.000021\n",
      "Epoch 2430, Train Loss: 0.0028, Val Loss: 0.0027\n",
      "Learning rate: 0.000021\n",
      "Epoch 2440, Train Loss: 0.0028, Val Loss: 0.0027\n",
      "Learning rate: 0.000021\n",
      "Epoch 2450, Train Loss: 0.0028, Val Loss: 0.0028\n",
      "Learning rate: 0.000020\n",
      "Epoch 2460, Train Loss: 0.0028, Val Loss: 0.0027\n",
      "Learning rate: 0.000020\n",
      "Epoch 2470, Train Loss: 0.0027, Val Loss: 0.0027\n",
      "Learning rate: 0.000020\n",
      "Epoch 2480, Train Loss: 0.0027, Val Loss: 0.0026\n",
      "Learning rate: 0.000020\n",
      "Epoch 2490, Train Loss: 0.0027, Val Loss: 0.0027\n",
      "Learning rate: 0.000020\n",
      "Epoch 2500, Train Loss: 0.0027, Val Loss: 0.0026\n",
      "Learning rate: 0.000019\n",
      "Epoch 2510, Train Loss: 0.0027, Val Loss: 0.0027\n",
      "Learning rate: 0.000019\n",
      "Epoch 2520, Train Loss: 0.0027, Val Loss: 0.0026\n",
      "Learning rate: 0.000019\n",
      "Epoch 2530, Train Loss: 0.0027, Val Loss: 0.0026\n",
      "Learning rate: 0.000019\n",
      "Epoch 2540, Train Loss: 0.0027, Val Loss: 0.0027\n",
      "Learning rate: 0.000019\n",
      "Epoch 2550, Train Loss: 0.0027, Val Loss: 0.0025\n",
      "Learning rate: 0.000019\n",
      "Epoch 2560, Train Loss: 0.0027, Val Loss: 0.0028\n",
      "Learning rate: 0.000019\n",
      "Epoch 2570, Train Loss: 0.0027, Val Loss: 0.0026\n",
      "Learning rate: 0.000019\n",
      "Epoch 2580, Train Loss: 0.0027, Val Loss: 0.0026\n",
      "Learning rate: 0.000018\n",
      "Epoch 2590, Train Loss: 0.0027, Val Loss: 0.0026\n",
      "Learning rate: 0.000018\n",
      "Epoch 2600, Train Loss: 0.0027, Val Loss: 0.0027\n",
      "Learning rate: 0.000018\n",
      "Epoch 2610, Train Loss: 0.0027, Val Loss: 0.0026\n",
      "Learning rate: 0.000018\n",
      "Epoch 2620, Train Loss: 0.0027, Val Loss: 0.0026\n",
      "Learning rate: 0.000018\n",
      "Epoch 2630, Train Loss: 0.0027, Val Loss: 0.0026\n",
      "Learning rate: 0.000018\n",
      "Epoch 2640, Train Loss: 0.0027, Val Loss: 0.0026\n",
      "Learning rate: 0.000017\n",
      "Epoch 2650, Train Loss: 0.0027, Val Loss: 0.0026\n",
      "Learning rate: 0.000017\n",
      "Epoch 2660, Train Loss: 0.0027, Val Loss: 0.0026\n",
      "Learning rate: 0.000017\n",
      "Epoch 2670, Train Loss: 0.0027, Val Loss: 0.0025\n",
      "Learning rate: 0.000017\n",
      "Epoch 2680, Train Loss: 0.0027, Val Loss: 0.0025\n",
      "Learning rate: 0.000017\n",
      "Epoch 2690, Train Loss: 0.0027, Val Loss: 0.0025\n",
      "Learning rate: 0.000017\n",
      "Epoch 2700, Train Loss: 0.0027, Val Loss: 0.0026\n",
      "Learning rate: 0.000017\n",
      "Epoch 2710, Train Loss: 0.0027, Val Loss: 0.0026\n",
      "Learning rate: 0.000017\n",
      "Epoch 2720, Train Loss: 0.0027, Val Loss: 0.0026\n",
      "Learning rate: 0.000016\n",
      "Epoch 2730, Train Loss: 0.0027, Val Loss: 0.0025\n",
      "Learning rate: 0.000016\n",
      "Epoch 2740, Train Loss: 0.0026, Val Loss: 0.0025\n",
      "Learning rate: 0.000016\n",
      "Epoch 2750, Train Loss: 0.0026, Val Loss: 0.0025\n",
      "Learning rate: 0.000016\n",
      "Epoch 2760, Train Loss: 0.0026, Val Loss: 0.0025\n",
      "Learning rate: 0.000016\n",
      "Epoch 2770, Train Loss: 0.0026, Val Loss: 0.0025\n",
      "Learning rate: 0.000016\n",
      "Epoch 2780, Train Loss: 0.0026, Val Loss: 0.0026\n",
      "Learning rate: 0.000015\n",
      "Epoch 2790, Train Loss: 0.0026, Val Loss: 0.0025\n",
      "Learning rate: 0.000015\n",
      "Epoch 2800, Train Loss: 0.0026, Val Loss: 0.0025\n",
      "Learning rate: 0.000015\n",
      "Epoch 2810, Train Loss: 0.0026, Val Loss: 0.0024\n",
      "Learning rate: 0.000015\n",
      "Epoch 2820, Train Loss: 0.0026, Val Loss: 0.0025\n",
      "Learning rate: 0.000015\n",
      "Epoch 2830, Train Loss: 0.0026, Val Loss: 0.0025\n",
      "Learning rate: 0.000015\n",
      "Epoch 2840, Train Loss: 0.0026, Val Loss: 0.0024\n",
      "Learning rate: 0.000015\n",
      "Epoch 2850, Train Loss: 0.0026, Val Loss: 0.0025\n",
      "Learning rate: 0.000014\n",
      "Epoch 2860, Train Loss: 0.0026, Val Loss: 0.0025\n",
      "Learning rate: 0.000014\n",
      "Epoch 2870, Train Loss: 0.0026, Val Loss: 0.0025\n",
      "Learning rate: 0.000014\n",
      "Epoch 2880, Train Loss: 0.0026, Val Loss: 0.0024\n",
      "Learning rate: 0.000014\n",
      "Epoch 2890, Train Loss: 0.0026, Val Loss: 0.0027\n",
      "Learning rate: 0.000014\n",
      "Epoch 2900, Train Loss: 0.0026, Val Loss: 0.0024\n",
      "Learning rate: 0.000014\n",
      "Epoch 2910, Train Loss: 0.0026, Val Loss: 0.0024\n",
      "Learning rate: 0.000014\n",
      "Epoch 2920, Train Loss: 0.0026, Val Loss: 0.0024\n",
      "Learning rate: 0.000014\n",
      "Epoch 2930, Train Loss: 0.0026, Val Loss: 0.0025\n",
      "Learning rate: 0.000014\n",
      "Epoch 2940, Train Loss: 0.0026, Val Loss: 0.0024\n",
      "Learning rate: 0.000014\n",
      "Epoch 2950, Train Loss: 0.0026, Val Loss: 0.0024\n",
      "Learning rate: 0.000014\n",
      "Epoch 2960, Train Loss: 0.0026, Val Loss: 0.0024\n",
      "Learning rate: 0.000013\n",
      "Epoch 2970, Train Loss: 0.0026, Val Loss: 0.0025\n",
      "Learning rate: 0.000013\n",
      "Epoch 2980, Train Loss: 0.0026, Val Loss: 0.0024\n",
      "Learning rate: 0.000013\n",
      "Epoch 2990, Train Loss: 0.0026, Val Loss: 0.0025\n",
      "Learning rate: 0.000013\n",
      "Epoch 3000, Train Loss: 0.0026, Val Loss: 0.0024\n",
      "Learning rate: 0.000013\n",
      "Epoch 3010, Train Loss: 0.0026, Val Loss: 0.0023\n",
      "Learning rate: 0.000013\n",
      "Epoch 3020, Train Loss: 0.0026, Val Loss: 0.0024\n",
      "Learning rate: 0.000013\n",
      "Epoch 3030, Train Loss: 0.0026, Val Loss: 0.0024\n",
      "Learning rate: 0.000013\n",
      "Epoch 3040, Train Loss: 0.0026, Val Loss: 0.0023\n",
      "Learning rate: 0.000012\n",
      "Epoch 3050, Train Loss: 0.0026, Val Loss: 0.0025\n",
      "Learning rate: 0.000012\n",
      "Epoch 3060, Train Loss: 0.0026, Val Loss: 0.0024\n",
      "Learning rate: 0.000012\n",
      "Epoch 3070, Train Loss: 0.0026, Val Loss: 0.0023\n",
      "Learning rate: 0.000012\n",
      "Epoch 3080, Train Loss: 0.0026, Val Loss: 0.0024\n",
      "Learning rate: 0.000012\n",
      "Epoch 3090, Train Loss: 0.0026, Val Loss: 0.0024\n",
      "Learning rate: 0.000012\n",
      "Epoch 3100, Train Loss: 0.0026, Val Loss: 0.0023\n",
      "Learning rate: 0.000012\n",
      "Epoch 3110, Train Loss: 0.0025, Val Loss: 0.0023\n",
      "Learning rate: 0.000012\n",
      "Epoch 3120, Train Loss: 0.0025, Val Loss: 0.0024\n",
      "Learning rate: 0.000012\n",
      "Epoch 3130, Train Loss: 0.0025, Val Loss: 0.0024\n",
      "Learning rate: 0.000012\n",
      "Epoch 3140, Train Loss: 0.0025, Val Loss: 0.0024\n",
      "Learning rate: 0.000012\n",
      "Epoch 3150, Train Loss: 0.0025, Val Loss: 0.0024\n",
      "Learning rate: 0.000012\n",
      "Epoch 3160, Train Loss: 0.0025, Val Loss: 0.0024\n",
      "Learning rate: 0.000012\n",
      "Epoch 3170, Train Loss: 0.0025, Val Loss: 0.0024\n",
      "Learning rate: 0.000012\n",
      "Epoch 3180, Train Loss: 0.0025, Val Loss: 0.0024\n",
      "Learning rate: 0.000012\n",
      "Epoch 3190, Train Loss: 0.0025, Val Loss: 0.0024\n",
      "Learning rate: 0.000011\n",
      "Epoch 3200, Train Loss: 0.0025, Val Loss: 0.0028\n",
      "Learning rate: 0.000011\n",
      "Epoch 3210, Train Loss: 0.0025, Val Loss: 0.0023\n",
      "Learning rate: 0.000011\n",
      "Epoch 3220, Train Loss: 0.0025, Val Loss: 0.0024\n",
      "Learning rate: 0.000011\n",
      "Epoch 3230, Train Loss: 0.0025, Val Loss: 0.0024\n",
      "Learning rate: 0.000011\n",
      "Epoch 3240, Train Loss: 0.0025, Val Loss: 0.0023\n",
      "Learning rate: 0.000011\n",
      "Epoch 3250, Train Loss: 0.0025, Val Loss: 0.0024\n",
      "Learning rate: 0.000011\n",
      "Epoch 3260, Train Loss: 0.0025, Val Loss: 0.0024\n",
      "Learning rate: 0.000011\n",
      "Epoch 3270, Train Loss: 0.0025, Val Loss: 0.0023\n",
      "Learning rate: 0.000010\n",
      "Epoch 3280, Train Loss: 0.0025, Val Loss: 0.0024\n",
      "Learning rate: 0.000010\n",
      "Epoch 3290, Train Loss: 0.0025, Val Loss: 0.0024\n",
      "Learning rate: 0.000010\n",
      "Epoch 3300, Train Loss: 0.0025, Val Loss: 0.0027\n",
      "Learning rate: 0.000010\n",
      "Epoch 3310, Train Loss: 0.0025, Val Loss: 0.0023\n",
      "Learning rate: 0.000010\n",
      "Epoch 3320, Train Loss: 0.0025, Val Loss: 0.0024\n",
      "Learning rate: 0.000010\n",
      "Epoch 3330, Train Loss: 0.0025, Val Loss: 0.0024\n",
      "Learning rate: 0.000010\n",
      "Epoch 3340, Train Loss: 0.0025, Val Loss: 0.0024\n",
      "Learning rate: 0.000010\n",
      "Epoch 3350, Train Loss: 0.0025, Val Loss: 0.0023\n",
      "Learning rate: 0.000010\n",
      "Epoch 3360, Train Loss: 0.0025, Val Loss: 0.0024\n",
      "Learning rate: 0.000010\n",
      "Epoch 3370, Train Loss: 0.0025, Val Loss: 0.0023\n",
      "Learning rate: 0.000010\n",
      "Epoch 3380, Train Loss: 0.0025, Val Loss: 0.0023\n",
      "Learning rate: 0.000010\n",
      "Epoch 3390, Train Loss: 0.0025, Val Loss: 0.0023\n",
      "Learning rate: 0.000010\n",
      "Epoch 3400, Train Loss: 0.0025, Val Loss: 0.0023\n",
      "Learning rate: 0.000009\n",
      "Epoch 3410, Train Loss: 0.0025, Val Loss: 0.0023\n",
      "Learning rate: 0.000009\n",
      "Epoch 3420, Train Loss: 0.0025, Val Loss: 0.0024\n",
      "Learning rate: 0.000009\n",
      "Epoch 3430, Train Loss: 0.0025, Val Loss: 0.0023\n",
      "Learning rate: 0.000009\n",
      "Epoch 3440, Train Loss: 0.0025, Val Loss: 0.0023\n",
      "Learning rate: 0.000009\n",
      "Epoch 3450, Train Loss: 0.0025, Val Loss: 0.0023\n",
      "Learning rate: 0.000009\n",
      "Epoch 3460, Train Loss: 0.0025, Val Loss: 0.0023\n",
      "Learning rate: 0.000009\n",
      "Epoch 3470, Train Loss: 0.0025, Val Loss: 0.0023\n",
      "Learning rate: 0.000009\n",
      "Epoch 3480, Train Loss: 0.0025, Val Loss: 0.0023\n",
      "Learning rate: 0.000009\n",
      "Epoch 3490, Train Loss: 0.0025, Val Loss: 0.0023\n",
      "Learning rate: 0.000009\n",
      "Epoch 3500, Train Loss: 0.0025, Val Loss: 0.0022\n",
      "Learning rate: 0.000009\n",
      "Epoch 3510, Train Loss: 0.0025, Val Loss: 0.0023\n",
      "Learning rate: 0.000009\n",
      "Epoch 3520, Train Loss: 0.0025, Val Loss: 0.0023\n",
      "Learning rate: 0.000009\n",
      "Epoch 3530, Train Loss: 0.0025, Val Loss: 0.0023\n",
      "Learning rate: 0.000008\n",
      "Epoch 3540, Train Loss: 0.0025, Val Loss: 0.0023\n",
      "Learning rate: 0.000008\n",
      "Epoch 3550, Train Loss: 0.0025, Val Loss: 0.0023\n",
      "Learning rate: 0.000008\n",
      "Epoch 3560, Train Loss: 0.0025, Val Loss: 0.0022\n",
      "Learning rate: 0.000008\n",
      "Epoch 3570, Train Loss: 0.0025, Val Loss: 0.0023\n",
      "Learning rate: 0.000008\n",
      "Epoch 3580, Train Loss: 0.0024, Val Loss: 0.0023\n",
      "Learning rate: 0.000008\n",
      "Epoch 3590, Train Loss: 0.0024, Val Loss: 0.0022\n",
      "Learning rate: 0.000008\n",
      "Epoch 3600, Train Loss: 0.0024, Val Loss: 0.0023\n",
      "Learning rate: 0.000008\n",
      "Epoch 3610, Train Loss: 0.0024, Val Loss: 0.0022\n",
      "Learning rate: 0.000008\n",
      "Epoch 3620, Train Loss: 0.0024, Val Loss: 0.0023\n",
      "Learning rate: 0.000008\n",
      "Epoch 3630, Train Loss: 0.0024, Val Loss: 0.0023\n",
      "Learning rate: 0.000008\n",
      "Epoch 3640, Train Loss: 0.0024, Val Loss: 0.0022\n",
      "Learning rate: 0.000008\n",
      "Epoch 3650, Train Loss: 0.0024, Val Loss: 0.0023\n",
      "Learning rate: 0.000008\n",
      "Epoch 3660, Train Loss: 0.0024, Val Loss: 0.0022\n",
      "Learning rate: 0.000008\n",
      "Epoch 3670, Train Loss: 0.0024, Val Loss: 0.0022\n",
      "Learning rate: 0.000008\n",
      "Epoch 3680, Train Loss: 0.0024, Val Loss: 0.0022\n",
      "Learning rate: 0.000007\n",
      "Epoch 3690, Train Loss: 0.0024, Val Loss: 0.0022\n",
      "Learning rate: 0.000007\n",
      "Epoch 3700, Train Loss: 0.0024, Val Loss: 0.0022\n",
      "Learning rate: 0.000007\n",
      "Epoch 3710, Train Loss: 0.0024, Val Loss: 0.0022\n",
      "Learning rate: 0.000007\n",
      "Epoch 3720, Train Loss: 0.0024, Val Loss: 0.0021\n",
      "Learning rate: 0.000007\n",
      "Epoch 3730, Train Loss: 0.0024, Val Loss: 0.0026\n",
      "Learning rate: 0.000007\n",
      "Epoch 3740, Train Loss: 0.0024, Val Loss: 0.0023\n",
      "Learning rate: 0.000007\n",
      "Epoch 3750, Train Loss: 0.0024, Val Loss: 0.0022\n",
      "Learning rate: 0.000007\n",
      "Epoch 3760, Train Loss: 0.0024, Val Loss: 0.0022\n",
      "Learning rate: 0.000007\n",
      "Epoch 3770, Train Loss: 0.0024, Val Loss: 0.0022\n",
      "Learning rate: 0.000007\n",
      "Epoch 3780, Train Loss: 0.0024, Val Loss: 0.0022\n",
      "Learning rate: 0.000007\n",
      "Epoch 3790, Train Loss: 0.0024, Val Loss: 0.0022\n",
      "Learning rate: 0.000007\n",
      "Epoch 3800, Train Loss: 0.0024, Val Loss: 0.0022\n",
      "Learning rate: 0.000007\n",
      "Epoch 3810, Train Loss: 0.0024, Val Loss: 0.0022\n",
      "Learning rate: 0.000007\n",
      "Early stopping at epoch 3817 due to no improvement in validation loss.\n",
      "Time taken: 6176.856509447098\n",
      "Best epoch: 3567, Best validation loss: 0.0021\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Difference:\n",
      "[-16355736.0000, -2977072.0000, -6925576.0000, -3878560.0000, 8363264.0000, 12721920.0000, 6293440.0000, -29665776.0000, 12300560.0000, -1036976.0000,\n",
      " 6578656.0000, 2308880.0000, 2132848.0000, 999696.0000, 13712704.0000, 1207616.0000, -768544.0000, 4277024.0000, -40155776.0000, 9666048.0000,\n",
      " 1076496.0000, 1349968.0000, -1728992.0000, 5372640.0000, 2814016.0000, 1158224.0000, 4750384.0000, -3361312.0000, -12291776.0000, -9219648.0000,\n",
      " -5092480.0000, -1362176.0000, 7677856.0000, 526416.0000, 2793072.0000, 758080.0000, 2381328.0000, 594240.0000, 6504400.0000]\n",
      "\n",
      "E_test_denorm:\n",
      "[52500000.0000, 210000000.0000, 105000000.0000, 210000000.0000, 210000000.0000, 210000000.0000, 210000000.0000, 105000000.0000, 210000000.0000, 210000000.0000,\n",
      " 210000000.0000, 210000000.0000, 210000000.0000, 210000000.0000, 210000000.0000, 210000000.0000, 210000000.0000, 210000000.0000, 105000000.0000, 210000000.0000,\n",
      " 210000000.0000, 210000000.0000, 210000000.0000, 210000000.0000, 210000000.0000, 210000000.0000, 210000000.0000, 210000000.0000, 210000000.0000, 210000000.0000,\n",
      " 210000000.0000, 210000000.0000, 210000000.0000, 210000000.0000, 210000000.0000, 210000000.0000, 210000000.0000, 210000000.0000, 210000000.0000]\n",
      "\n",
      "Pred_test_denorm:\n",
      "[68855736.0000, 212977072.0000, 111925576.0000, 213878560.0000, 201636736.0000, 197278080.0000, 203706560.0000, 134665776.0000, 197699440.0000, 211036976.0000,\n",
      " 203421344.0000, 207691120.0000, 207867152.0000, 209000304.0000, 196287296.0000, 208792384.0000, 210768544.0000, 205722976.0000, 145155776.0000, 200333952.0000,\n",
      " 208923504.0000, 208650032.0000, 211728992.0000, 204627360.0000, 207185984.0000, 208841776.0000, 205249616.0000, 213361312.0000, 222291776.0000, 219219648.0000,\n",
      " 215092480.0000, 211362176.0000, 202322144.0000, 209473584.0000, 207206928.0000, 209241920.0000, 207618672.0000, 209405760.0000, 203495600.0000]\n",
      "\n",
      "\n",
      "MAPE: 4.7479%\n",
      "MdAPE: 2.5207314640283585%\n",
      "Best epoch: 3567\n"
     ]
    }
   ],
   "source": [
    "seed=42\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "# Define loss function, optimizer, and scheduler\n",
    "scheduler = 1\n",
    "# loss_fn = nn.MSELoss()\n",
    "# loss_fn = nn.PoissonNLLLoss(log_input=True, full=True,)\n",
    "loss_fn = nn.HuberLoss(delta=1.0)\n",
    "# optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-4)\n",
    "\n",
    "optimizer = optim.AdamW(model.parameters(), lr=1e-4, weight_decay=0.01) # 12.98 solo 1.5k runs\n",
    "\n",
    "# optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9, weight_decay=1e-5) # 15% with steplr30\n",
    "\n",
    "if scheduler:\n",
    "    scheduler = ReduceLROnPlateau(optimizer, 'min', patience=20, factor=0.98)\n",
    "\n",
    "\n",
    "# Move the model to the device\n",
    "model = model.to(device)\n",
    "E_test = E_test.to(device)\n",
    "\n",
    "# Initialize variables for tracking\n",
    "best_model = None\n",
    "best_val_loss = float('inf')\n",
    "best_epoch = 0\n",
    "epochs_without_improvement = 0\n",
    "patience = 250  # for early stopping\n",
    "\n",
    "# For plotting\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "\n",
    "try:\n",
    "    start_time = time.time()\n",
    "    for epoch in range(epochs_dl):\n",
    "        # Training\n",
    "        model.train()\n",
    "        pred_train = model(x_categ[:x_cont_train.shape[0]], x_cont_train)\n",
    "        loss = loss_fn(pred_train, E_train)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "    \n",
    "        # Gradient clipping\n",
    "        nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Validation\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            pred_val = model(x_categ[:x_cont_test.shape[0]], x_cont_test)\n",
    "            val_loss = loss_fn(pred_val, E_test)\n",
    "        \n",
    "        # Update scheduler\n",
    "        if scheduler:\n",
    "            scheduler.step(val_loss)\n",
    "        \n",
    "        # Check if validation loss has improved\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            best_epoch = epoch\n",
    "            best_model = copy.deepcopy(model.state_dict())\n",
    "            epochs_without_improvement = 0\n",
    "        else:\n",
    "            epochs_without_improvement += 1\n",
    "        \n",
    "        # Early stopping\n",
    "        if epochs_without_improvement >= patience:\n",
    "            print(f'Early stopping at epoch {epoch} due to no improvement in validation loss.')\n",
    "            break\n",
    "        \n",
    "        # Store losses for plotting\n",
    "        train_losses.append(loss.item())\n",
    "        val_losses.append(val_loss.item())\n",
    "        \n",
    "        # Print progress\n",
    "        if epoch % 10 == 0:\n",
    "            print(f'Epoch {epoch}, Train Loss: {loss.item():.4f}, Val Loss: {val_loss.item():.4f}')\n",
    "            print(f'Learning rate: {optimizer.param_groups[0][\"lr\"]:.6f}')\n",
    "except KeyboardInterrupt:\n",
    "    print(\"Training interrupted by user.\")\n",
    "\n",
    "finally:\n",
    "    print(f\"Time taken: {time.time() - start_time}\")\n",
    "    print(f'Best epoch: {best_epoch}, Best validation loss: {best_val_loss:.4f}')\n",
    "    \n",
    "    # Load the best model\n",
    "    model.load_state_dict(best_model)\n",
    "    \n",
    "    # Plot the loss values of the training set and the validation set\n",
    "    plt.plot(train_losses, label='Training Loss')\n",
    "    plt.plot(val_losses, label='Validation Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    # Test the model\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        pred_test = model(x_categ[:x_cont_test.shape[0]], x_cont_test)\n",
    "    \n",
    "    # Move the predictions back to the CPU for further processing\n",
    "    pred_test = pred_test.to(\"cpu\")\n",
    "    E_test = E_test.to(\"cpu\")\n",
    "    # Denormalize the E_tensor\n",
    "    E_test_denorm = denormalize_data(E_test, E_return, E_return2, scaling_type)\n",
    "    E_test_denorm_np = E_test_denorm.numpy()\n",
    "    # Denormalize the predictions\n",
    "    pred_test_denorm = denormalize_data(pred_test, E_return, E_return2, scaling_type)\n",
    "    pred_test_denorm_np = pred_test_denorm.numpy()\n",
    "    \n",
    "    # MAPE\n",
    "    mape = np.mean(np.abs((E_test_denorm_np - pred_test_denorm_np) / np.abs(E_test_denorm_np))) * 100\n",
    "    # MdAPE\n",
    "    mdape = np.median(np.abs((E_test_denorm_np - pred_test_denorm_np) / np.abs(E_test_denorm_np))) * 100\n",
    "    \n",
    "    # Calculate the difference\n",
    "    difference = E_test_denorm_np - pred_test_denorm_np\n",
    "    \n",
    "    \n",
    "    sample = random.randint(0,len(pred_test_denorm))\n",
    "    print(\"Difference:\\n\" + format_array(difference[sample], 4))\n",
    "    print(\"\\nE_test_denorm:\\n\" + format_array(E_test_denorm_np[sample], 4))\n",
    "    print(\"\\nPred_test_denorm:\\n\" + format_array(pred_test_denorm_np[sample], 4))\n",
    "    print(f\"\\n\\nMAPE: {mape.item():.4f}%\")\n",
    "    print(f'MdAPE: {mdape}%')\n",
    "    print(f'Best epoch: {best_epoch}')\n",
    "\n",
    "    model_name = 'warren_sp25_run15k_edit_original.pth'\n",
    "    torch.save(model.state_dict(), model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-08T17:55:02.905294Z",
     "iopub.status.busy": "2025-02-08T17:55:02.904934Z"
    },
    "scrolled": true,
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Train Loss: 0.8384, Val Loss: 0.7481\n",
      "Learning rate: 0.000100\n",
      "Epoch 10, Train Loss: 0.4270, Val Loss: 0.6453\n",
      "Learning rate: 0.000100\n",
      "Epoch 20, Train Loss: 0.1837, Val Loss: 0.4496\n",
      "Learning rate: 0.000100\n",
      "Epoch 30, Train Loss: 0.1578, Val Loss: 0.3229\n",
      "Learning rate: 0.000100\n",
      "Epoch 40, Train Loss: 0.1374, Val Loss: 0.3452\n",
      "Learning rate: 0.000100\n",
      "Epoch 50, Train Loss: 0.1284, Val Loss: 0.3696\n",
      "Learning rate: 0.000100\n",
      "Epoch 60, Train Loss: 0.1213, Val Loss: 0.3124\n",
      "Learning rate: 0.000098\n",
      "Epoch 70, Train Loss: 0.1151, Val Loss: 0.2453\n",
      "Learning rate: 0.000098\n",
      "Epoch 80, Train Loss: 0.1100, Val Loss: 0.1881\n",
      "Learning rate: 0.000098\n",
      "Epoch 90, Train Loss: 0.1059, Val Loss: 0.1531\n",
      "Learning rate: 0.000098\n",
      "Epoch 100, Train Loss: 0.1022, Val Loss: 0.1369\n",
      "Learning rate: 0.000098\n",
      "Epoch 110, Train Loss: 0.0989, Val Loss: 0.1227\n",
      "Learning rate: 0.000098\n",
      "Epoch 120, Train Loss: 0.0958, Val Loss: 0.1183\n",
      "Learning rate: 0.000098\n",
      "Epoch 130, Train Loss: 0.0930, Val Loss: 0.1258\n",
      "Learning rate: 0.000098\n",
      "Epoch 140, Train Loss: 0.0904, Val Loss: 0.1160\n",
      "Learning rate: 0.000098\n",
      "Epoch 150, Train Loss: 0.0880, Val Loss: 0.1191\n",
      "Learning rate: 0.000098\n",
      "Epoch 160, Train Loss: 0.0858, Val Loss: 0.1121\n",
      "Learning rate: 0.000098\n",
      "Epoch 170, Train Loss: 0.0838, Val Loss: 0.1222\n",
      "Learning rate: 0.000098\n",
      "Epoch 180, Train Loss: 0.0820, Val Loss: 0.1076\n",
      "Learning rate: 0.000098\n",
      "Epoch 190, Train Loss: 0.0803, Val Loss: 0.1120\n",
      "Learning rate: 0.000098\n",
      "Epoch 200, Train Loss: 0.0786, Val Loss: 0.1060\n",
      "Learning rate: 0.000096\n",
      "Epoch 210, Train Loss: 0.0771, Val Loss: 0.1028\n",
      "Learning rate: 0.000096\n",
      "Epoch 220, Train Loss: 0.0757, Val Loss: 0.1141\n",
      "Learning rate: 0.000096\n",
      "Epoch 230, Train Loss: 0.0744, Val Loss: 0.1074\n",
      "Learning rate: 0.000094\n",
      "Epoch 240, Train Loss: 0.0730, Val Loss: 0.0976\n",
      "Learning rate: 0.000094\n",
      "Epoch 250, Train Loss: 0.0719, Val Loss: 0.1003\n",
      "Learning rate: 0.000094\n",
      "Epoch 260, Train Loss: 0.0708, Val Loss: 0.1101\n",
      "Learning rate: 0.000094\n"
     ]
    }
   ],
   "source": [
    "class CombinedRegressionLoss(nn.Module):\n",
    "    def __init__(self, alpha=0.4, beta=0.4, gamma=0.2):\n",
    "        super().__init__()\n",
    "        self.alpha = alpha\n",
    "        self.beta = beta\n",
    "        self.gamma = gamma\n",
    "        self.huber_loss = nn.HuberLoss(delta=1.0)\n",
    "        self.mse_loss = nn.MSELoss()\n",
    "        self.mae_loss = nn.L1Loss()\n",
    "\n",
    "    def forward(self, pred, target):\n",
    "        huber = self.huber_loss(pred, target)\n",
    "        mse = self.mse_loss(pred, target)\n",
    "        mae = self.mae_loss(pred, target)\n",
    "        return self.alpha * huber + self.beta * mse + self.gamma * mae\n",
    "\n",
    "class NormalizedCombinedRegressionLoss_original(nn.Module):\n",
    "    def __init__(self, alpha=0.4, beta=0.4, gamma=0.2):\n",
    "        super().__init__()\n",
    "        self.alpha = alpha\n",
    "        self.beta = beta\n",
    "        self.gamma = gamma\n",
    "        self.huber_loss = nn.HuberLoss(delta=1.0)\n",
    "        self.mse_loss = nn.MSELoss()\n",
    "        self.mae_loss = nn.L1Loss()\n",
    "\n",
    "    def forward(self, pred, target):\n",
    "        huber = self.huber_loss(pred, target)\n",
    "        mse = self.mse_loss(pred, target)\n",
    "        mae = self.mae_loss(pred, target)\n",
    "\n",
    "        # Scale losses based on their theoretical properties\n",
    "        # Huber loss is already balanced, so we keep it as is\n",
    "        # MSE is quadratic, so we take its square root to make it comparable\n",
    "        # MAE is linear, so we keep it as is\n",
    "        scaled_mse = torch.sqrt(mse)\n",
    "\n",
    "        return self.alpha * huber + self.beta * scaled_mse + self.gamma * mae\n",
    "\n",
    "\n",
    "# Set the seed for reproducibility\n",
    "seed = 42\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "\n",
    "# Define combined loss function, optimizer and scheduler\n",
    "loss_fn = NormalizedCombinedRegressionLoss_original(alpha=0.4, beta=0.4, gamma=0.2)\n",
    "optimizer = optim.AdamW(model.parameters(), lr=1e-4, weight_decay=0.01)\n",
    "scheduler = ReduceLROnPlateau(optimizer, 'min', patience=20, factor=0.98)\n",
    "\n",
    "# Move the model to the device\n",
    "model = model.to(device)\n",
    "E_test = E_test.to(device)\n",
    "\n",
    "# Initialize variables for tracking\n",
    "best_model = None\n",
    "best_val_loss = float('inf')\n",
    "best_epoch = 0\n",
    "epochs_without_improvement = 0\n",
    "patience = 250  # for early stopping\n",
    "\n",
    "# For plotting\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "\n",
    "try:\n",
    "    start_time = time.time()\n",
    "    for epoch in range(epochs_dl):\n",
    "        # Training\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        pred_train = model(x_categ[:x_cont_train.shape[0]], x_cont_train)\n",
    "        loss = loss_fn(pred_train, E_train)\n",
    "        loss.backward()\n",
    "        nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Validation\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            pred_val = model(x_categ[:x_cont_test.shape[0]], x_cont_test)\n",
    "            val_loss = loss_fn(pred_val, E_test)\n",
    "        \n",
    "        # Update scheduler\n",
    "        scheduler.step(val_loss)\n",
    "        \n",
    "        # Check if validation loss has improved\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            best_epoch = epoch\n",
    "            best_model = copy.deepcopy(model.state_dict())\n",
    "            epochs_without_improvement = 0\n",
    "        else:\n",
    "            epochs_without_improvement += 1\n",
    "        \n",
    "        # Early stopping\n",
    "        if epochs_without_improvement >= patience:\n",
    "            print(f'Early stopping at epoch {epoch} due to no improvement in validation loss.')\n",
    "            break\n",
    "        \n",
    "        # Store losses for plotting\n",
    "        train_losses.append(loss.item())\n",
    "        val_losses.append(val_loss.item())\n",
    "        \n",
    "        # Clear cache to free memory\n",
    "        torch.cuda.empty_cache()\n",
    "        \n",
    "        # Print progress\n",
    "        if epoch % 10 == 0:\n",
    "            print(f'Epoch {epoch}, Train Loss: {loss.item():.4f}, Val Loss: {val_loss.item():.4f}')\n",
    "            print(f'Learning rate: {optimizer.param_groups[0][\"lr\"]:.6f}')\n",
    "except KeyboardInterrupt:\n",
    "    print(\"Training interrupted by user.\")\n",
    "\n",
    "finally:\n",
    "    print(f\"Time taken: {time.time() - start_time}\")\n",
    "    print(f'Best epoch: {best_epoch}, Best validation loss: {best_val_loss:.4f}')\n",
    "    \n",
    "    # Load the best model\n",
    "    model.load_state_dict(best_model)\n",
    "    \n",
    "    # Plot the loss values of the training set and the validation set\n",
    "    plt.plot(train_losses, label='Training Loss')\n",
    "    plt.plot(val_losses, label='Validation Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    # Test the model\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        pred_test = model(x_categ[:x_cont_test.shape[0]], x_cont_test)\n",
    "    \n",
    "    # Move the predictions back to the CPU for further processing\n",
    "    pred_test = pred_test.to(\"cpu\")\n",
    "    E_test = E_test.to(\"cpu\")\n",
    "    # Denormalize the E_tensor\n",
    "    E_test_denorm = denormalize_data(E_test, E_return, E_return2, scaling_type)\n",
    "    E_test_denorm_np = E_test_denorm.numpy()\n",
    "    # Denormalize the predictions\n",
    "    pred_test_denorm = denormalize_data(pred_test, E_return, E_return2, scaling_type)\n",
    "    pred_test_denorm_np = pred_test_denorm.numpy()\n",
    "    \n",
    "    # MAPE\n",
    "    mape = np.mean(np.abs((E_test_denorm_np - pred_test_denorm_np) / np.abs(E_test_denorm_np))) * 100\n",
    "    # MdAPE\n",
    "    mdape = np.median(np.abs((E_test_denorm_np - pred_test_denorm_np) / np.abs(E_test_denorm_np))) * 100\n",
    "    \n",
    "    # Calculate the difference\n",
    "    difference = E_test_denorm_np - pred_test_denorm_np\n",
    "    \n",
    "    \n",
    "    sample = random.randint(0,len(pred_test_denorm))\n",
    "    print(\"Difference:\\n\" + format_array(difference[sample], 4))\n",
    "    print(\"\\nE_test_denorm:\\n\" + format_array(E_test_denorm_np[sample], 4))\n",
    "    print(\"\\nPred_test_denorm:\\n\" + format_array(pred_test_denorm_np[sample], 4))\n",
    "    print(f\"\\n\\nMAPE: {mape.item():.4f}%\")\n",
    "    print(f'MdAPE: {mdape}%')\n",
    "    print(f'Best epoch: {best_epoch}')\n",
    "\n",
    "    model_name = 'warren_sp25_run15k_edit_combinedregression.pth'\n",
    "    torch.save(model.state_dict(), model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-08T12:35:50.544942Z",
     "iopub.status.busy": "2025-02-08T12:35:50.544574Z",
     "iopub.status.idle": "2025-02-08T15:07:29.683414Z",
     "shell.execute_reply": "2025-02-08T15:07:29.682355Z",
     "shell.execute_reply.started": "2025-02-08T12:35:50.544907Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Train Loss: 0.8396, Val Loss: 0.7718\n",
      "Learning rate: 0.000100\n",
      "Epoch 10, Train Loss: 0.4177, Val Loss: 0.6333\n",
      "Learning rate: 0.000100\n",
      "Epoch 20, Train Loss: 0.1739, Val Loss: 0.4252\n",
      "Learning rate: 0.000100\n",
      "Epoch 30, Train Loss: 0.1535, Val Loss: 0.3122\n",
      "Learning rate: 0.000100\n",
      "Epoch 40, Train Loss: 0.1354, Val Loss: 0.3200\n",
      "Learning rate: 0.000100\n",
      "Epoch 50, Train Loss: 0.1268, Val Loss: 0.3481\n",
      "Learning rate: 0.000099\n",
      "Epoch 60, Train Loss: 0.1198, Val Loss: 0.2859\n",
      "Learning rate: 0.000099\n",
      "Epoch 70, Train Loss: 0.1140, Val Loss: 0.2152\n",
      "Learning rate: 0.000099\n",
      "Epoch 80, Train Loss: 0.1094, Val Loss: 0.1699\n",
      "Learning rate: 0.000099\n",
      "Epoch 90, Train Loss: 0.1055, Val Loss: 0.1429\n",
      "Learning rate: 0.000099\n",
      "Epoch 100, Train Loss: 0.1020, Val Loss: 0.1314\n",
      "Learning rate: 0.000099\n",
      "Epoch 110, Train Loss: 0.0988, Val Loss: 0.1238\n",
      "Learning rate: 0.000099\n",
      "Epoch 120, Train Loss: 0.0958, Val Loss: 0.1216\n",
      "Learning rate: 0.000099\n",
      "Epoch 130, Train Loss: 0.0929, Val Loss: 0.1209\n",
      "Learning rate: 0.000099\n",
      "Epoch 140, Train Loss: 0.0904, Val Loss: 0.1143\n",
      "Learning rate: 0.000099\n",
      "Epoch 150, Train Loss: 0.0879, Val Loss: 0.1198\n",
      "Learning rate: 0.000098\n",
      "Epoch 160, Train Loss: 0.0857, Val Loss: 0.1169\n",
      "Learning rate: 0.000097\n",
      "Epoch 170, Train Loss: 0.0838, Val Loss: 0.1251\n",
      "Learning rate: 0.000097\n",
      "Epoch 180, Train Loss: 0.0819, Val Loss: 0.1160\n",
      "Learning rate: 0.000096\n",
      "Epoch 190, Train Loss: 0.0802, Val Loss: 0.1096\n",
      "Learning rate: 0.000095\n",
      "Epoch 200, Train Loss: 0.0788, Val Loss: 0.1110\n",
      "Learning rate: 0.000095\n",
      "Epoch 210, Train Loss: 0.0772, Val Loss: 0.1112\n",
      "Learning rate: 0.000094\n",
      "Epoch 220, Train Loss: 0.0758, Val Loss: 0.1053\n",
      "Learning rate: 0.000094\n",
      "Epoch 230, Train Loss: 0.0746, Val Loss: 0.1016\n",
      "Learning rate: 0.000093\n",
      "Epoch 240, Train Loss: 0.0733, Val Loss: 0.1028\n",
      "Learning rate: 0.000093\n",
      "Epoch 250, Train Loss: 0.0721, Val Loss: 0.0991\n",
      "Learning rate: 0.000093\n",
      "Epoch 260, Train Loss: 0.0710, Val Loss: 0.1013\n",
      "Learning rate: 0.000093\n",
      "Epoch 270, Train Loss: 0.0700, Val Loss: 0.1102\n",
      "Learning rate: 0.000093\n",
      "Epoch 280, Train Loss: 0.0691, Val Loss: 0.1079\n",
      "Learning rate: 0.000092\n",
      "Epoch 290, Train Loss: 0.0681, Val Loss: 0.0966\n",
      "Learning rate: 0.000091\n",
      "Epoch 300, Train Loss: 0.0673, Val Loss: 0.1149\n",
      "Learning rate: 0.000091\n",
      "Epoch 310, Train Loss: 0.0664, Val Loss: 0.0963\n",
      "Learning rate: 0.000090\n",
      "Epoch 320, Train Loss: 0.0656, Val Loss: 0.0961\n",
      "Learning rate: 0.000090\n",
      "Epoch 330, Train Loss: 0.0648, Val Loss: 0.0896\n",
      "Learning rate: 0.000090\n",
      "Epoch 340, Train Loss: 0.0640, Val Loss: 0.0920\n",
      "Learning rate: 0.000090\n",
      "Epoch 350, Train Loss: 0.0633, Val Loss: 0.0896\n",
      "Learning rate: 0.000090\n",
      "Epoch 360, Train Loss: 0.0626, Val Loss: 0.0847\n",
      "Learning rate: 0.000089\n",
      "Epoch 370, Train Loss: 0.0620, Val Loss: 0.0737\n",
      "Learning rate: 0.000089\n",
      "Epoch 380, Train Loss: 0.0614, Val Loss: 0.0842\n",
      "Learning rate: 0.000089\n",
      "Epoch 390, Train Loss: 0.0610, Val Loss: 0.0841\n",
      "Learning rate: 0.000088\n",
      "Epoch 400, Train Loss: 0.0605, Val Loss: 0.0854\n",
      "Learning rate: 0.000087\n",
      "Epoch 410, Train Loss: 0.0599, Val Loss: 0.0870\n",
      "Learning rate: 0.000087\n",
      "Epoch 420, Train Loss: 0.0594, Val Loss: 0.0680\n",
      "Learning rate: 0.000086\n",
      "Epoch 430, Train Loss: 0.0590, Val Loss: 0.0776\n",
      "Learning rate: 0.000086\n",
      "Epoch 440, Train Loss: 0.0585, Val Loss: 0.0732\n",
      "Learning rate: 0.000085\n",
      "Epoch 450, Train Loss: 0.0581, Val Loss: 0.0736\n",
      "Learning rate: 0.000085\n",
      "Epoch 460, Train Loss: 0.0577, Val Loss: 0.0767\n",
      "Learning rate: 0.000084\n",
      "Epoch 470, Train Loss: 0.0573, Val Loss: 0.0791\n",
      "Learning rate: 0.000083\n",
      "Epoch 480, Train Loss: 0.0569, Val Loss: 0.0750\n",
      "Learning rate: 0.000083\n",
      "Epoch 490, Train Loss: 0.0566, Val Loss: 0.0807\n",
      "Learning rate: 0.000083\n",
      "Epoch 500, Train Loss: 0.0562, Val Loss: 0.0858\n",
      "Learning rate: 0.000083\n",
      "Epoch 510, Train Loss: 0.0558, Val Loss: 0.0648\n",
      "Learning rate: 0.000082\n",
      "Epoch 520, Train Loss: 0.0555, Val Loss: 0.0679\n",
      "Learning rate: 0.000082\n",
      "Epoch 530, Train Loss: 0.0552, Val Loss: 0.0750\n",
      "Learning rate: 0.000081\n",
      "Epoch 540, Train Loss: 0.0549, Val Loss: 0.0751\n",
      "Learning rate: 0.000081\n",
      "Epoch 550, Train Loss: 0.0545, Val Loss: 0.0699\n",
      "Learning rate: 0.000080\n",
      "Epoch 560, Train Loss: 0.0543, Val Loss: 0.0834\n",
      "Learning rate: 0.000079\n",
      "Epoch 570, Train Loss: 0.0540, Val Loss: 0.0625\n",
      "Learning rate: 0.000079\n",
      "Epoch 580, Train Loss: 0.0537, Val Loss: 0.0658\n",
      "Learning rate: 0.000079\n",
      "Epoch 590, Train Loss: 0.0533, Val Loss: 0.0615\n",
      "Learning rate: 0.000079\n",
      "Epoch 600, Train Loss: 0.0530, Val Loss: 0.0614\n",
      "Learning rate: 0.000079\n",
      "Epoch 610, Train Loss: 0.0530, Val Loss: 0.0647\n",
      "Learning rate: 0.000079\n",
      "Epoch 620, Train Loss: 0.0525, Val Loss: 0.0664\n",
      "Learning rate: 0.000079\n",
      "Epoch 630, Train Loss: 0.0523, Val Loss: 0.0685\n",
      "Learning rate: 0.000078\n",
      "Epoch 640, Train Loss: 0.0521, Val Loss: 0.0749\n",
      "Learning rate: 0.000078\n",
      "Epoch 650, Train Loss: 0.0518, Val Loss: 0.0649\n",
      "Learning rate: 0.000077\n",
      "Epoch 660, Train Loss: 0.0516, Val Loss: 0.0635\n",
      "Learning rate: 0.000077\n",
      "Epoch 670, Train Loss: 0.0513, Val Loss: 0.0548\n",
      "Learning rate: 0.000076\n",
      "Epoch 680, Train Loss: 0.0510, Val Loss: 0.0568\n",
      "Learning rate: 0.000076\n",
      "Epoch 690, Train Loss: 0.0508, Val Loss: 0.0526\n",
      "Learning rate: 0.000075\n",
      "Epoch 700, Train Loss: 0.0506, Val Loss: 0.0591\n",
      "Learning rate: 0.000075\n",
      "Epoch 710, Train Loss: 0.0504, Val Loss: 0.0622\n",
      "Learning rate: 0.000075\n",
      "Epoch 720, Train Loss: 0.0502, Val Loss: 0.0551\n",
      "Learning rate: 0.000075\n",
      "Epoch 730, Train Loss: 0.0499, Val Loss: 0.0545\n",
      "Learning rate: 0.000074\n",
      "Epoch 740, Train Loss: 0.0498, Val Loss: 0.0585\n",
      "Learning rate: 0.000073\n",
      "Epoch 750, Train Loss: 0.0496, Val Loss: 0.0633\n",
      "Learning rate: 0.000073\n",
      "Epoch 760, Train Loss: 0.0493, Val Loss: 0.0596\n",
      "Learning rate: 0.000072\n",
      "Epoch 770, Train Loss: 0.0492, Val Loss: 0.0587\n",
      "Learning rate: 0.000072\n",
      "Epoch 780, Train Loss: 0.0490, Val Loss: 0.0580\n",
      "Learning rate: 0.000072\n",
      "Epoch 790, Train Loss: 0.0488, Val Loss: 0.0530\n",
      "Learning rate: 0.000071\n",
      "Epoch 800, Train Loss: 0.0486, Val Loss: 0.0507\n",
      "Learning rate: 0.000071\n",
      "Epoch 810, Train Loss: 0.0484, Val Loss: 0.0670\n",
      "Learning rate: 0.000070\n",
      "Epoch 820, Train Loss: 0.0483, Val Loss: 0.0607\n",
      "Learning rate: 0.000070\n",
      "Epoch 830, Train Loss: 0.0482, Val Loss: 0.0602\n",
      "Learning rate: 0.000070\n",
      "Epoch 840, Train Loss: 0.0479, Val Loss: 0.0593\n",
      "Learning rate: 0.000069\n",
      "Epoch 850, Train Loss: 0.0477, Val Loss: 0.0611\n",
      "Learning rate: 0.000069\n",
      "Epoch 860, Train Loss: 0.0476, Val Loss: 0.0500\n",
      "Learning rate: 0.000068\n",
      "Epoch 870, Train Loss: 0.0474, Val Loss: 0.0553\n",
      "Learning rate: 0.000068\n",
      "Epoch 880, Train Loss: 0.0473, Val Loss: 0.0532\n",
      "Learning rate: 0.000068\n",
      "Epoch 890, Train Loss: 0.0471, Val Loss: 0.0498\n",
      "Learning rate: 0.000068\n",
      "Epoch 900, Train Loss: 0.0469, Val Loss: 0.0519\n",
      "Learning rate: 0.000068\n",
      "Epoch 910, Train Loss: 0.0467, Val Loss: 0.0535\n",
      "Learning rate: 0.000067\n",
      "Epoch 920, Train Loss: 0.0466, Val Loss: 0.0485\n",
      "Learning rate: 0.000067\n",
      "Epoch 930, Train Loss: 0.0466, Val Loss: 0.0502\n",
      "Learning rate: 0.000067\n",
      "Epoch 940, Train Loss: 0.0463, Val Loss: 0.0504\n",
      "Learning rate: 0.000067\n",
      "Epoch 950, Train Loss: 0.0462, Val Loss: 0.0501\n",
      "Learning rate: 0.000066\n",
      "Epoch 960, Train Loss: 0.0460, Val Loss: 0.0520\n",
      "Learning rate: 0.000066\n",
      "Epoch 970, Train Loss: 0.0459, Val Loss: 0.0534\n",
      "Learning rate: 0.000066\n",
      "Epoch 980, Train Loss: 0.0457, Val Loss: 0.0588\n",
      "Learning rate: 0.000066\n",
      "Epoch 990, Train Loss: 0.0456, Val Loss: 0.0528\n",
      "Learning rate: 0.000065\n",
      "Epoch 1000, Train Loss: 0.0455, Val Loss: 0.0473\n",
      "Learning rate: 0.000065\n",
      "Epoch 1010, Train Loss: 0.0454, Val Loss: 0.0533\n",
      "Learning rate: 0.000065\n",
      "Epoch 1020, Train Loss: 0.0452, Val Loss: 0.0538\n",
      "Learning rate: 0.000064\n",
      "Epoch 1030, Train Loss: 0.0451, Val Loss: 0.0511\n",
      "Learning rate: 0.000064\n",
      "Epoch 1040, Train Loss: 0.0449, Val Loss: 0.0466\n",
      "Learning rate: 0.000064\n",
      "Epoch 1050, Train Loss: 0.0448, Val Loss: 0.0511\n",
      "Learning rate: 0.000064\n",
      "Epoch 1060, Train Loss: 0.0447, Val Loss: 0.0524\n",
      "Learning rate: 0.000064\n",
      "Epoch 1070, Train Loss: 0.0445, Val Loss: 0.0462\n",
      "Learning rate: 0.000063\n",
      "Epoch 1080, Train Loss: 0.0444, Val Loss: 0.0535\n",
      "Learning rate: 0.000062\n",
      "Epoch 1090, Train Loss: 0.0443, Val Loss: 0.0539\n",
      "Learning rate: 0.000062\n",
      "Epoch 1100, Train Loss: 0.0442, Val Loss: 0.0529\n",
      "Learning rate: 0.000062\n",
      "Epoch 1110, Train Loss: 0.0441, Val Loss: 0.0528\n",
      "Learning rate: 0.000062\n",
      "Epoch 1120, Train Loss: 0.0439, Val Loss: 0.0441\n",
      "Learning rate: 0.000061\n",
      "Epoch 1130, Train Loss: 0.0438, Val Loss: 0.0545\n",
      "Learning rate: 0.000061\n",
      "Epoch 1140, Train Loss: 0.0437, Val Loss: 0.0557\n",
      "Learning rate: 0.000061\n",
      "Epoch 1150, Train Loss: 0.0436, Val Loss: 0.0466\n",
      "Learning rate: 0.000061\n",
      "Epoch 1160, Train Loss: 0.0435, Val Loss: 0.0457\n",
      "Learning rate: 0.000060\n",
      "Epoch 1170, Train Loss: 0.0433, Val Loss: 0.0463\n",
      "Learning rate: 0.000059\n",
      "Epoch 1180, Train Loss: 0.0432, Val Loss: 0.0490\n",
      "Learning rate: 0.000059\n",
      "Epoch 1190, Train Loss: 0.0431, Val Loss: 0.0512\n",
      "Learning rate: 0.000059\n",
      "Epoch 1200, Train Loss: 0.0430, Val Loss: 0.0479\n",
      "Learning rate: 0.000059\n",
      "Epoch 1210, Train Loss: 0.0429, Val Loss: 0.0473\n",
      "Learning rate: 0.000058\n",
      "Epoch 1220, Train Loss: 0.0428, Val Loss: 0.0516\n",
      "Learning rate: 0.000058\n",
      "Epoch 1230, Train Loss: 0.0427, Val Loss: 0.0436\n",
      "Learning rate: 0.000058\n",
      "Epoch 1240, Train Loss: 0.0425, Val Loss: 0.0460\n",
      "Learning rate: 0.000058\n",
      "Epoch 1250, Train Loss: 0.0424, Val Loss: 0.0441\n",
      "Learning rate: 0.000057\n",
      "Epoch 1260, Train Loss: 0.0423, Val Loss: 0.0421\n",
      "Learning rate: 0.000057\n",
      "Epoch 1270, Train Loss: 0.0423, Val Loss: 0.0513\n",
      "Learning rate: 0.000056\n",
      "Epoch 1280, Train Loss: 0.0422, Val Loss: 0.0482\n",
      "Learning rate: 0.000056\n",
      "Epoch 1290, Train Loss: 0.0421, Val Loss: 0.0437\n",
      "Learning rate: 0.000056\n",
      "Epoch 1300, Train Loss: 0.0420, Val Loss: 0.0458\n",
      "Learning rate: 0.000055\n",
      "Epoch 1310, Train Loss: 0.0418, Val Loss: 0.0433\n",
      "Learning rate: 0.000055\n",
      "Epoch 1320, Train Loss: 0.0418, Val Loss: 0.0472\n",
      "Learning rate: 0.000055\n",
      "Epoch 1330, Train Loss: 0.0417, Val Loss: 0.0452\n",
      "Learning rate: 0.000054\n",
      "Epoch 1340, Train Loss: 0.0416, Val Loss: 0.0438\n",
      "Learning rate: 0.000054\n",
      "Epoch 1350, Train Loss: 0.0414, Val Loss: 0.0420\n",
      "Learning rate: 0.000054\n",
      "Epoch 1360, Train Loss: 0.0413, Val Loss: 0.0408\n",
      "Learning rate: 0.000054\n",
      "Epoch 1370, Train Loss: 0.0413, Val Loss: 0.0407\n",
      "Learning rate: 0.000054\n",
      "Epoch 1380, Train Loss: 0.0412, Val Loss: 0.0440\n",
      "Learning rate: 0.000053\n",
      "Epoch 1390, Train Loss: 0.0411, Val Loss: 0.0411\n",
      "Learning rate: 0.000053\n",
      "Epoch 1400, Train Loss: 0.0410, Val Loss: 0.0456\n",
      "Learning rate: 0.000053\n",
      "Epoch 1410, Train Loss: 0.0409, Val Loss: 0.0440\n",
      "Learning rate: 0.000053\n",
      "Epoch 1420, Train Loss: 0.0408, Val Loss: 0.0472\n",
      "Learning rate: 0.000052\n",
      "Epoch 1430, Train Loss: 0.0407, Val Loss: 0.0483\n",
      "Learning rate: 0.000052\n",
      "Epoch 1440, Train Loss: 0.0406, Val Loss: 0.0409\n",
      "Learning rate: 0.000052\n",
      "Epoch 1450, Train Loss: 0.0405, Val Loss: 0.0407\n",
      "Learning rate: 0.000051\n",
      "Epoch 1460, Train Loss: 0.0404, Val Loss: 0.0410\n",
      "Learning rate: 0.000051\n",
      "Epoch 1470, Train Loss: 0.0403, Val Loss: 0.0429\n",
      "Learning rate: 0.000050\n",
      "Epoch 1480, Train Loss: 0.0403, Val Loss: 0.0415\n",
      "Learning rate: 0.000050\n",
      "Epoch 1490, Train Loss: 0.0402, Val Loss: 0.0402\n",
      "Learning rate: 0.000050\n",
      "Epoch 1500, Train Loss: 0.0401, Val Loss: 0.0424\n",
      "Learning rate: 0.000050\n",
      "Epoch 1510, Train Loss: 0.0400, Val Loss: 0.0512\n",
      "Learning rate: 0.000050\n",
      "Epoch 1520, Train Loss: 0.0399, Val Loss: 0.0422\n",
      "Learning rate: 0.000049\n",
      "Epoch 1530, Train Loss: 0.0398, Val Loss: 0.0404\n",
      "Learning rate: 0.000049\n",
      "Epoch 1540, Train Loss: 0.0397, Val Loss: 0.0394\n",
      "Learning rate: 0.000049\n",
      "Epoch 1550, Train Loss: 0.0396, Val Loss: 0.0440\n",
      "Learning rate: 0.000048\n",
      "Epoch 1560, Train Loss: 0.0396, Val Loss: 0.0397\n",
      "Learning rate: 0.000048\n",
      "Epoch 1570, Train Loss: 0.0395, Val Loss: 0.0390\n",
      "Learning rate: 0.000048\n",
      "Epoch 1580, Train Loss: 0.0394, Val Loss: 0.0432\n",
      "Learning rate: 0.000048\n",
      "Epoch 1590, Train Loss: 0.0393, Val Loss: 0.0388\n",
      "Learning rate: 0.000048\n",
      "Epoch 1600, Train Loss: 0.0392, Val Loss: 0.0431\n",
      "Learning rate: 0.000047\n",
      "Epoch 1610, Train Loss: 0.0392, Val Loss: 0.0399\n",
      "Learning rate: 0.000047\n",
      "Epoch 1620, Train Loss: 0.0391, Val Loss: 0.0421\n",
      "Learning rate: 0.000047\n",
      "Epoch 1630, Train Loss: 0.0390, Val Loss: 0.0434\n",
      "Learning rate: 0.000046\n",
      "Epoch 1640, Train Loss: 0.0389, Val Loss: 0.0370\n",
      "Learning rate: 0.000046\n",
      "Epoch 1650, Train Loss: 0.0388, Val Loss: 0.0432\n",
      "Learning rate: 0.000046\n",
      "Epoch 1660, Train Loss: 0.0388, Val Loss: 0.0376\n",
      "Learning rate: 0.000045\n",
      "Epoch 1670, Train Loss: 0.0387, Val Loss: 0.0396\n",
      "Learning rate: 0.000045\n",
      "Epoch 1680, Train Loss: 0.0386, Val Loss: 0.0355\n",
      "Learning rate: 0.000045\n",
      "Epoch 1690, Train Loss: 0.0385, Val Loss: 0.0414\n",
      "Learning rate: 0.000045\n",
      "Epoch 1700, Train Loss: 0.0385, Val Loss: 0.0420\n",
      "Learning rate: 0.000044\n",
      "Epoch 1710, Train Loss: 0.0383, Val Loss: 0.0386\n",
      "Learning rate: 0.000044\n",
      "Epoch 1720, Train Loss: 0.0383, Val Loss: 0.0381\n",
      "Learning rate: 0.000044\n",
      "Epoch 1730, Train Loss: 0.0382, Val Loss: 0.0379\n",
      "Learning rate: 0.000043\n",
      "Epoch 1740, Train Loss: 0.0382, Val Loss: 0.0405\n",
      "Learning rate: 0.000043\n",
      "Epoch 1750, Train Loss: 0.0381, Val Loss: 0.0419\n",
      "Learning rate: 0.000043\n",
      "Epoch 1760, Train Loss: 0.0380, Val Loss: 0.0391\n",
      "Learning rate: 0.000043\n",
      "Epoch 1770, Train Loss: 0.0380, Val Loss: 0.0408\n",
      "Learning rate: 0.000043\n",
      "Epoch 1780, Train Loss: 0.0379, Val Loss: 0.0414\n",
      "Learning rate: 0.000042\n",
      "Epoch 1790, Train Loss: 0.0378, Val Loss: 0.0436\n",
      "Learning rate: 0.000042\n",
      "Epoch 1800, Train Loss: 0.0377, Val Loss: 0.0413\n",
      "Learning rate: 0.000042\n",
      "Epoch 1810, Train Loss: 0.0377, Val Loss: 0.0360\n",
      "Learning rate: 0.000041\n",
      "Epoch 1820, Train Loss: 0.0376, Val Loss: 0.0375\n",
      "Learning rate: 0.000041\n",
      "Epoch 1830, Train Loss: 0.0375, Val Loss: 0.0352\n",
      "Learning rate: 0.000041\n",
      "Epoch 1840, Train Loss: 0.0374, Val Loss: 0.0365\n",
      "Learning rate: 0.000041\n",
      "Epoch 1850, Train Loss: 0.0374, Val Loss: 0.0398\n",
      "Learning rate: 0.000040\n",
      "Epoch 1860, Train Loss: 0.0373, Val Loss: 0.0359\n",
      "Learning rate: 0.000040\n",
      "Epoch 1870, Train Loss: 0.0373, Val Loss: 0.0392\n",
      "Learning rate: 0.000040\n",
      "Epoch 1880, Train Loss: 0.0372, Val Loss: 0.0362\n",
      "Learning rate: 0.000040\n",
      "Epoch 1890, Train Loss: 0.0371, Val Loss: 0.0396\n",
      "Learning rate: 0.000040\n",
      "Epoch 1900, Train Loss: 0.0371, Val Loss: 0.0385\n",
      "Learning rate: 0.000040\n",
      "Epoch 1910, Train Loss: 0.0370, Val Loss: 0.0361\n",
      "Learning rate: 0.000040\n",
      "Epoch 1920, Train Loss: 0.0369, Val Loss: 0.0347\n",
      "Learning rate: 0.000040\n",
      "Epoch 1930, Train Loss: 0.0369, Val Loss: 0.0362\n",
      "Learning rate: 0.000039\n",
      "Epoch 1940, Train Loss: 0.0368, Val Loss: 0.0383\n",
      "Learning rate: 0.000039\n",
      "Epoch 1950, Train Loss: 0.0367, Val Loss: 0.0374\n",
      "Learning rate: 0.000039\n",
      "Epoch 1960, Train Loss: 0.0367, Val Loss: 0.0380\n",
      "Learning rate: 0.000038\n",
      "Epoch 1970, Train Loss: 0.0366, Val Loss: 0.0346\n",
      "Learning rate: 0.000038\n",
      "Epoch 1980, Train Loss: 0.0365, Val Loss: 0.0349\n",
      "Learning rate: 0.000038\n",
      "Epoch 1990, Train Loss: 0.0365, Val Loss: 0.0405\n",
      "Learning rate: 0.000038\n",
      "Epoch 2000, Train Loss: 0.0364, Val Loss: 0.0387\n",
      "Learning rate: 0.000038\n",
      "Epoch 2010, Train Loss: 0.0364, Val Loss: 0.0355\n",
      "Learning rate: 0.000038\n",
      "Epoch 2020, Train Loss: 0.0363, Val Loss: 0.0349\n",
      "Learning rate: 0.000038\n",
      "Epoch 2030, Train Loss: 0.0362, Val Loss: 0.0339\n",
      "Learning rate: 0.000037\n",
      "Epoch 2040, Train Loss: 0.0362, Val Loss: 0.0354\n",
      "Learning rate: 0.000037\n",
      "Epoch 2050, Train Loss: 0.0361, Val Loss: 0.0331\n",
      "Learning rate: 0.000037\n",
      "Epoch 2060, Train Loss: 0.0360, Val Loss: 0.0364\n",
      "Learning rate: 0.000037\n",
      "Epoch 2070, Train Loss: 0.0360, Val Loss: 0.0340\n",
      "Learning rate: 0.000037\n",
      "Epoch 2080, Train Loss: 0.0359, Val Loss: 0.0358\n",
      "Learning rate: 0.000037\n",
      "Epoch 2090, Train Loss: 0.0359, Val Loss: 0.0354\n",
      "Learning rate: 0.000036\n",
      "Epoch 2100, Train Loss: 0.0358, Val Loss: 0.0328\n",
      "Learning rate: 0.000036\n",
      "Epoch 2110, Train Loss: 0.0357, Val Loss: 0.0342\n",
      "Learning rate: 0.000036\n",
      "Epoch 2120, Train Loss: 0.0357, Val Loss: 0.0321\n",
      "Learning rate: 0.000036\n",
      "Epoch 2130, Train Loss: 0.0356, Val Loss: 0.0348\n",
      "Learning rate: 0.000036\n",
      "Epoch 2140, Train Loss: 0.0356, Val Loss: 0.0357\n",
      "Learning rate: 0.000036\n",
      "Epoch 2150, Train Loss: 0.0355, Val Loss: 0.0334\n",
      "Learning rate: 0.000036\n",
      "Epoch 2160, Train Loss: 0.0355, Val Loss: 0.0355\n",
      "Learning rate: 0.000035\n",
      "Epoch 2170, Train Loss: 0.0354, Val Loss: 0.0346\n",
      "Learning rate: 0.000035\n",
      "Epoch 2180, Train Loss: 0.0353, Val Loss: 0.0324\n",
      "Learning rate: 0.000035\n",
      "Epoch 2190, Train Loss: 0.0353, Val Loss: 0.0349\n",
      "Learning rate: 0.000034\n",
      "Epoch 2200, Train Loss: 0.0352, Val Loss: 0.0346\n",
      "Learning rate: 0.000034\n",
      "Epoch 2210, Train Loss: 0.0351, Val Loss: 0.0338\n",
      "Learning rate: 0.000034\n",
      "Epoch 2220, Train Loss: 0.0351, Val Loss: 0.0340\n",
      "Learning rate: 0.000034\n",
      "Epoch 2230, Train Loss: 0.0350, Val Loss: 0.0321\n",
      "Learning rate: 0.000034\n",
      "Epoch 2240, Train Loss: 0.0350, Val Loss: 0.0357\n",
      "Learning rate: 0.000034\n",
      "Epoch 2250, Train Loss: 0.0349, Val Loss: 0.0333\n",
      "Learning rate: 0.000033\n",
      "Epoch 2260, Train Loss: 0.0349, Val Loss: 0.0337\n",
      "Learning rate: 0.000033\n",
      "Epoch 2270, Train Loss: 0.0348, Val Loss: 0.0345\n",
      "Learning rate: 0.000033\n",
      "Epoch 2280, Train Loss: 0.0348, Val Loss: 0.0325\n",
      "Learning rate: 0.000033\n",
      "Epoch 2290, Train Loss: 0.0347, Val Loss: 0.0351\n",
      "Learning rate: 0.000033\n",
      "Epoch 2300, Train Loss: 0.0347, Val Loss: 0.0359\n",
      "Learning rate: 0.000033\n",
      "Epoch 2310, Train Loss: 0.0346, Val Loss: 0.0335\n",
      "Learning rate: 0.000032\n",
      "Epoch 2320, Train Loss: 0.0346, Val Loss: 0.0325\n",
      "Learning rate: 0.000032\n",
      "Epoch 2330, Train Loss: 0.0345, Val Loss: 0.0338\n",
      "Learning rate: 0.000032\n",
      "Epoch 2340, Train Loss: 0.0345, Val Loss: 0.0310\n",
      "Learning rate: 0.000032\n",
      "Epoch 2350, Train Loss: 0.0344, Val Loss: 0.0328\n",
      "Learning rate: 0.000032\n",
      "Epoch 2360, Train Loss: 0.0344, Val Loss: 0.0322\n",
      "Learning rate: 0.000031\n",
      "Epoch 2370, Train Loss: 0.0343, Val Loss: 0.0302\n",
      "Learning rate: 0.000031\n",
      "Epoch 2380, Train Loss: 0.0342, Val Loss: 0.0321\n",
      "Learning rate: 0.000031\n",
      "Epoch 2390, Train Loss: 0.0342, Val Loss: 0.0326\n",
      "Learning rate: 0.000031\n",
      "Epoch 2400, Train Loss: 0.0342, Val Loss: 0.0318\n",
      "Learning rate: 0.000031\n",
      "Epoch 2410, Train Loss: 0.0341, Val Loss: 0.0307\n",
      "Learning rate: 0.000031\n",
      "Epoch 2420, Train Loss: 0.0340, Val Loss: 0.0310\n",
      "Learning rate: 0.000030\n",
      "Epoch 2430, Train Loss: 0.0340, Val Loss: 0.0303\n",
      "Learning rate: 0.000030\n",
      "Epoch 2440, Train Loss: 0.0339, Val Loss: 0.0306\n",
      "Learning rate: 0.000030\n",
      "Epoch 2450, Train Loss: 0.0339, Val Loss: 0.0304\n",
      "Learning rate: 0.000030\n",
      "Epoch 2460, Train Loss: 0.0338, Val Loss: 0.0309\n",
      "Learning rate: 0.000030\n",
      "Epoch 2470, Train Loss: 0.0338, Val Loss: 0.0312\n",
      "Learning rate: 0.000029\n",
      "Epoch 2480, Train Loss: 0.0338, Val Loss: 0.0304\n",
      "Learning rate: 0.000029\n",
      "Epoch 2490, Train Loss: 0.0337, Val Loss: 0.0304\n",
      "Learning rate: 0.000029\n",
      "Epoch 2500, Train Loss: 0.0337, Val Loss: 0.0295\n",
      "Learning rate: 0.000029\n",
      "Epoch 2510, Train Loss: 0.0336, Val Loss: 0.0321\n",
      "Learning rate: 0.000029\n",
      "Epoch 2520, Train Loss: 0.0336, Val Loss: 0.0312\n",
      "Learning rate: 0.000029\n",
      "Epoch 2530, Train Loss: 0.0335, Val Loss: 0.0301\n",
      "Learning rate: 0.000028\n",
      "Epoch 2540, Train Loss: 0.0335, Val Loss: 0.0340\n",
      "Learning rate: 0.000028\n",
      "Epoch 2550, Train Loss: 0.0334, Val Loss: 0.0315\n",
      "Learning rate: 0.000028\n",
      "Epoch 2560, Train Loss: 0.0334, Val Loss: 0.0296\n",
      "Learning rate: 0.000028\n",
      "Epoch 2570, Train Loss: 0.0333, Val Loss: 0.0290\n",
      "Learning rate: 0.000028\n",
      "Epoch 2580, Train Loss: 0.0333, Val Loss: 0.0305\n",
      "Learning rate: 0.000028\n",
      "Epoch 2590, Train Loss: 0.0333, Val Loss: 0.0288\n",
      "Learning rate: 0.000028\n",
      "Epoch 2600, Train Loss: 0.0332, Val Loss: 0.0296\n",
      "Learning rate: 0.000028\n",
      "Epoch 2610, Train Loss: 0.0332, Val Loss: 0.0292\n",
      "Learning rate: 0.000028\n",
      "Epoch 2620, Train Loss: 0.0331, Val Loss: 0.0318\n",
      "Learning rate: 0.000028\n",
      "Epoch 2630, Train Loss: 0.0331, Val Loss: 0.0301\n",
      "Learning rate: 0.000027\n",
      "Epoch 2640, Train Loss: 0.0330, Val Loss: 0.0297\n",
      "Learning rate: 0.000027\n",
      "Epoch 2650, Train Loss: 0.0330, Val Loss: 0.0296\n",
      "Learning rate: 0.000027\n",
      "Epoch 2660, Train Loss: 0.0329, Val Loss: 0.0292\n",
      "Learning rate: 0.000027\n",
      "Epoch 2670, Train Loss: 0.0329, Val Loss: 0.0293\n",
      "Learning rate: 0.000027\n",
      "Epoch 2680, Train Loss: 0.0328, Val Loss: 0.0293\n",
      "Learning rate: 0.000027\n",
      "Epoch 2690, Train Loss: 0.0328, Val Loss: 0.0277\n",
      "Learning rate: 0.000027\n",
      "Epoch 2700, Train Loss: 0.0328, Val Loss: 0.0287\n",
      "Learning rate: 0.000027\n",
      "Epoch 2710, Train Loss: 0.0327, Val Loss: 0.0288\n",
      "Learning rate: 0.000027\n",
      "Epoch 2720, Train Loss: 0.0327, Val Loss: 0.0288\n",
      "Learning rate: 0.000027\n",
      "Epoch 2730, Train Loss: 0.0326, Val Loss: 0.0292\n",
      "Learning rate: 0.000026\n",
      "Epoch 2740, Train Loss: 0.0326, Val Loss: 0.0285\n",
      "Learning rate: 0.000026\n",
      "Epoch 2750, Train Loss: 0.0326, Val Loss: 0.0322\n",
      "Learning rate: 0.000026\n",
      "Epoch 2760, Train Loss: 0.0325, Val Loss: 0.0301\n",
      "Learning rate: 0.000026\n",
      "Epoch 2770, Train Loss: 0.0325, Val Loss: 0.0302\n",
      "Learning rate: 0.000025\n",
      "Epoch 2780, Train Loss: 0.0324, Val Loss: 0.0329\n",
      "Learning rate: 0.000025\n",
      "Epoch 2790, Train Loss: 0.0324, Val Loss: 0.0291\n",
      "Learning rate: 0.000025\n",
      "Epoch 2800, Train Loss: 0.0323, Val Loss: 0.0293\n",
      "Learning rate: 0.000025\n",
      "Epoch 2810, Train Loss: 0.0323, Val Loss: 0.0277\n",
      "Learning rate: 0.000025\n",
      "Epoch 2820, Train Loss: 0.0323, Val Loss: 0.0286\n",
      "Learning rate: 0.000025\n",
      "Epoch 2830, Train Loss: 0.0322, Val Loss: 0.0293\n",
      "Learning rate: 0.000025\n",
      "Epoch 2840, Train Loss: 0.0322, Val Loss: 0.0285\n",
      "Learning rate: 0.000024\n",
      "Epoch 2850, Train Loss: 0.0321, Val Loss: 0.0311\n",
      "Learning rate: 0.000024\n",
      "Epoch 2860, Train Loss: 0.0321, Val Loss: 0.0279\n",
      "Learning rate: 0.000024\n",
      "Epoch 2870, Train Loss: 0.0321, Val Loss: 0.0271\n",
      "Learning rate: 0.000024\n",
      "Epoch 2880, Train Loss: 0.0320, Val Loss: 0.0292\n",
      "Learning rate: 0.000024\n",
      "Epoch 2890, Train Loss: 0.0320, Val Loss: 0.0277\n",
      "Learning rate: 0.000024\n",
      "Epoch 2900, Train Loss: 0.0319, Val Loss: 0.0281\n",
      "Learning rate: 0.000024\n",
      "Epoch 2910, Train Loss: 0.0319, Val Loss: 0.0275\n",
      "Learning rate: 0.000024\n",
      "Epoch 2920, Train Loss: 0.0319, Val Loss: 0.0274\n",
      "Learning rate: 0.000024\n",
      "Epoch 2930, Train Loss: 0.0318, Val Loss: 0.0269\n",
      "Learning rate: 0.000024\n",
      "Epoch 2940, Train Loss: 0.0318, Val Loss: 0.0273\n",
      "Learning rate: 0.000024\n",
      "Epoch 2950, Train Loss: 0.0318, Val Loss: 0.0278\n",
      "Learning rate: 0.000023\n",
      "Epoch 2960, Train Loss: 0.0317, Val Loss: 0.0275\n",
      "Learning rate: 0.000023\n",
      "Epoch 2970, Train Loss: 0.0317, Val Loss: 0.0288\n",
      "Learning rate: 0.000023\n",
      "Epoch 2980, Train Loss: 0.0316, Val Loss: 0.0265\n",
      "Learning rate: 0.000023\n",
      "Epoch 2990, Train Loss: 0.0316, Val Loss: 0.0272\n",
      "Learning rate: 0.000023\n",
      "Epoch 3000, Train Loss: 0.0315, Val Loss: 0.0280\n",
      "Learning rate: 0.000023\n",
      "Epoch 3010, Train Loss: 0.0315, Val Loss: 0.0272\n",
      "Learning rate: 0.000023\n",
      "Epoch 3020, Train Loss: 0.0315, Val Loss: 0.0293\n",
      "Learning rate: 0.000022\n",
      "Epoch 3030, Train Loss: 0.0314, Val Loss: 0.0265\n",
      "Learning rate: 0.000022\n",
      "Epoch 3040, Train Loss: 0.0314, Val Loss: 0.0269\n",
      "Learning rate: 0.000022\n",
      "Epoch 3050, Train Loss: 0.0314, Val Loss: 0.0264\n",
      "Learning rate: 0.000022\n",
      "Epoch 3060, Train Loss: 0.0313, Val Loss: 0.0281\n",
      "Learning rate: 0.000022\n",
      "Epoch 3070, Train Loss: 0.0313, Val Loss: 0.0260\n",
      "Learning rate: 0.000022\n",
      "Epoch 3080, Train Loss: 0.0313, Val Loss: 0.0267\n",
      "Learning rate: 0.000022\n",
      "Epoch 3090, Train Loss: 0.0312, Val Loss: 0.0267\n",
      "Learning rate: 0.000021\n",
      "Epoch 3100, Train Loss: 0.0312, Val Loss: 0.0260\n",
      "Learning rate: 0.000021\n",
      "Epoch 3110, Train Loss: 0.0312, Val Loss: 0.0268\n",
      "Learning rate: 0.000021\n",
      "Epoch 3120, Train Loss: 0.0311, Val Loss: 0.0290\n",
      "Learning rate: 0.000021\n",
      "Epoch 3130, Train Loss: 0.0311, Val Loss: 0.0273\n",
      "Learning rate: 0.000021\n",
      "Epoch 3140, Train Loss: 0.0311, Val Loss: 0.0261\n",
      "Learning rate: 0.000021\n",
      "Epoch 3150, Train Loss: 0.0310, Val Loss: 0.0261\n",
      "Learning rate: 0.000021\n",
      "Epoch 3160, Train Loss: 0.0310, Val Loss: 0.0259\n",
      "Learning rate: 0.000021\n",
      "Epoch 3170, Train Loss: 0.0310, Val Loss: 0.0262\n",
      "Learning rate: 0.000021\n",
      "Epoch 3180, Train Loss: 0.0309, Val Loss: 0.0263\n",
      "Learning rate: 0.000020\n",
      "Epoch 3190, Train Loss: 0.0309, Val Loss: 0.0261\n",
      "Learning rate: 0.000020\n",
      "Epoch 3200, Train Loss: 0.0309, Val Loss: 0.0264\n",
      "Learning rate: 0.000020\n",
      "Epoch 3210, Train Loss: 0.0308, Val Loss: 0.0265\n",
      "Learning rate: 0.000020\n",
      "Epoch 3220, Train Loss: 0.0308, Val Loss: 0.0274\n",
      "Learning rate: 0.000020\n",
      "Epoch 3230, Train Loss: 0.0308, Val Loss: 0.0261\n",
      "Learning rate: 0.000020\n",
      "Epoch 3240, Train Loss: 0.0307, Val Loss: 0.0257\n",
      "Learning rate: 0.000020\n",
      "Epoch 3250, Train Loss: 0.0307, Val Loss: 0.0255\n",
      "Learning rate: 0.000020\n",
      "Epoch 3260, Train Loss: 0.0307, Val Loss: 0.0252\n",
      "Learning rate: 0.000020\n",
      "Epoch 3270, Train Loss: 0.0306, Val Loss: 0.0274\n",
      "Learning rate: 0.000020\n",
      "Epoch 3280, Train Loss: 0.0306, Val Loss: 0.0256\n",
      "Learning rate: 0.000020\n",
      "Epoch 3290, Train Loss: 0.0306, Val Loss: 0.0257\n",
      "Learning rate: 0.000020\n",
      "Epoch 3300, Train Loss: 0.0305, Val Loss: 0.0266\n",
      "Learning rate: 0.000020\n",
      "Epoch 3310, Train Loss: 0.0305, Val Loss: 0.0263\n",
      "Learning rate: 0.000020\n",
      "Epoch 3320, Train Loss: 0.0305, Val Loss: 0.0254\n",
      "Learning rate: 0.000019\n",
      "Epoch 3330, Train Loss: 0.0304, Val Loss: 0.0254\n",
      "Learning rate: 0.000019\n",
      "Epoch 3340, Train Loss: 0.0304, Val Loss: 0.0256\n",
      "Learning rate: 0.000019\n",
      "Epoch 3350, Train Loss: 0.0304, Val Loss: 0.0250\n",
      "Learning rate: 0.000019\n",
      "Epoch 3360, Train Loss: 0.0303, Val Loss: 0.0255\n",
      "Learning rate: 0.000019\n",
      "Epoch 3370, Train Loss: 0.0303, Val Loss: 0.0251\n",
      "Learning rate: 0.000019\n",
      "Epoch 3380, Train Loss: 0.0303, Val Loss: 0.0251\n",
      "Learning rate: 0.000019\n",
      "Epoch 3390, Train Loss: 0.0302, Val Loss: 0.0250\n",
      "Learning rate: 0.000019\n",
      "Epoch 3400, Train Loss: 0.0302, Val Loss: 0.0254\n",
      "Learning rate: 0.000018\n",
      "Epoch 3410, Train Loss: 0.0302, Val Loss: 0.0252\n",
      "Learning rate: 0.000018\n",
      "Epoch 3420, Train Loss: 0.0302, Val Loss: 0.0250\n",
      "Learning rate: 0.000018\n",
      "Epoch 3430, Train Loss: 0.0301, Val Loss: 0.0254\n",
      "Learning rate: 0.000018\n",
      "Epoch 3440, Train Loss: 0.0301, Val Loss: 0.0256\n",
      "Learning rate: 0.000018\n",
      "Epoch 3450, Train Loss: 0.0301, Val Loss: 0.0248\n",
      "Learning rate: 0.000018\n",
      "Epoch 3460, Train Loss: 0.0300, Val Loss: 0.0249\n",
      "Learning rate: 0.000018\n",
      "Epoch 3470, Train Loss: 0.0300, Val Loss: 0.0249\n",
      "Learning rate: 0.000018\n",
      "Epoch 3480, Train Loss: 0.0300, Val Loss: 0.0262\n",
      "Learning rate: 0.000018\n",
      "Epoch 3490, Train Loss: 0.0300, Val Loss: 0.0251\n",
      "Learning rate: 0.000018\n",
      "Epoch 3500, Train Loss: 0.0299, Val Loss: 0.0252\n",
      "Learning rate: 0.000018\n",
      "Epoch 3510, Train Loss: 0.0299, Val Loss: 0.0244\n",
      "Learning rate: 0.000017\n",
      "Epoch 3520, Train Loss: 0.0299, Val Loss: 0.0245\n",
      "Learning rate: 0.000017\n",
      "Epoch 3530, Train Loss: 0.0298, Val Loss: 0.0251\n",
      "Learning rate: 0.000017\n",
      "Epoch 3540, Train Loss: 0.0298, Val Loss: 0.0254\n",
      "Learning rate: 0.000017\n",
      "Epoch 3550, Train Loss: 0.0298, Val Loss: 0.0277\n",
      "Learning rate: 0.000017\n",
      "Epoch 3560, Train Loss: 0.0298, Val Loss: 0.0250\n",
      "Learning rate: 0.000017\n",
      "Epoch 3570, Train Loss: 0.0297, Val Loss: 0.0253\n",
      "Learning rate: 0.000017\n",
      "Epoch 3580, Train Loss: 0.0297, Val Loss: 0.0255\n",
      "Learning rate: 0.000017\n",
      "Epoch 3590, Train Loss: 0.0297, Val Loss: 0.0250\n",
      "Learning rate: 0.000017\n",
      "Epoch 3600, Train Loss: 0.0297, Val Loss: 0.0253\n",
      "Learning rate: 0.000017\n",
      "Epoch 3610, Train Loss: 0.0296, Val Loss: 0.0247\n",
      "Learning rate: 0.000017\n",
      "Epoch 3620, Train Loss: 0.0296, Val Loss: 0.0245\n",
      "Learning rate: 0.000016\n",
      "Epoch 3630, Train Loss: 0.0296, Val Loss: 0.0248\n",
      "Learning rate: 0.000016\n",
      "Epoch 3640, Train Loss: 0.0296, Val Loss: 0.0244\n",
      "Learning rate: 0.000016\n",
      "Epoch 3650, Train Loss: 0.0295, Val Loss: 0.0249\n",
      "Learning rate: 0.000016\n",
      "Epoch 3660, Train Loss: 0.0295, Val Loss: 0.0245\n",
      "Learning rate: 0.000016\n",
      "Epoch 3670, Train Loss: 0.0295, Val Loss: 0.0248\n",
      "Learning rate: 0.000016\n",
      "Epoch 3680, Train Loss: 0.0294, Val Loss: 0.0249\n",
      "Learning rate: 0.000016\n",
      "Epoch 3690, Train Loss: 0.0294, Val Loss: 0.0244\n",
      "Learning rate: 0.000016\n",
      "Epoch 3700, Train Loss: 0.0294, Val Loss: 0.0243\n",
      "Learning rate: 0.000016\n",
      "Epoch 3710, Train Loss: 0.0294, Val Loss: 0.0234\n",
      "Learning rate: 0.000016\n",
      "Epoch 3720, Train Loss: 0.0293, Val Loss: 0.0251\n",
      "Learning rate: 0.000016\n",
      "Epoch 3730, Train Loss: 0.0293, Val Loss: 0.0242\n",
      "Learning rate: 0.000016\n",
      "Epoch 3740, Train Loss: 0.0293, Val Loss: 0.0245\n",
      "Learning rate: 0.000016\n",
      "Epoch 3750, Train Loss: 0.0293, Val Loss: 0.0246\n",
      "Learning rate: 0.000015\n",
      "Epoch 3760, Train Loss: 0.0292, Val Loss: 0.0235\n",
      "Learning rate: 0.000015\n",
      "Epoch 3770, Train Loss: 0.0292, Val Loss: 0.0250\n",
      "Learning rate: 0.000015\n",
      "Epoch 3780, Train Loss: 0.0292, Val Loss: 0.0240\n",
      "Learning rate: 0.000015\n",
      "Epoch 3790, Train Loss: 0.0292, Val Loss: 0.0237\n",
      "Learning rate: 0.000015\n",
      "Epoch 3800, Train Loss: 0.0291, Val Loss: 0.0240\n",
      "Learning rate: 0.000015\n",
      "Epoch 3810, Train Loss: 0.0291, Val Loss: 0.0249\n",
      "Learning rate: 0.000015\n",
      "Epoch 3820, Train Loss: 0.0291, Val Loss: 0.0240\n",
      "Learning rate: 0.000015\n",
      "Epoch 3830, Train Loss: 0.0291, Val Loss: 0.0239\n",
      "Learning rate: 0.000015\n",
      "Epoch 3840, Train Loss: 0.0290, Val Loss: 0.0235\n",
      "Learning rate: 0.000015\n",
      "Epoch 3850, Train Loss: 0.0290, Val Loss: 0.0241\n",
      "Learning rate: 0.000015\n",
      "Epoch 3860, Train Loss: 0.0290, Val Loss: 0.0238\n",
      "Learning rate: 0.000015\n",
      "Epoch 3870, Train Loss: 0.0289, Val Loss: 0.0235\n",
      "Learning rate: 0.000014\n",
      "Epoch 3880, Train Loss: 0.0289, Val Loss: 0.0234\n",
      "Learning rate: 0.000014\n",
      "Epoch 3890, Train Loss: 0.0289, Val Loss: 0.0232\n",
      "Learning rate: 0.000014\n",
      "Epoch 3900, Train Loss: 0.0289, Val Loss: 0.0237\n",
      "Learning rate: 0.000014\n",
      "Epoch 3910, Train Loss: 0.0289, Val Loss: 0.0240\n",
      "Learning rate: 0.000014\n",
      "Epoch 3920, Train Loss: 0.0288, Val Loss: 0.0234\n",
      "Learning rate: 0.000014\n",
      "Epoch 3930, Train Loss: 0.0288, Val Loss: 0.0237\n",
      "Learning rate: 0.000014\n",
      "Epoch 3940, Train Loss: 0.0288, Val Loss: 0.0235\n",
      "Learning rate: 0.000014\n",
      "Epoch 3950, Train Loss: 0.0288, Val Loss: 0.0234\n",
      "Learning rate: 0.000014\n",
      "Epoch 3960, Train Loss: 0.0287, Val Loss: 0.0239\n",
      "Learning rate: 0.000014\n",
      "Epoch 3970, Train Loss: 0.0287, Val Loss: 0.0241\n",
      "Learning rate: 0.000014\n",
      "Epoch 3980, Train Loss: 0.0287, Val Loss: 0.0235\n",
      "Learning rate: 0.000014\n",
      "Epoch 3990, Train Loss: 0.0287, Val Loss: 0.0232\n",
      "Learning rate: 0.000013\n",
      "Epoch 4000, Train Loss: 0.0287, Val Loss: 0.0234\n",
      "Learning rate: 0.000013\n",
      "Epoch 4010, Train Loss: 0.0286, Val Loss: 0.0231\n",
      "Learning rate: 0.000013\n",
      "Epoch 4020, Train Loss: 0.0286, Val Loss: 0.0235\n",
      "Learning rate: 0.000013\n",
      "Epoch 4030, Train Loss: 0.0286, Val Loss: 0.0232\n",
      "Learning rate: 0.000013\n",
      "Epoch 4040, Train Loss: 0.0286, Val Loss: 0.0228\n",
      "Learning rate: 0.000013\n",
      "Epoch 4050, Train Loss: 0.0286, Val Loss: 0.0231\n",
      "Learning rate: 0.000013\n",
      "Epoch 4060, Train Loss: 0.0285, Val Loss: 0.0231\n",
      "Learning rate: 0.000013\n",
      "Epoch 4070, Train Loss: 0.0285, Val Loss: 0.0231\n",
      "Learning rate: 0.000013\n",
      "Epoch 4080, Train Loss: 0.0285, Val Loss: 0.0230\n",
      "Learning rate: 0.000013\n",
      "Epoch 4090, Train Loss: 0.0285, Val Loss: 0.0229\n",
      "Learning rate: 0.000013\n",
      "Epoch 4100, Train Loss: 0.0285, Val Loss: 0.0232\n",
      "Learning rate: 0.000012\n",
      "Epoch 4110, Train Loss: 0.0284, Val Loss: 0.0231\n",
      "Learning rate: 0.000012\n",
      "Epoch 4120, Train Loss: 0.0284, Val Loss: 0.0233\n",
      "Learning rate: 0.000012\n",
      "Epoch 4130, Train Loss: 0.0284, Val Loss: 0.0233\n",
      "Learning rate: 0.000012\n",
      "Epoch 4140, Train Loss: 0.0284, Val Loss: 0.0229\n",
      "Learning rate: 0.000012\n",
      "Epoch 4150, Train Loss: 0.0283, Val Loss: 0.0234\n",
      "Learning rate: 0.000012\n",
      "Epoch 4160, Train Loss: 0.0283, Val Loss: 0.0234\n",
      "Learning rate: 0.000012\n",
      "Epoch 4170, Train Loss: 0.0283, Val Loss: 0.0230\n",
      "Learning rate: 0.000012\n",
      "Epoch 4180, Train Loss: 0.0283, Val Loss: 0.0222\n",
      "Learning rate: 0.000012\n",
      "Epoch 4190, Train Loss: 0.0283, Val Loss: 0.0226\n",
      "Learning rate: 0.000012\n",
      "Epoch 4200, Train Loss: 0.0282, Val Loss: 0.0228\n",
      "Learning rate: 0.000012\n",
      "Epoch 4210, Train Loss: 0.0282, Val Loss: 0.0229\n",
      "Learning rate: 0.000012\n",
      "Epoch 4220, Train Loss: 0.0282, Val Loss: 0.0226\n",
      "Learning rate: 0.000012\n",
      "Epoch 4230, Train Loss: 0.0282, Val Loss: 0.0226\n",
      "Learning rate: 0.000012\n",
      "Epoch 4240, Train Loss: 0.0282, Val Loss: 0.0222\n",
      "Learning rate: 0.000012\n",
      "Epoch 4250, Train Loss: 0.0281, Val Loss: 0.0224\n",
      "Learning rate: 0.000012\n",
      "Epoch 4260, Train Loss: 0.0281, Val Loss: 0.0222\n",
      "Learning rate: 0.000012\n",
      "Epoch 4270, Train Loss: 0.0281, Val Loss: 0.0225\n",
      "Learning rate: 0.000012\n",
      "Epoch 4280, Train Loss: 0.0281, Val Loss: 0.0225\n",
      "Learning rate: 0.000011\n",
      "Epoch 4290, Train Loss: 0.0281, Val Loss: 0.0218\n",
      "Learning rate: 0.000011\n",
      "Epoch 4300, Train Loss: 0.0281, Val Loss: 0.0223\n",
      "Learning rate: 0.000011\n",
      "Epoch 4310, Train Loss: 0.0280, Val Loss: 0.0227\n",
      "Learning rate: 0.000011\n",
      "Epoch 4320, Train Loss: 0.0280, Val Loss: 0.0232\n",
      "Learning rate: 0.000011\n",
      "Epoch 4330, Train Loss: 0.0280, Val Loss: 0.0222\n",
      "Learning rate: 0.000011\n",
      "Epoch 4340, Train Loss: 0.0280, Val Loss: 0.0222\n",
      "Learning rate: 0.000011\n",
      "Epoch 4350, Train Loss: 0.0280, Val Loss: 0.0223\n",
      "Learning rate: 0.000011\n",
      "Epoch 4360, Train Loss: 0.0279, Val Loss: 0.0223\n",
      "Learning rate: 0.000011\n",
      "Epoch 4370, Train Loss: 0.0279, Val Loss: 0.0219\n",
      "Learning rate: 0.000011\n",
      "Epoch 4380, Train Loss: 0.0279, Val Loss: 0.0224\n",
      "Learning rate: 0.000011\n",
      "Epoch 4390, Train Loss: 0.0279, Val Loss: 0.0227\n",
      "Learning rate: 0.000011\n",
      "Epoch 4400, Train Loss: 0.0278, Val Loss: 0.0220\n",
      "Learning rate: 0.000011\n",
      "Epoch 4410, Train Loss: 0.0279, Val Loss: 0.0222\n",
      "Learning rate: 0.000011\n",
      "Epoch 4420, Train Loss: 0.0278, Val Loss: 0.0221\n",
      "Learning rate: 0.000010\n",
      "Epoch 4430, Train Loss: 0.0278, Val Loss: 0.0219\n",
      "Learning rate: 0.000010\n",
      "Epoch 4440, Train Loss: 0.0278, Val Loss: 0.0226\n",
      "Learning rate: 0.000010\n",
      "Epoch 4450, Train Loss: 0.0278, Val Loss: 0.0222\n",
      "Learning rate: 0.000010\n",
      "Epoch 4460, Train Loss: 0.0277, Val Loss: 0.0220\n",
      "Learning rate: 0.000010\n",
      "Epoch 4470, Train Loss: 0.0277, Val Loss: 0.0220\n",
      "Learning rate: 0.000010\n",
      "Epoch 4480, Train Loss: 0.0277, Val Loss: 0.0225\n",
      "Learning rate: 0.000010\n",
      "Epoch 4490, Train Loss: 0.0277, Val Loss: 0.0222\n",
      "Learning rate: 0.000010\n",
      "Epoch 4500, Train Loss: 0.0277, Val Loss: 0.0218\n",
      "Learning rate: 0.000010\n",
      "Epoch 4510, Train Loss: 0.0277, Val Loss: 0.0221\n",
      "Learning rate: 0.000010\n",
      "Epoch 4520, Train Loss: 0.0276, Val Loss: 0.0218\n",
      "Learning rate: 0.000010\n",
      "Epoch 4530, Train Loss: 0.0276, Val Loss: 0.0222\n",
      "Learning rate: 0.000010\n",
      "Epoch 4540, Train Loss: 0.0276, Val Loss: 0.0220\n",
      "Learning rate: 0.000010\n",
      "Epoch 4550, Train Loss: 0.0276, Val Loss: 0.0220\n",
      "Learning rate: 0.000010\n",
      "Epoch 4560, Train Loss: 0.0276, Val Loss: 0.0215\n",
      "Learning rate: 0.000010\n",
      "Epoch 4570, Train Loss: 0.0276, Val Loss: 0.0221\n",
      "Learning rate: 0.000010\n",
      "Epoch 4580, Train Loss: 0.0275, Val Loss: 0.0217\n",
      "Learning rate: 0.000010\n",
      "Epoch 4590, Train Loss: 0.0275, Val Loss: 0.0218\n",
      "Learning rate: 0.000010\n",
      "Epoch 4600, Train Loss: 0.0275, Val Loss: 0.0216\n",
      "Learning rate: 0.000010\n",
      "Epoch 4610, Train Loss: 0.0275, Val Loss: 0.0218\n",
      "Learning rate: 0.000010\n",
      "Epoch 4620, Train Loss: 0.0275, Val Loss: 0.0217\n",
      "Learning rate: 0.000009\n",
      "Epoch 4630, Train Loss: 0.0275, Val Loss: 0.0215\n",
      "Learning rate: 0.000009\n",
      "Epoch 4640, Train Loss: 0.0274, Val Loss: 0.0215\n",
      "Learning rate: 0.000009\n",
      "Epoch 4650, Train Loss: 0.0274, Val Loss: 0.0215\n",
      "Learning rate: 0.000009\n",
      "Epoch 4660, Train Loss: 0.0274, Val Loss: 0.0218\n",
      "Learning rate: 0.000009\n",
      "Epoch 4670, Train Loss: 0.0274, Val Loss: 0.0212\n",
      "Learning rate: 0.000009\n",
      "Epoch 4680, Train Loss: 0.0274, Val Loss: 0.0216\n",
      "Learning rate: 0.000009\n",
      "Epoch 4690, Train Loss: 0.0274, Val Loss: 0.0217\n",
      "Learning rate: 0.000009\n",
      "Epoch 4700, Train Loss: 0.0273, Val Loss: 0.0215\n",
      "Learning rate: 0.000009\n",
      "Epoch 4710, Train Loss: 0.0273, Val Loss: 0.0216\n",
      "Learning rate: 0.000009\n",
      "Epoch 4720, Train Loss: 0.0273, Val Loss: 0.0216\n",
      "Learning rate: 0.000009\n",
      "Epoch 4730, Train Loss: 0.0273, Val Loss: 0.0216\n",
      "Learning rate: 0.000009\n",
      "Epoch 4740, Train Loss: 0.0273, Val Loss: 0.0216\n",
      "Learning rate: 0.000009\n",
      "Epoch 4750, Train Loss: 0.0273, Val Loss: 0.0221\n",
      "Learning rate: 0.000009\n",
      "Epoch 4760, Train Loss: 0.0273, Val Loss: 0.0215\n",
      "Learning rate: 0.000009\n",
      "Epoch 4770, Train Loss: 0.0272, Val Loss: 0.0216\n",
      "Learning rate: 0.000009\n",
      "Epoch 4780, Train Loss: 0.0272, Val Loss: 0.0209\n",
      "Learning rate: 0.000009\n",
      "Epoch 4790, Train Loss: 0.0272, Val Loss: 0.0213\n",
      "Learning rate: 0.000009\n",
      "Epoch 4800, Train Loss: 0.0272, Val Loss: 0.0215\n",
      "Learning rate: 0.000009\n",
      "Epoch 4810, Train Loss: 0.0272, Val Loss: 0.0215\n",
      "Learning rate: 0.000009\n",
      "Epoch 4820, Train Loss: 0.0272, Val Loss: 0.0212\n",
      "Learning rate: 0.000009\n",
      "Epoch 4830, Train Loss: 0.0272, Val Loss: 0.0213\n",
      "Learning rate: 0.000009\n",
      "Epoch 4840, Train Loss: 0.0271, Val Loss: 0.0212\n",
      "Learning rate: 0.000009\n",
      "Epoch 4850, Train Loss: 0.0271, Val Loss: 0.0214\n",
      "Learning rate: 0.000008\n",
      "Epoch 4860, Train Loss: 0.0271, Val Loss: 0.0212\n",
      "Learning rate: 0.000008\n",
      "Epoch 4870, Train Loss: 0.0271, Val Loss: 0.0210\n",
      "Learning rate: 0.000008\n",
      "Epoch 4880, Train Loss: 0.0271, Val Loss: 0.0210\n",
      "Learning rate: 0.000008\n",
      "Epoch 4890, Train Loss: 0.0271, Val Loss: 0.0208\n",
      "Learning rate: 0.000008\n",
      "Epoch 4900, Train Loss: 0.0270, Val Loss: 0.0213\n",
      "Learning rate: 0.000008\n",
      "Epoch 4910, Train Loss: 0.0270, Val Loss: 0.0211\n",
      "Learning rate: 0.000008\n",
      "Epoch 4920, Train Loss: 0.0270, Val Loss: 0.0213\n",
      "Learning rate: 0.000008\n",
      "Epoch 4930, Train Loss: 0.0270, Val Loss: 0.0213\n",
      "Learning rate: 0.000008\n",
      "Epoch 4940, Train Loss: 0.0270, Val Loss: 0.0210\n",
      "Learning rate: 0.000008\n",
      "Epoch 4950, Train Loss: 0.0270, Val Loss: 0.0209\n",
      "Learning rate: 0.000008\n",
      "Epoch 4960, Train Loss: 0.0270, Val Loss: 0.0212\n",
      "Learning rate: 0.000008\n",
      "Epoch 4970, Train Loss: 0.0269, Val Loss: 0.0215\n",
      "Learning rate: 0.000008\n",
      "Epoch 4980, Train Loss: 0.0269, Val Loss: 0.0209\n",
      "Learning rate: 0.000008\n",
      "Epoch 4990, Train Loss: 0.0269, Val Loss: 0.0210\n",
      "Learning rate: 0.000008\n",
      "Epoch 5000, Train Loss: 0.0269, Val Loss: 0.0211\n",
      "Learning rate: 0.000008\n",
      "Epoch 5010, Train Loss: 0.0269, Val Loss: 0.0210\n",
      "Learning rate: 0.000008\n",
      "Epoch 5020, Train Loss: 0.0269, Val Loss: 0.0211\n",
      "Learning rate: 0.000008\n",
      "Epoch 5030, Train Loss: 0.0269, Val Loss: 0.0209\n",
      "Learning rate: 0.000008\n",
      "Epoch 5040, Train Loss: 0.0268, Val Loss: 0.0212\n",
      "Learning rate: 0.000008\n",
      "Epoch 5050, Train Loss: 0.0268, Val Loss: 0.0207\n",
      "Learning rate: 0.000008\n",
      "Epoch 5060, Train Loss: 0.0268, Val Loss: 0.0206\n",
      "Learning rate: 0.000007\n",
      "Epoch 5070, Train Loss: 0.0268, Val Loss: 0.0205\n",
      "Learning rate: 0.000007\n",
      "Epoch 5080, Train Loss: 0.0268, Val Loss: 0.0209\n",
      "Learning rate: 0.000007\n",
      "Epoch 5090, Train Loss: 0.0268, Val Loss: 0.0210\n",
      "Learning rate: 0.000007\n",
      "Epoch 5100, Train Loss: 0.0268, Val Loss: 0.0211\n",
      "Learning rate: 0.000007\n",
      "Epoch 5110, Train Loss: 0.0267, Val Loss: 0.0208\n",
      "Learning rate: 0.000007\n",
      "Epoch 5120, Train Loss: 0.0267, Val Loss: 0.0208\n",
      "Learning rate: 0.000007\n",
      "Epoch 5130, Train Loss: 0.0267, Val Loss: 0.0208\n",
      "Learning rate: 0.000007\n",
      "Epoch 5140, Train Loss: 0.0267, Val Loss: 0.0207\n",
      "Learning rate: 0.000007\n",
      "Epoch 5150, Train Loss: 0.0267, Val Loss: 0.0209\n",
      "Learning rate: 0.000007\n",
      "Epoch 5160, Train Loss: 0.0267, Val Loss: 0.0206\n",
      "Learning rate: 0.000007\n",
      "Epoch 5170, Train Loss: 0.0267, Val Loss: 0.0206\n",
      "Learning rate: 0.000007\n",
      "Epoch 5180, Train Loss: 0.0267, Val Loss: 0.0205\n",
      "Learning rate: 0.000007\n",
      "Epoch 5190, Train Loss: 0.0266, Val Loss: 0.0206\n",
      "Learning rate: 0.000007\n",
      "Epoch 5200, Train Loss: 0.0266, Val Loss: 0.0206\n",
      "Learning rate: 0.000007\n",
      "Epoch 5210, Train Loss: 0.0266, Val Loss: 0.0206\n",
      "Learning rate: 0.000007\n",
      "Epoch 5220, Train Loss: 0.0266, Val Loss: 0.0207\n",
      "Learning rate: 0.000007\n",
      "Epoch 5230, Train Loss: 0.0266, Val Loss: 0.0205\n",
      "Learning rate: 0.000007\n",
      "Epoch 5240, Train Loss: 0.0266, Val Loss: 0.0206\n",
      "Learning rate: 0.000007\n",
      "Epoch 5250, Train Loss: 0.0266, Val Loss: 0.0207\n",
      "Learning rate: 0.000007\n",
      "Epoch 5260, Train Loss: 0.0265, Val Loss: 0.0207\n",
      "Learning rate: 0.000007\n",
      "Epoch 5270, Train Loss: 0.0265, Val Loss: 0.0208\n",
      "Learning rate: 0.000007\n",
      "Epoch 5280, Train Loss: 0.0265, Val Loss: 0.0202\n",
      "Learning rate: 0.000007\n",
      "Epoch 5290, Train Loss: 0.0265, Val Loss: 0.0205\n",
      "Learning rate: 0.000007\n",
      "Epoch 5300, Train Loss: 0.0265, Val Loss: 0.0207\n",
      "Learning rate: 0.000007\n",
      "Epoch 5310, Train Loss: 0.0265, Val Loss: 0.0206\n",
      "Learning rate: 0.000007\n",
      "Epoch 5320, Train Loss: 0.0265, Val Loss: 0.0207\n",
      "Learning rate: 0.000007\n",
      "Epoch 5330, Train Loss: 0.0265, Val Loss: 0.0202\n",
      "Learning rate: 0.000006\n",
      "Epoch 5340, Train Loss: 0.0264, Val Loss: 0.0207\n",
      "Learning rate: 0.000006\n",
      "Epoch 5350, Train Loss: 0.0264, Val Loss: 0.0204\n",
      "Learning rate: 0.000006\n",
      "Epoch 5360, Train Loss: 0.0264, Val Loss: 0.0203\n",
      "Learning rate: 0.000006\n",
      "Epoch 5370, Train Loss: 0.0264, Val Loss: 0.0203\n",
      "Learning rate: 0.000006\n",
      "Epoch 5380, Train Loss: 0.0264, Val Loss: 0.0205\n",
      "Learning rate: 0.000006\n",
      "Epoch 5390, Train Loss: 0.0264, Val Loss: 0.0206\n",
      "Learning rate: 0.000006\n",
      "Epoch 5400, Train Loss: 0.0264, Val Loss: 0.0204\n",
      "Learning rate: 0.000006\n",
      "Epoch 5410, Train Loss: 0.0264, Val Loss: 0.0204\n",
      "Learning rate: 0.000006\n",
      "Epoch 5420, Train Loss: 0.0264, Val Loss: 0.0201\n",
      "Learning rate: 0.000006\n",
      "Epoch 5430, Train Loss: 0.0263, Val Loss: 0.0201\n",
      "Learning rate: 0.000006\n",
      "Epoch 5440, Train Loss: 0.0263, Val Loss: 0.0204\n",
      "Learning rate: 0.000006\n",
      "Epoch 5450, Train Loss: 0.0263, Val Loss: 0.0203\n",
      "Learning rate: 0.000006\n",
      "Epoch 5460, Train Loss: 0.0263, Val Loss: 0.0203\n",
      "Learning rate: 0.000006\n",
      "Epoch 5470, Train Loss: 0.0263, Val Loss: 0.0203\n",
      "Learning rate: 0.000006\n",
      "Epoch 5480, Train Loss: 0.0263, Val Loss: 0.0201\n",
      "Learning rate: 0.000006\n",
      "Epoch 5490, Train Loss: 0.0263, Val Loss: 0.0201\n",
      "Learning rate: 0.000006\n",
      "Epoch 5500, Train Loss: 0.0263, Val Loss: 0.0204\n",
      "Learning rate: 0.000006\n",
      "Epoch 5510, Train Loss: 0.0263, Val Loss: 0.0205\n",
      "Learning rate: 0.000006\n",
      "Training interrupted by user.\n",
      "Time taken: 9097.513398885727\n",
      "Best epoch: 5504, Best validation loss: 0.0199\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAjcAAAGwCAYAAABVdURTAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuNSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/xnp5ZAAAACXBIWXMAAA9hAAAPYQGoP6dpAABQPUlEQVR4nO3deXgUReI+8Ld7JjO5D8gJBMJ9XyYQAx64RAO6CIgriywgIiwKrIi4iMghrsSTRYEFL2B11SD+hOW73ERQBCQIgiCRQyEJkAMIuY/JzNTvj85MMiSEJHSmc7yf55lnMt3V3TVtNK9V1VWSEEKAiIiIqJGQta4AERERkZoYboiIiKhRYbghIiKiRoXhhoiIiBoVhhsiIiJqVBhuiIiIqFFhuCEiIqJGRa91BZzNarXi8uXL8PLygiRJWleHiIiIqkEIgdzcXLRo0QKyXHXbTJMLN5cvX0ZoaKjW1SAiIqJaSElJQatWraos0+TCjZeXFwDl5nh7e2tcGyIiIqqOnJwchIaG2v+OV6XJhRtbV5S3tzfDDRERUQNTnSElHFBMREREjQrDDRERETUqDDdERETUqDS5MTdERHT7LBYLSkpKtK4GNTIGg+GWj3lXB8MNERFVmxACaWlpyMrK0roq1AjJsoy2bdvCYDDc1nkYboiIqNpswSYwMBDu7u6cDJVUY5tkNzU1Fa1bt76t3y2GGyIiqhaLxWIPNs2bN9e6OtQIBQQE4PLlyzCbzXBxcan1eTigmIiIqsU2xsbd3V3jmlBjZeuOslgst3UehhsiIqoRdkVRXVHrd4vhhoiIiBoVhhsiIiJqVBhuiIiIaigsLAzLli2rdvm9e/dCkiQ+Qu8kDDcqKTZbcPF6AVKzC7WuChERlZIkqcrXokWLanXew4cPY8qUKdUuP2DAAKSmpsLHx6dW16suhigFHwVXyclLORi16gBaN3PHd3+/T+vqEBERgNTUVPvP69evx4IFC3D69Gn7Nk9PT/vPQghYLBbo9bf+0xgQEFCjehgMBgQHB9foGKo9ttyoxDbAW0BoWxEiIicRQqDAZNbkJUT1/lsbHBxsf/n4+ECSJPvnX3/9FV5eXti2bRvCw8NhNBrx/fff47fffsPw4cMRFBQET09P9OvXD7t373Y4743dUpIk4aOPPsLIkSPh7u6Ojh07YvPmzfb9N7aorFu3Dr6+vtixYwe6du0KT09PDBkyxCGMmc1m/O1vf4Ovry+aN2+OOXPmYMKECRgxYkSt/5ldv34d48ePh5+fH9zd3TF06FCcPXvWvj8pKQnDhg2Dn58fPDw80L17d2zdutV+7NixYxEQEAA3Nzd07NgRa9eurXVd6hJbblRie3itmv++ERE1eIUlFnRbsEOTa59aHAN3gzp/wl588UW8/fbbaNeuHfz8/JCSkoIHH3wQr732GoxGIz755BMMGzYMp0+fRuvWrW96nldeeQVvvvkm3nrrLSxfvhxjx45FUlISmjVrVmn5goICvP322/j0008hyzL+8pe/YPbs2fjss88AAG+88QY+++wzrF27Fl27dsW7776LTZs24b77at878MQTT+Ds2bPYvHkzvL29MWfOHDz44IM4deoUXFxcMG3aNJhMJnz33Xfw8PDAqVOn7K1b8+fPx6lTp7Bt2zb4+/vj3LlzKCysn0MxGG5UYns2n+GGiKhhWbx4Me6//37752bNmqF37972z6+++io2btyIzZs3Y/r06Tc9zxNPPIExY8YAAJYsWYL33nsPCQkJGDJkSKXlS0pKsHr1arRv3x4AMH36dCxevNi+f/ny5Zg7dy5GjhwJAFixYoW9FaU2bKFm//79GDBgAADgs88+Q2hoKDZt2oQ//elPSE5OxqhRo9CzZ08AQLt27ezHJycno2/fvoiIiACgtF7VVww3KuGUVkTU1Li56HBqcYxm11aL7Y+1TV5eHhYtWoQtW7YgNTUVZrMZhYWFSE5OrvI8vXr1sv/s4eEBb29vZGRk3LS8u7u7PdgAQEhIiL18dnY20tPT0b9/f/t+nU6H8PBwWK3WGn0/m8TEROj1ekRGRtq3NW/eHJ07d0ZiYiIA4G9/+xuefvpp7Ny5E9HR0Rg1apT9ez399NMYNWoUjh49igceeAAjRoywh6T6hmNuVCLbW27YdENETYMkSXA36DV5qTlLsoeHh8Pn2bNnY+PGjViyZAn27duHY8eOoWfPnjCZTFWe58a1kCRJqjKIVFZe678hTz31FH7//XeMGzcOJ06cQEREBJYvXw4AGDp0KJKSkvDcc8/h8uXLGDx4MGbPnq1pfW+G4UYltn/PrMw2REQN2v79+/HEE09g5MiR6NmzJ4KDg3HhwgWn1sHHxwdBQUE4fPiwfZvFYsHRo0drfc6uXbvCbDbj0KFD9m3Xrl3D6dOn0a1bN/u20NBQTJ06FV9//TWef/55fPjhh/Z9AQEBmDBhAv7zn/9g2bJl+OCDD2pdn7rEbimV8WkpIqKGrWPHjvj6668xbNgwSJKE+fPn17or6HbMmDEDsbGx6NChA7p06YLly5fj+vXr1Wq1OnHiBLy8vOyfJUlC7969MXz4cEyePBnvv/8+vLy88OKLL6Jly5YYPnw4AGDmzJkYOnQoOnXqhOvXr2PPnj3o2rUrAGDBggUIDw9H9+7dUVxcjP/973/2ffUNw41K7I+CM9sQETVoS5cuxZNPPokBAwbA398fc+bMQU5OjtPrMWfOHKSlpWH8+PHQ6XSYMmUKYmJioNPderzRPffc4/BZp9PBbDZj7dq1ePbZZ/HHP/4RJpMJ99xzD7Zu3WrvIrNYLJg2bRouXrwIb29vDBkyBP/85z8BKHP1zJ07FxcuXICbmxvuvvtuxMXFqf/FVSAJrTv4nCwnJwc+Pj7Izs6Gt7e3auc9dTkHD763DwFeRhyeF63aeYmI6ouioiKcP38ebdu2haurq9bVaXKsViu6du2Kxx57DK+++qrW1akTVf2O1eTvN1tuVMKWGyIiUlNSUhJ27tyJe++9F8XFxVixYgXOnz+Pxx9/XOuq1XscUKwS2d4HynRDRES3T5ZlrFu3Dv369cPAgQNx4sQJ7N69u96Oc6lP2HKjEj4tRUREagoNDcX+/fu1rkaDxJYblZQtv8B0Q0REpCXNw83KlSsRFhYGV1dXREZGIiEhocryy5YtQ+fOneHm5obQ0FA899xzKCoqclJtb65s4UwiIiLSkqbhZv369Zg1axYWLlyIo0ePonfv3oiJibnpdNWff/45XnzxRSxcuBCJiYn4+OOPsX79erz00ktOrnlluLYUERFRfaBpuFm6dCkmT56MiRMnolu3bli9ejXc3d2xZs2aSssfOHAAAwcOxOOPP46wsDA88MADGDNmTJWtPcXFxcjJyXF41YWyp6WYboiIiLSkWbgxmUw4cuQIoqPL5oSRZRnR0dE4ePBgpccMGDAAR44csYeZ33//HVu3bsWDDz540+vExsbCx8fH/goNDVX3i9jqbltbqk7OTkRERNWlWbi5evUqLBYLgoKCHLYHBQUhLS2t0mMef/xxLF68GHfddRdcXFzQvn17DBo0qMpuqblz5yI7O9v+SklJUfV72JQNKK6T0xMRkYYGDRqEmTNn2j+HhYVh2bJlVR4jSRI2bdp029dW6zxNieYDimti7969WLJkCf71r3/h6NGj+Prrr7Fly5YqZ2o0Go3w9vZ2eNUFdksREdU/w4YNw5AhQyrdt2/fPkiShJ9//rnG5z18+DCmTJlyu9VzsGjRIvTp06fC9tTUVAwdOlTVa91o3bp18PX1rdNrOJNm89z4+/tDp9MhPT3dYXt6ejqCg4MrPWb+/PkYN24cnnrqKQBAz549kZ+fjylTpmDevHmQZe2ymgR2SxER1TeTJk3CqFGjcPHiRbRq1cph39q1axEREYFevXrV+LwBAQFqVfGWbvY3kW5OszRgMBgQHh6O+Ph4+zar1Yr4+HhERUVVekxBQUGFAGNbQEzrFhMuv0BEVP/88Y9/REBAANatW+ewPS8vDxs2bMCkSZNw7do1jBkzBi1btoS7uzt69uyJL774osrz3tgtdfbsWdxzzz1wdXVFt27dsGvXrgrHzJkzB506dYK7uzvatWuH+fPno6SkBIDScvLKK6/g+PHjkCQJkiTZ63xjt9SJEyfwhz/8AW5ubmjevDmmTJmCvLw8+/4nnngCI0aMwNtvv42QkBA0b94c06ZNs1+rNpKTkzF8+HB4enrC29sbjz32mEPjxPHjx3HffffBy8sL3t7eCA8Px48//ghAWUZi2LBh8PPzg4eHB7p3746tW7fWui7VoekMxbNmzcKECRMQERGB/v37Y9myZcjPz8fEiRMBAOPHj0fLli0RGxsLQGleXLp0Kfr27YvIyEicO3cO8+fPx7Bhw6q1SqozCLbdEFFTIQRQUqDNtV3cy/6vsgp6vR7jx4/HunXrMG/ePEilx2zYsAEWiwVjxoxBXl4ewsPDMWfOHHh7e2PLli0YN24c2rdvj/79+9/yGlarFY888giCgoJw6NAhZGdnO4zPsfHy8sK6devQokULnDhxApMnT4aXlxf+/ve/Y/To0Th58iS2b9+O3bt3AwB8fHwqnCM/Px8xMTGIiorC4cOHkZGRgaeeegrTp093CHB79uxBSEgI9uzZg3PnzmH06NHo06cPJk+efMvvU9n3swWbb7/9FmazGdOmTcPo0aOxd+9eAMDYsWPRt29frFq1CjqdDseOHbOvND5t2jSYTCZ899138PDwwKlTp+Dp6VnjetSEpuFm9OjRuHLlChYsWIC0tDT06dMH27dvtw8yTk5OdmipefnllyFJEl5++WVcunQJAQEBGDZsGF577TWtvoKdLHOeGyJqYkoKgCUttLn2S5cBg0e1ij755JN466238O2332LQoEEAlC6pUaNG2Z+knT17tr38jBkzsGPHDnz55ZfVCje7d+/Gr7/+ih07dqBFC+V+LFmypMI4mZdfftn+c1hYGGbPno24uDj8/e9/h5ubGzw9PaHX66vshvr8889RVFSETz75BB4eyvdfsWIFhg0bhjfeeMP+99PPzw8rVqyATqdDly5d8NBDDyE+Pr5W4SY+Ph4nTpzA+fPn7U8cf/LJJ+jevTsOHz6Mfv36ITk5GS+88AK6dOkCAOjYsaP9+OTkZIwaNQo9e/YEALRr167GdagpzdeWmj59OqZPn17pPlsitNHr9Vi4cCEWLlzohJrVDJ+WIiKqn7p06YIBAwZgzZo1GDRoEM6dO4d9+/Zh8eLFAACLxYIlS5bgyy+/xKVLl2AymVBcXAx3d/dqnT8xMRGhoaH2YAOg0uEV69evx3vvvYfffvsNeXl5MJvNNX7IJTExEb1797YHGwAYOHAgrFYrTp8+bQ833bt3d+jRCAkJwYkTJ2p0rfLXDA0NdZhKpVu3bvD19UViYiL69euHWbNm4amnnsKnn36K6Oho/OlPf0L79u0BAH/729/w9NNPY+fOnYiOjsaoUaNqNc6pJjQPN41F2fILTDdE1ES4uCstKFpduwYmTZqEGTNmYOXKlVi7di3at2+Pe++9FwDw1ltv4d1338WyZcvQs2dPeHh4YObMmTCZTKpV9+DBgxg7dixeeeUVxMTEwMfHB3FxcXjnnXdUu0Z5ti4hG0mSYLVa6+RagPKk1+OPP44tW7Zg27ZtWLhwIeLi4jBy5Eg89dRTiImJwZYtW7Bz507ExsbinXfewYwZM+qsPg3qUfD6TOLyC0TU1EiS0jWkxasa423Ke+yxxyDLMj7//HN88sknePLJJ+3jb/bv34/hw4fjL3/5C3r37o127drhzJkz1T53165dkZKSgtTUVPu2H374waHMgQMH0KZNG8ybNw8RERHo2LEjkpKSHMoYDAZYLJZbXuv48ePIz8+3b9u/fz9kWUbnzp2rXeeasH2/8vPEnTp1CllZWejWrZt9W6dOnfDcc89h586deOSRR7B27Vr7vtDQUEydOhVff/01nn/+eXz44Yd1UlcbhhuVyEXX8Qf5KKKk2jX7ERFR3fH09MTo0aMxd+5cpKam4oknnrDv69ixI3bt2oUDBw4gMTERf/3rXytMU1KV6OhodOrUCRMmTMDx48exb98+zJs3z6FMx44dkZycjLi4OPz222947733sHHjRocyYWFhOH/+PI4dO4arV6+iuLi4wrXGjh0LV1dXTJgwASdPnsSePXswY8YMjBs3rsKkuDVlsVhw7Ngxh1diYiKio6PRs2dPjB07FkePHkVCQgLGjx+Pe++9FxERESgsLMT06dOxd+9eJCUlYf/+/Th8+DC6du0KAJg5cyZ27NiB8+fP4+jRo9izZ499X11huFGJ/vo5rDG8jX/oP9K6KkREVIlJkybh+vXriImJcRgf8/LLL+OOO+5ATEwMBg0ahODgYIwYMaLa55VlGRs3bkRhYSH69++Pp556qsKDLg8//DCee+45TJ8+HX369MGBAwcwf/58hzKjRo3CkCFDcN999yEgIKDSx9Hd3d2xY8cOZGZmol+/fnj00UcxePBgrFixomY3oxJ5eXno27evw2vYsGGQJAn//e9/4efnh3vuuQfR0dFo164d1q9fD0CZkuXatWsYP348OnXqhMceewxDhw7FK6+8AkAJTdOmTUPXrl0xZMgQdOrUCf/6179uu75VkYTWE8Q4WU5ODnx8fJCdna3qbMWZZw6g2edDcVH4o9Urv6l2XiKi+qKoqAjnz59H27Zt4erqqnV1qBGq6nesJn+/2XKjEklWxmbrUHcDtoiIiOjWGG5UIsnKI3c6WDWfLZmIiKgpY7hRiaRTHrvTwcInpoiIiDTEcKMWqVzLjcZVISIiasoYblQi6dgtRURNA/8bR3VFrd8thhu1SMqAYj0sbLkhokbJNuttQYFGi2VSo2ebFfp2F8Pm8gsqkfTKrZRh5ZgbImqUdDodfH19kZGRAUCZc0Wq4UzBRDdjtVpx5coVuLu7Q6+/vXjCcKMSSVIawfSwoITphogaKduK1baAQ6QmWZbRunXr2w7NDDcqsT8tJQmGGyJqtCRJQkhICAIDA1FSUqJ1daiRMRgMkOXbHzHDcKMWuexWCqsFvLVE1JjpdLrbHhdBVFc4oFgltkn8AEBYzRrWhIiIqGljuFGJVL7lxsJwQ0REpBWGG5VI5ZpnhbBoWBMiIqKmjeFGJZKurOXGypYbIiIizTDcqESSyg0gZrghIiLSDMONSiRZgkWUPpdvZbcUERGRVhhuVCIBMEMZd8OnpYiIiLTDcKMSSZJgtd1OttwQERFphuFGJbLElhsiIqL6gOFGJZIkwVJ6OwVbboiIiDTDcKMie7jh01JERESaYbhRkcXWLSUYboiIiLTCcKMic+ntlNhyQ0REpBmGGxWZbSuBW0q0rQgREVETxnCjIvvTUmy5ISIi0gzDjYrKHgVnyw0REZFWGG5UVGLvljJpWxEiIqImjOFGRbaWG465ISIi0g7DjYo4oJiIiEh7DDcqsrfccPkFIiIizTDcqIgtN0RERNpjuFGRPdxYOaCYiIhIKww3KiobUMxuKSIiIq0w3KjILPFpKSIiIq0x3KiorFuK4YaIiEgrDDcqEpBKf7BqWxEiIqImjOFGRVbb7RQWbStCRETUhDHcqMhqG3NjZcsNERGRVhhuVGRruRFWttwQERFpheFGReyWIiIi0h7DjYrs4YbdUkRERJphuFGRVWLLDRERkdYYblQk2C1FRESkOYYbFZV1SzHcEBERaYXhRkVC4pgbIiIirTHcqMjecgOGGyIiIq0w3KiI3VJERETaY7hREee5ISIi0h7DjYpsY24kttwQERFphuFGRfYBxVwVnIiISDMMNyoq65ZiuCEiItIKw42KOKCYiIhIeww3KhJcfoGIiEhzDDcqsi2/ILFbioiISDMMNyqySOyWIiIi0hrDjYqEpCv9iS03REREWmG4UZHggGIiIiLNMdyoSEgSAI65ISIi0hLDjYqsKO2W4tNSREREmmG4URG7pYiIiLTHcKMiq21tKQ4oJiIi0gzDjYrKWm4YboiIiLTCcKMizlBMRESkPYYbFVlL57mRGG6IiIg0w3CjIi6/QEREpD2GGxWVdUsx3BAREWlF83CzcuVKhIWFwdXVFZGRkUhISKiyfFZWFqZNm4aQkBAYjUZ06tQJW7dudVJtq2YfUMxwQ0REpBm9lhdfv349Zs2ahdWrVyMyMhLLli1DTEwMTp8+jcDAwArlTSYT7r//fgQGBuKrr75Cy5YtkZSUBF9fX+dXvhL2R8EZboiIiDSjabhZunQpJk+ejIkTJwIAVq9ejS1btmDNmjV48cUXK5Rfs2YNMjMzceDAAbi4uAAAwsLCqrxGcXExiouL7Z9zcnLU+wI3YrcUERGR5jTrljKZTDhy5Aiio6PLKiPLiI6OxsGDBys9ZvPmzYiKisK0adMQFBSEHj16YMmSJbBYbv50UmxsLHx8fOyv0NBQ1b+LTdmAYj4tRUREpBXNws3Vq1dhsVgQFBTksD0oKAhpaWmVHvP777/jq6++gsViwdatWzF//ny88847+Mc//nHT68ydOxfZ2dn2V0pKiqrfo7yyAcWizq5BREREVdO0W6qmrFYrAgMD8cEHH0Cn0yE8PByXLl3CW2+9hYULF1Z6jNFohNFodEr9hMSWGyIiIq1pFm78/f2h0+mQnp7usD09PR3BwcGVHhMSEgIXFxfodDr7tq5duyItLQ0mkwkGg6FO63wrfBSciIhIe5p1SxkMBoSHhyM+Pt6+zWq1Ij4+HlFRUZUeM3DgQJw7dw7Wcms3nTlzBiEhIZoHG4CT+BEREdUHms5zM2vWLHz44Yf497//jcTERDz99NPIz8+3Pz01fvx4zJ07117+6aefRmZmJp599lmcOXMGW7ZswZIlSzBt2jStvoIDe8sNVwUnIiLSjKZjbkaPHo0rV65gwYIFSEtLQ58+fbB9+3b7IOPk5GTIcln+Cg0NxY4dO/Dcc8+hV69eaNmyJZ599lnMmTNHq6/gQHBtKSIiIs1JQjStR3tycnLg4+OD7OxseHt7q3rueSs+wWtXZ6DQLRhuc06rem4iIqKmrCZ/vzVffqExsXJAMRERkeYYbtQkScobu6WIiIg0w3CjorIxN02qp4+IiKheYbhRESfxIyIi0h7DjYpsLTcAW26IiIi0wnCjKo65ISIi0hrDjYrKxtzwaSkiIiKtMNyoSJQ+LcVHwYmIiLTDcKMie8sNl18gIiLSDMONiiSJC2cSERFpjeFGRVZwzA0REZHWGG7UJJc+LQUBcCI/IiIiTTDcqEiUv51svSEiItIEw42KyibxA2DlXDdERERaYLhRk8SWGyIiIq0x3KhIOIQbttwQERFpgeFGRVL5bim23BAREWmC4UZFVrnc7eSYGyIiIk0w3KiKY26IiIi0xnCjIsFuKSIiIs0x3KhILp3EDwDDDRERkUYYblQkATCL0lvKMTdERESaYLhRkSxJsKK09YYtN0RERJpguFGTBFhtt5Tz3BAREWmC4UZFSsuNLdyw5YaIiEgLDDcqkgBYwDE3REREWmK4UZEkAYJjboiIiDTFcKMiWZLKWm4YboiIiDTBcKMiSULZ01LsliIiItIEw42KJA4oJiIi0hzDjYocBhTzUXAiIiJNMNyoiJP4ERERaY/hRkVS+Un8rAw3REREWmC4UZEsSbAKttwQERFpieFGZRxzQ0REpC2GGxVx+QUiIiLtMdyoyHHMDVtuiIiItMBwoyK5/CR+bLkhIiLSBMONihwn8WPLDRERkRYYblQkseWGiIhIcww3KpJQbuFMznNDRESkCYYbFXHMDRERkfYYblTk8LQUx9wQERFpguFGRZznhoiISHsMNypyWBWc89wQERFpguFGRZIkQXDMDRERkaYYblQkSYBFsFuKiIhISww3KpKlco+CM9wQERFpguFGRRJQ1i3FMTdERESaYLhRkSyz5YaIiEhrDDcqK5vEjy03REREWqhVuElJScHFixftnxMSEjBz5kx88MEHqlWsIeI8N0RERNqrVbh5/PHHsWfPHgBAWloa7r//fiQkJGDevHlYvHixqhVsSBxmKOaYGyIiIk3UKtycPHkS/fv3BwB8+eWX6NGjBw4cOIDPPvsM69atU7N+DYosARbOc0NERKSpWoWbkpISGI1GAMDu3bvx8MMPAwC6dOmC1NRU9WrXwEiQINgtRUREpKlahZvu3btj9erV2LdvH3bt2oUhQ4YAAC5fvozmzZurWsGGRJK4/AIREZHWahVu3njjDbz//vsYNGgQxowZg969ewMANm/ebO+uaookSSr3tBRbboiIiLSgr81BgwYNwtWrV5GTkwM/Pz/79ilTpsDd3V21yjU0cvkBxXwUnIiISBO1arkpLCxEcXGxPdgkJSVh2bJlOH36NAIDA1WtYEMiAbAKttwQERFpqVbhZvjw4fjkk08AAFlZWYiMjMQ777yDESNGYNWqVapWsCFxmKGYY26IiIg0Uatwc/ToUdx9990AgK+++gpBQUFISkrCJ598gvfee0/VCjYkEsrPUCw0rQsREVFTVatwU1BQAC8vLwDAzp078cgjj0CWZdx5551ISkpStYINieQwQzFbboiIiLRQq3DToUMHbNq0CSkpKdixYwceeOABAEBGRga8vb1VrWBD4jBDMcfcEBERaaJW4WbBggWYPXs2wsLC0L9/f0RFRQFQWnH69u2ragUbElnimBsiIiKt1epR8EcffRR33XUXUlNT7XPcAMDgwYMxcuRI1SrX0ChjbthyQ0REpKVahRsACA4ORnBwsH118FatWjXpCfwA26rgtgHFbLkhIiLSQq26paxWKxYvXgwfHx+0adMGbdq0ga+vL1599VVYrU24xaL88gtsuSEiItJErVpu5s2bh48//hivv/46Bg4cCAD4/vvvsWjRIhQVFeG1115TtZINhSxJELaWm6Yc8oiIiDRUq3Dz73//Gx999JF9NXAA6NWrF1q2bIlnnnmmyYYbCWy5ISIi0lqtuqUyMzPRpUuXCtu7dOmCzMzM265UQyXL4JgbIiIijdUq3PTu3RsrVqyosH3FihXo1atXjc+3cuVKhIWFwdXVFZGRkUhISKjWcXFxcZAkCSNGjKjxNeuChHKT+PFRcCIiIk3UqlvqzTffxEMPPYTdu3fb57g5ePAgUlJSsHXr1hqda/369Zg1axZWr16NyMhILFu2DDExMbdchPPChQuYPXu2fRmI+kCSALPQKR/YckNERKSJWrXc3HvvvThz5gxGjhyJrKwsZGVl4ZFHHsEvv/yCTz/9tEbnWrp0KSZPnoyJEyeiW7duWL16Ndzd3bFmzZqbHmOxWDB27Fi88soraNeuXW2+Qp2QJAkWlIYbq1nbyhARETVRtZ7npkWLFhUGDh8/fhwff/wxPvjgg2qdw2Qy4ciRI5g7d659myzLiI6OxsGDB2963OLFixEYGIhJkyZh3759VV6juLgYxcXF9s85OTnVqlttyBJgZrcUERGRpmrVcqOWq1evwmKxICgoyGF7UFAQ0tLSKj3m+++/x8cff4wPP/ywWteIjY2Fj4+P/RUaGnrb9b4ZCWy5ISIi0pqm4aamcnNzMW7cOHz44Yfw9/ev1jFz585Fdna2/ZWSklJn9XNsuWG4ISIi0kKtu6XU4O/vD51Oh/T0dIft6enpCA4OrlD+t99+w4ULFzBs2DD7NtuMyHq9HqdPn0b79u0djjEajTAajXVQ+4okCWy5ISIi0liNws0jjzxS5f6srKwaXdxgMCA8PBzx8fH2x7mtVivi4+Mxffr0CuW7dOmCEydOOGx7+eWXkZubi3fffbdOu5yqQ5IkmO3hhmNuiIiItFCjcOPj43PL/ePHj69RBWbNmoUJEyYgIiIC/fv3x7Jly5Cfn4+JEycCAMaPH4+WLVsiNjYWrq6u6NGjh8Pxvr6+AFBhuxYcZihmyw0REZEmahRu1q5dq3oFRo8ejStXrmDBggVIS0tDnz59sH37dvsg4+TkZMhywxgaJEtS2Tw3DDdERESa0HTMjc306dMr7YYCgL1791Z57Lp169SvUC1JEltuiIiItNYwmkQaCLn8mBsLww0REZEWGG7UxJYbIiIizTHcqMih5YbhhoiISBMMNypSnpZiuCEiItISw42KlJYbri1FRESkJYYbFXGGYiIiIu0x3KhIkgCz4IBiIiIiLTHcqIirghMREWmP4UZFyqrgXFuKiIhISww3KpIkifPcEBERaYzhRkWOLTcMN0RERFpguFER15YiIiLSHsONiiSHGYotQFEO15giIiJyMoYbFTnMUGzKBd7pAsSN0bRORERETQ3DjYpkSSqb5wYASvKBszsBq1W7ShERETUxDDcqcpihuDxTrvMrQ0RE1EQx3KhIgoSSysJNcZ7zK0NERNREMdyo6OYtN/nOrwwREVETxXCjIklC2arg5ZnYckNEROQsDDcqkiWp8pYbi8n5lSEiImqiGG5UJJWfobg8c7HzK0NERNREMdyoSJYkWCFX7JpiuCEiInIahhsVSaXvxTA47rAw3BARETkLw42KJEmJNxXCDVtuiIiInIbhRkWl2QbFcHHcwQHFRERETsNwoyK5NN0UCbbcEBERaYXhRkU3H3PDlhsiIiJnYbhRka3lpgJzkXMrQkRE1IQx3KjIlm10sDjuMLPlhoiIyFkYblRkCzd6mB138FFwIiIip2G4UZHtUXB9hZYbhhsiIiJnYbhRkVzacuNyY7jhgGIiIiKnYbhRkVT6vJQ7bhhAzAHFRERETsNwoyJby82XlkGOOzigmIiIyGkYbtRUGm7eMj8GDHsPGDhT2cABxURERE7DcKMiudzaUuKO8YBvqLKDA4qJiIichuFGReWn8BMCgM6ofOCAYiIiIqdhuFFR+RmKBQDoS8MNW26IiIichuFGReVXX7AKAehK15hiyw0REZHTMNyoSCrfciPAlhsiIiINMNyoiC03RERE2mO4UVGFVcHZckNEROR0DDcqKh9tlJYb29NSDDdERETOwnCjIrnCmJvSbinOUExEROQ0DDcqqjjmhi03REREzsZwo6Ly4UaZ54YtN0RERM7GcKMiqdyoG2EFW26IiIg0wHCjItmh5UaUPS1lNQNWqzaVIiIiamIYblRUfhI/q0DZPDcAW2+IiIichOFGRQ4tN6Jcyw3AuW6IiIichOFGRVW33HBQMRERkTMw3KjMlm+EEMoHW8Bhyw0REZFTMNyoTFeabqzCtsH2xBRbboiIiJyB4UZltlmKLaI03ejZckNERORMDDcqk0vvqNXWdMO5boiIiJyK4UZlZd1SN7bcsFuKiIjIGRhuVCaXPg9uYcsNERGRJhhuVCaz5YaIiEhTDDcq08k3e1qKLTdERETOwHCjMvvTUrZ0Y5ulmE9LEREROQXDjcpsSzCUjbkp7ZbiPDdEREROwXCjsrJuKbbcEBERaYHhRmVyhRmK2XJDRETkTAw3KtPd+Cg4W26IiIiciuFGZbYxN/ZuKT4tRURE5FQMNyqzTeJnX36B89wQERE5FcONynQ3LpzJlhsiIiKnYrhRmX1AsbV0A1tuiIiInIrhRmXyjY+Cs+WGiIjIqRhuVKYrvaP2bikXN+W9pEibChERETUxDDcqK+uWsoUbd+W9JF+jGhERETUt9SLcrFy5EmFhYXB1dUVkZCQSEhJuWvbDDz/E3XffDT8/P/j5+SE6OrrK8s5WYRI/Q2m4MRVoUyEiIqImRvNws379esyaNQsLFy7E0aNH0bt3b8TExCAjI6PS8nv37sWYMWOwZ88eHDx4EKGhoXjggQdw6dIlJ9e8chUm8bO33DDcEBEROYPm4Wbp0qWYPHkyJk6ciG7dumH16tVwd3fHmjVrKi3/2Wef4ZlnnkGfPn3QpUsXfPTRR7BarYiPj3dyzSunk24YUGzwUN4ZboiIiJxC03BjMplw5MgRREdH27fJsozo6GgcPHiwWucoKChASUkJmjVrVun+4uJi5OTkOLzqknTjDMW2AcXsliIiInIKTcPN1atXYbFYEBQU5LA9KCgIaWlp1TrHnDlz0KJFC4eAVF5sbCx8fHzsr9DQ0Nuud1UqdkvZWm44oJiIiMgZNO+Wuh2vv/464uLisHHjRri6ulZaZu7cucjOzra/UlJS6rROuhvnueGAYiIiIqfSa3lxf39/6HQ6pKenO2xPT09HcHBwlce+/fbbeP3117F792706tXrpuWMRiOMRqMq9a0O6cYZig2eyrspz2l1ICIiaso0bbkxGAwIDw93GAxsGxwcFRV10+PefPNNvPrqq9i+fTsiIiKcUdVq05WOubFP4mf0Ut7NRYClRJtKERERNSGattwAwKxZszBhwgRERESgf//+WLZsGfLz8zFx4kQAwPjx49GyZUvExsYCAN544w0sWLAAn3/+OcLCwuxjczw9PeHp6anZ97DR3bgquC3cAEBxLuBe+cBnIiIiUofm4Wb06NG4cuUKFixYgLS0NPTp0wfbt2+3DzJOTk6GLJc1MK1atQomkwmPPvqow3kWLlyIRYsWObPqlZJunMRP5wLo3QBzIcMNERGRE2gebgBg+vTpmD59eqX79u7d6/D5woULdV+h22Cb58beLQUARs+ycENERER1qkE/LVUf6UsH3Zgt1rKNtq4phhsiIqI6x3CjMlcXHQCgqKSScMMnpoiIiOocw43KXF2UW1pUYinbaPRW3ovrdnZkIiIiYrhRnave1nJTPtywW4qIiMhZGG5U5maoJNzYJvJjuCEiIqpzDDcqq3LMDcMNERFRnWO4UZlRr9zSwkq7pTigmIiIqK4x3KgswEtZx+pSVmHZRnu44YBiIiKiusZwo7KuIcqTUb+m5lRcgoHdUkRERHWO4UZlbf09YNDJyDdZylpvGG6IiIichuFGZS46GYHeStdURm6xspGT+BERETkNw00daOZhAABczzcpG2yT+BVmaVMhIiKiJoThpg7Ywk2mLdx4Birv+Rka1YiIiKjpYLipA83cS8NNQWm48SgNN4XXAbNJo1oRERE1DQw3daBCy42bX9nOzN81qBEREVHTwXBTB5p53hBu5HK3+eunNKgRERFR08FwUwfs3VL5lXRBpZ1wcm2IiIiaFoabOmDrlrqWV1y2sdMQ5b31AMfCQgD5V51UMyIiosaP4aYOtGnuAQA4l5EHi22W4sipynvBDUHmm1eBt9oDv2x0Yg2JiIgaL4abOtA+wANuLjrkmyw4f7V04j7vlsp7brpj4X3vKO9bX3BeBYmIiBoxhps6oNfJ6NZCmbjvWEq2stHDX3kvzlYm8/tiDHBkXdlBFj4iTkREpAaGmzoS0UZ5/Dvh/DVlg6tv2c7vlwKntwL/92zZtqJswGJ2XgWJiIgaKYabOnJnu+YAgB9+z1Q2lH8cfP+7lR/0bu+yn61WoKSojmpHRETUeDHc1JGIMD/oZAnJmQVlq4PfSs5F5ekpAPh0OPB2R6VFh4iIiKqN4aaOeLm6oEdLHwDA92evKBsHzb31gebSx8fPfwcU5wDn4uuohkRERI0Tw00dGtxFWVNq+8k0ZcOgF299UElBWesNAMj6OqgZERFR48VwU4eG9ggGAHx/7iqyC0uUje7+VR+UnQKc3VX2WedS9nNuOnB6u2P4ISIiIgcMN3WoY5AXOgR6osQisPtU6fw2j66p+qD37wE+/1PZZ6ncP6Iv/gx8MRo4+on6lSUiImokGG7q2B97hQAAPk9IVjZ4BjoWkF2AZ364+QmunAYW+Sivy0eVbac2qV9RIiKiRoLhpo493r81XHQSjiRdx/GULKB5R8cCk3YCgV1vfoJd8yvZKKlZRSIiokaF4aaOBXq7YlivFgCA5d+cA3R6wL+TsvOpeKDlHTU/qSlPxRoSERE1Lgw3TvDMfR2gkyXsTkzHwd+uAY+vBybtAlpFlBUyeFX/hNfOqV9JIiKiRoLhxgk6BHri8f6tAQCvbT0Fq29bILS/Y6E7S1cNd/G49QkLrqlcQyIiosaD4cZJZkZ3hJdRj5OXcvDljykVCwycCQz/FzD7DBA+8dYnvHF1cSIiIgLAcOM0zT2NeDZaGUz8jy2JSMkscCxg9AT6jlXeh7556xO+06kOaklERNTwMdw40cSBbRHRxg95xWbM3nAcVutNJuPTG4B51WiZSf9F3QoSERE1Agw3TqSTJbzzWG+4G3Q4dD4Taw9cuHlhF1fg/sVlnx98u2KZVQOApd2Ay8eAva8DSQeBXzYpK4oTERE1UQw3TtamuQfmPaTMa/PGtl9xJCnz5oX7jgPc/IBOQ4GIScCoj4HmHRzL5FwCPrgX2BsLrB0CbJgAHP6wDr8BERFR/cZwo4HH+7dGTPcgmCxWPPXvH/H7lZvMW+PeDJj1K/DnzwFZBno+CgR1v/UFtv0dKMyqfF/OZaCksNZ1JyIiqu8YbjQgSRL+OboPerfywfWCEoz7OKHiAGMbF1cl2NiMfL96F9mzpOK2a78BS7sCHwxy3F6QycU4iYio0WC40Yi7QY+Pn+iHsObuuJRViMfeP4hzGbm3PtDFDegz9tblLv/k+NliBpaXzoZ85dey7RsmAm+2rRh4iIiIGiiGGw35exqx/q9RaB/ggdTsIoz81wF8e+bKrQ98aOmtW3AuJgAHVgCpx4F1fwRebe64/+pZ5f2Xr5X31GMVz3E9CfhlIwcoExFRg8Jwo7Egb1d8+dcoRLTxQ26RGRPWJOC1LadQbLbc/CAXV6D3n4G/fgc0a3/zcjvnAe/fA1zYV3Hfz+uBiz9WXbl3ewEbngBObazWdyEiIqoPGG7qgeaeRvznqUiMu7MNAODDfecxfMV+nLyUXfWBIb2BHo84bnNvXnnZG333FvDRYMdt58uFoN/3lv188Uj1zgkAlhLgp8+UVh8iIiINMNzUE64uOrw6ogc+Gh+B5h4G/JqWi4dXfI95G0/gWl7xzQ80epf9/MhHwNgNta/Ev/9Y9vMnw8t+Lr5FyCrv6L+B/z4DLA+vfT2IiIhuA8NNPRPdLQjbZ96DYb1bwCqAzw4lI/wfuxG7LRHX800VD/AKLvu5y0OAR+DtVcBUAGRfctz203+qf/yPa5V3a8nt1YOIiKiWGG7qoQAvI5aP6Ysv/xqFHi2Vlpn3v/0dA9/4Bgv/exLnr+aXFe42Qnk98BpgcAd8Q8v2GTxrfvFLR4B/drv5fiGA09uBnNTK96efLPv51y01vz4REdFtkoRoWhOc5OTkwMfHB9nZ2fD29r71ARqzWgX+sSUR206mIjW7CAAgScCA9s3xp/BQDOkRDFcXneNBb7QFCjOBxz4BivOUbqLeY4CODwC7FwJZyTWvyLD3gPAJwK9bgbgxyrZF2cD+d4GrZ4CArkBeGnBgueNxj64BeoyqxTcnIiIqU5O/3ww3DYQQAt+fu4q1+y/gm18z7Ns9jXoM7hqImO7BiGzbDM09jUBeBnDhe6DrMEDnosxI7OKmHLAy0nGem5p44XfgrXZln5/cAayJcSzTrD2Q+ZvjtnlpZdeviqVEmVDQK6h29SMiokaL4aYKDTXclJeSWYANRy5iw48p9tYcm05BnrizXXNEtm2OyHbN4O9pdDw46SCw7iHgvpeU8Tr/neacSo+JAzoPLfuc+rPSovTwcqBFX2XbtheBQ6uBcRuB9veVlS3/K1qcA7j6OKfORERUbzDcVKExhBsbq1Xgp5QsbD2Rin1nr+BMesU1qtoHeKBHSx/0bOmDbi280TXYG34uZmV8js3hj4Atz1d+kYAutW/pKS+oJ3DHOGV8kFcQsKhcQFmUrUwUuNhP+RzYHXjmQNn+r6cAyQeBwG7Ame3A1P1AcI/Kr2O1ANkpgF/Y7deZiIjqDYabKjSmcHOja3nFSDifiUPnM/HD79fwa1rlyzkEeBnRKcgTHQO90CHQE2HNPdDOkIkWa/uVFZqTpHRpFVwDlvVUr5It+gJ3TgO+fqps26JsZZDyF6OVz6F3ApF/BTJ/ByKnArEtHc/R+SHgz58pkxD+vyeBtvcAD69QBiN9/09g9yJg6JvKOW508UdlLqBmbW9d1+JcIP0XoFV/x/W9AMBsAoqyAM/bfDqNiIiqheGmCo053Nzoer4JR5Ov49TlHJy8nI1TqTlIybz5iuCvu3yEP+u+AQDM77MfbZq7o01zD0R/1Q2S1eysatfOY58C3R4GXmkGiHKzOz+5A2h9p/J4+zevAse/ULYvusXcPUIAn44Eft+jLHfRb5LjflvL0/QfAf+O6n0PIiKqFMNNFZpSuKlMblEJzmbk4VxGHs6m5+JcRh6SMgtwMbMQJosFw+X9OCI64aIoa5HoLyXiS+OrNz3nnj+fRjN3A3qvKdcaEtAViHgS2PZCXX6dMjoDYKlkHiBACTLv9VVagmxmnwM8A25+vvJddQYv4KWLZfvyrgBvd1B+dvMD5ly4raoTEdGt1eTvt95JdaJ6wsvVBXe09sMdrf0ctlusAmk5RUi6dieirhUgKbMAydcKkJSZj8RrPTHGNA/uKMI43W78rWQ6dhlfQJCUhf5FK5GxTlmeYaA8F58ZYgEAs6+PQOLBTnjRfTDuLoiv+y92s2ADABmJjsEGUMLJPS8oy1D0HQcMX1G2L/F/jmOQTLnK5IYubkrXV8G1sn0c20NEVO+w5YZuSQiBrIISJGUWIC27EBm5xchLv4DMnBz8agrC9QITruebcC3fBKM5F+2ly/hJdAAgAQDukY9jrC4eWyyReM+w8qbXyYYnfFBxULRTTP4G8A1TBiz/95nKy/R7CrhvnjLAem3pk1+dhihdYtcvAAGdqn89q1XpPtO53LqsuRj47FGgxR3A/a9U/xpERI0Iu6WqwHBTtwpNFlzLL8b1/BJkloaezHwTrhc4vhfk5cI7/zyOFAZhm/7vOCtaYXLJ8xgiJ2C1YRkA4IrwRoCU43D+/1oG4LrwxBP6nRp8u1LNOwDXzt18n5ufEnbyryjbbEti5GcAE7cpMzcfXAH4hALTEpSlKn7ZCFz7TVkI9dORQOF1YOYJwLc18Nse4NMRyjkWXK84uLkyJUXK6vFERI0Ew00VGG7qFyEEcorMSggqDUPWy8eRKvyQUeyCAb/9E21zj2K+zxLkFRThbLEvcgoKsVb/Ou7S/YLD1k740jIIb7l8AAAoFAb8Klqjr3yT8FHfBPVwXLLiRpP3ALmpQNzjyufwiUCvx4BNzwAPv6c8KWY2Af8IAFoPACZuBQ6uVGai/tO/ga5/rHhOixkouOq4LpmN1Vq98ERE5GQMN1VguGn4hBAoKixA8a87cC0wCtlWV2QXliCvyIz8YjPyTRbkFpUgIO07jD33PJINHZAjeeEXXVes1Y9G66JTeMO0BP9nicKikvH43fUvlV5nhyUCX1vuwvulLUlayfNoDc/8ypfMuHr/e3ARJfDZrYwRst79AuR9b5UVeOxToMsfgdzLgE8rZdtbHZRWpSGvK8tyGL2A/4xSngwDgCl7lfmNinLKZos2FytPkLm4Ahm/Ko/ACytw6SjQ8X5lLBIRUR1iuKkCw00Tk5GoDPqtZPkHIQSKzVYUFBTA91/dIReXPR6ecP9GpHl2QaHJDK/0w3DPPgP3/Evof/lTe5n/8/0LhmXVYMX0BiLTrS2aFZ4HAPzfoO0we7fAgzsGwViciaRuU9Hm1GqH8lcHzEdx55GQfVvARSfDRSfDoJPhopOgkyVIDD5EpAKGmyow3NBtOx4HmPKUAcZWC3DofaDvWKVlQ28ErpyGNe8KisLuQ0GxGf7vKK0fZoM39CZlDNHF0GFolfJ/9lP+3uwe+BYmoVlhEr4N/AvcTJnon7VVk69XW5dFMwQgG8+UPIse8gWM0+3EDms/DJF/xBNiAZL0YXDRyegoXUKh3httcRl3Ww5BLwtsdx+Oldcm4YD7H7Am6CV7SHLRyTDoJbjoZOhlGS56qTQ42V4SDPqyzyGZh1Dk0x4usgS/nFPIDY2Gi4sObsVX4SJZIfm0LD2XVHpuGXqdBL0sQS8r22WZYYyoPmK4qQLDDWkqI1FZ1DTiSUAuXc09KxnwDAb0Bseyif8HpB5XHle38QkFIiYCbQY6LFoqWoRDunzE4XCTdxsYcpLq6pvUmW8tvXCv7mcAStdgjO5HAMAWS3+8bh4DX+Tj34bXEWf5A45Z2yPB2gVZ8MIf5KNYY3i7wvlGFr+CDw3vwL90cPpP1g54xLQIApWPLZIkwFcugknnBr1ODxedBG+pCGbZCB9dIb4smIz/M/4Rn3hOhE6W4SJL0EmAQScg61zsLVZ6WS59L/1cGqp0sgQXnRKilM+l59CVfS5frvxnfel5dDIgSaXlJWWfzn6MDLncNn25fbbttjraPsul52Kwo/qM4aYKDDfUqFityrttEPD1JCBxM3DnM2XhqTALgAB2vwJ0ilEGIaccUp7KKu++ecCZHcClH51V+3pjk2UA2kuX0VO+4LD9P+bBiJJPob2cigOWbsiHG+7XKSGyS9FaFEEJpO+5rMDDuoN4xvQ3fGftBRNcYELZY/6D5GMYpjuI50umApDghQL4SHkOk2XeSA8zXGFCHtxvWqYyf9Z9AwNKcE60xCD5ON4zj6zROexByR58UBqolAAkAZBLQ5QEJQzqZaUFzNYNKUuATlLCkvKOssAlOYYtSXK8pu2zLNlesJ+n/PnlcufXyUoZudw+SVJCp3zjMfb3snqVv5ZU7nvLN9RHklAuEKL0vI5lbzyXXMnx5d8llJWxvSRI9mFs9mvayjfhbl6Gmyow3BDVkNkESDKgKzfn5/UkZVZo7xBl4PHqu4CsJGXenzPblTLjNwOfPHzr84fdDVzYVzd1b0SOGSMQaE5FC8slXJJboKX1MgDgiuSPQ/pw7NDdiwdKvsEwy26H4zKFF5Zb/4SLIgB+yMJlazNcFT7oIF2CDIEfrF2RBzfM038GV8mEf5ofRabwggUyvJGPK1Am/Byv24FCGLHBMqha9R0hf49gKROrLcNgm/PKgBJYIcHM+WNvixKKyoJU+dAllSsj2wOjYwATAqVBEQ5BzB66Ss9tP489WFW8plwa0Gz7AeVzpyBPvDL8Jgsc1xLDTRUYbojqAXOx0qJkexqrpBDQuyrvFxMAgydgyldmg24VAXi1APLSgZ8+BfIylMkPD62ueN6+fwG6j1SCl84I/LgG+DlO2efWTFkM1VritK9JZQpcg+FelAYAOBP6J3RK2WDfZ5YNyHFrjWb5FadwOBY0CrkuzdEmOwGSsMKnOBXXjS2Qq/eHLEpQIHvC3ZwNAYEi2RNu5hxccwnGafe+KJQ8YLTkIV3fErLVBP+SVFzWtwKEFT6Wa0jWt0Wo6Rx8zVdxwHgXzNCjAK7wslzHdfjAKgAzZFgFIKxWWKwCVgCi9N0qlNndhRBKGZS+l362Wq2wCCVMWIUofZXbLwQa61/gO1r74utnBqp6ToabKjDcEJHDfD75V5VxUF0eUgaIC6sStE5vVR6Tz78ChPRRJma0mpVV5a9fULr/8tKVp/G6jQACuyhrmAFAxxilNaqkoOyavq2V8VXUeLj6AkVZys86I2AprrgdAGQXJVQbvJTlXCohvFsC+Vdg6RAD+copwGqBlHMJUmkYNwd0hzmol/I7ai6E/toZCFkPi2szuFw9BatrM5g9giCV5MPUvCuEJEMy5cOqMwKSDMmUh8JmXWEx+MDj8gHIJfnwvrgHFr0b0rs9Cc+0BOQER8Eq6eGaex6Fnm3glvM7sgMiAKsZelMWrLIBxa6BsEAHWM1wKb4OvSkHJoMPLJChtxSgyDUIFlkPa4t+6N+3j6q3m+GmCgw3RNRgWUqUP27lZ58WQtlekq90H7r6KGVyLgGeQcogdnORMh2CR6CyDlvaCWUmbWFV9l/6ETB4AIPmAhmngAMrgHO7gMELgKvnlC7Hvn9RuhxP/bdivWS9Evxu5NMayE4GvFsp8yyl/KBsb3MXkPR93dwjqh88g4BZiWVj/1TAcFMFhhsiokamIFPpytQb4NDPI6xKsCvOU8JbSYHSKleYqTyhaMpTQp/BA8i7orTICYvSGnf5JyDzPBDcU2mRyb8KePgDRm8lEAqr0j16PUmZJNO9OaB3U8amyXolZJ78Wjk+/6pSj4AuSrA8s02pn6sPUJQN+HcCvEKAoO7KzOFXzgB+bYATG5S6nN2pPGFZUqiE0jM7lJZAi0mpg9kEJB9QWhCLskvDZonSYpR0QAm+zdoB3i2Va+ZcAlJ/Vr4rALTqr3QHA0pdSgqB7IsAhFIvj4DSiTwtyn2DUK6TcwmApNxPG48A5Z/HgBmqr4XHcFMFhhsiIqI6VgdLudTk7zcXkSEiIiJ1abxGHcMNERERNSr1ItysXLkSYWFhcHV1RWRkJBISEqosv2HDBnTp0gWurq7o2bMntm5tWNPUExERUd3RPNysX78es2bNwsKFC3H06FH07t0bMTExyMjIqLT8gQMHMGbMGEyaNAk//fQTRowYgREjRuDkyZNOrjkRERHVR5oPKI6MjES/fv2wYsUKAMqkR6GhoZgxYwZefPHFCuVHjx6N/Px8/O9//7Nvu/POO9GnTx+sXl3JpF434IBiIiKihqfBDCg2mUw4cuQIoqOj7dtkWUZ0dDQOHjxY6TEHDx50KA8AMTExNy1fXFyMnJwchxcRERE1XpqGm6tXr8JisSAoKMhhe1BQENLS0io9Ji0trUblY2Nj4ePjY3+FhoaqU3kiIiKqlzQfc1PX5s6di+zsbPsrJSVF6yoRERFRHdJ0aVZ/f3/odDqkp6c7bE9PT0dwcHClxwQHB9eovNFohNFoVKfCREREVO9p2nJjMBgQHh6O+Ph4+zar1Yr4+HhERUVVekxUVJRDeQDYtWvXTcsTERFR06Jpyw0AzJo1CxMmTEBERAT69++PZcuWIT8/HxMnTgQAjB8/Hi1btkRsbCwA4Nlnn8W9996Ld955Bw899BDi4uLw448/4oMPPtDyaxAREVE9oXm4GT16NK5cuYIFCxYgLS0Nffr0wfbt2+2DhpOTkyGXm8Z5wIAB+Pzzz/Hyyy/jpZdeQseOHbFp0yb06NFDq69ARERE9Yjm89w4G+e5ISIiangazDw3RERERGrTvFvK2WwNVZzMj4iIqOGw/d2uTodTkws3ubm5AMDJ/IiIiBqg3Nxc+Pj4VFmmyY25sVqtuHz5Mry8vCBJkqrnzsnJQWhoKFJSUjie5zbwPt4+3kN18D6qg/dRHU39PgohkJubixYtWjg8aFSZJtdyI8syWrVqVafX8Pb2bpK/eGrjfbx9vIfq4H1UB++jOpryfbxVi40NBxQTERFRo8JwQ0RERI0Kw42KjEYjFi5cyLWsbhPv4+3jPVQH76M6eB/VwftYfU1uQDERERE1bmy5ISIiokaF4YaIiIgaFYYbIiIialQYboiIiKhRYbhRycqVKxEWFgZXV1dERkYiISFB6ypp6rvvvsOwYcPQokULSJKETZs2OewXQmDBggUICQmBm5sboqOjcfbsWYcymZmZGDt2LLy9veHr64tJkyYhLy/PoczPP/+Mu+++G66urggNDcWbb75Z11/NaWJjY9GvXz94eXkhMDAQI0aMwOnTpx3KFBUVYdq0aWjevDk8PT0xatQopKenO5RJTk7GQw89BHd3dwQGBuKFF16A2Wx2KLN3717ccccdMBqN6NChA9atW1fXX89pVq1ahV69etknPouKisK2bdvs+3kPa+7111+HJEmYOXOmfRvv460tWrQIkiQ5vLp06WLfz3uoIkG3LS4uThgMBrFmzRrxyy+/iMmTJwtfX1+Rnp6uddU0s3XrVjFv3jzx9ddfCwBi48aNDvtff/114ePjIzZt2iSOHz8uHn74YdG2bVtRWFhoLzNkyBDRu3dv8cMPP4h9+/aJDh06iDFjxtj3Z2dni6CgIDF27Fhx8uRJ8cUXXwg3Nzfx/vvvO+tr1qmYmBixdu1acfLkSXHs2DHx4IMPitatW4u8vDx7malTp4rQ0FARHx8vfvzxR3HnnXeKAQMG2PebzWbRo0cPER0dLX766SexdetW4e/vL+bOnWsv8/vvvwt3d3cxa9YscerUKbF8+XKh0+nE9u3bnfp968rmzZvFli1bxJkzZ8Tp06fFSy+9JFxcXMTJkyeFELyHNZWQkCDCwsJEr169xLPPPmvfzvt4awsXLhTdu3cXqamp9teVK1fs+3kP1cNwo4L+/fuLadOm2T9bLBbRokULERsbq2Gt6o8bw43VahXBwcHirbfesm/LysoSRqNRfPHFF0IIIU6dOiUAiMOHD9vLbNu2TUiSJC5duiSEEOJf//qX8PPzE8XFxfYyc+bMEZ07d67jb6SNjIwMAUB8++23Qgjlnrm4uIgNGzbYyyQmJgoA4uDBg0IIJWTKsizS0tLsZVatWiW8vb3t9+3vf/+76N69u8O1Ro8eLWJiYur6K2nGz89PfPTRR7yHNZSbmys6duwodu3aJe699157uOF9rJ6FCxeK3r17V7qP91Bd7Ja6TSaTCUeOHEF0dLR9myzLiI6OxsGDBzWsWf11/vx5pKWlOdwzHx8fREZG2u/ZwYMH4evri4iICHuZ6OhoyLKMQ4cO2cvcc889MBgM9jIxMTE4ffo0rl+/7qRv4zzZ2dkAgGbNmgEAjhw5gpKSEof72KVLF7Ru3drhPvbs2RNBQUH2MjExMcjJycEvv/xiL1P+HLYyjfH312KxIC4uDvn5+YiKiuI9rKFp06bhoYceqvBdeR+r7+zZs2jRogXatWuHsWPHIjk5GQDvodoYbm7T1atXYbFYHH7ZACAoKAhpaWka1ap+s92Xqu5ZWloaAgMDHfbr9Xo0a9bMoUxl5yh/jcbCarVi5syZGDhwIHr06AFA+Y4GgwG+vr4OZW+8j7e6Rzcrk5OTg8LCwrr4Ok534sQJeHp6wmg0YurUqdi4cSO6devGe1gDcXFxOHr0KGJjYyvs432snsjISKxbtw7bt2/HqlWrcP78edx9993Izc3lPVRZk1sVnKghmjZtGk6ePInvv/9e66o0SJ07d8axY8eQnZ2Nr776ChMmTMC3336rdbUajJSUFDz77LPYtWsXXF1dta5OgzV06FD7z7169UJkZCTatGmDL7/8Em5ubhrWrPFhy81t8vf3h06nqzCiPT09HcHBwRrVqn6z3Zeq7llwcDAyMjIc9pvNZmRmZjqUqewc5a/RGEyfPh3/+9//sGfPHrRq1cq+PTg4GCaTCVlZWQ7lb7yPt7pHNyvj7e3daP6DazAY0KFDB4SHhyM2Nha9e/fGu+++y3tYTUeOHEFGRgbuuOMO6PV66PV6fPvtt3jvvfeg1+sRFBTE+1gLvr6+6NSpE86dO8ffRZUx3Nwmg8GA8PBwxMfH27dZrVbEx8cjKipKw5rVX23btkVwcLDDPcvJycGhQ4fs9ywqKgpZWVk4cuSIvcw333wDq9WKyMhIe5nvvvsOJSUl9jK7du1C586d4efn56RvU3eEEJg+fTo2btyIb775Bm3btnXYHx4eDhcXF4f7ePr0aSQnJzvcxxMnTjgExV27dsHb2xvdunWzlyl/DluZxvz7a7VaUVxczHtYTYMHD8aJEydw7Ngx+ysiIgJjx461/8z7WHN5eXn47bffEBISwt9FtWk9orkxiIuLE0ajUaxbt06cOnVKTJkyRfj6+jqMaG9qcnNzxU8//SR++uknAUAsXbpU/PTTTyIpKUkIoTwK7uvrK/773/+Kn3/+WQwfPrzSR8H79u0rDh06JL7//nvRsWNHh0fBs7KyRFBQkBg3bpw4efKkiIuLE+7u7o3mUfCnn35a+Pj4iL179zo8OlpQUGAvM3XqVNG6dWvxzTffiB9//FFERUWJqKgo+37bo6MPPPCAOHbsmNi+fbsICAio9NHRF154QSQmJoqVK1c2qkdHX3zxRfHtt9+K8+fPi59//lm8+OKLQpIksXPnTiEE72FtlX9aSgjex+p4/vnnxd69e8X58+fF/v37RXR0tPD39xcZGRlCCN5DNTHcqGT58uWidevWwmAwiP79+4sffvhB6yppas+ePQJAhdeECROEEMrj4PPnzxdBQUHCaDSKwYMHi9OnTzuc49q1a2LMmDHC09NTeHt7i4kTJ4rc3FyHMsePHxd33XWXMBqNomXLluL111931lesc5XdPwBi7dq19jKFhYXimWeeEX5+fsLd3V2MHDlSpKamOpznwoULYujQocLNzU34+/uL559/XpSUlDiU2bNnj+jTp48wGAyiXbt2Dtdo6J588knRpk0bYTAYREBAgBg8eLA92AjBe1hbN4Yb3sdbGz16tAgJCREGg0G0bNlSjB49Wpw7d86+n/dQPZIQQmjTZkRERESkPo65ISIiokaF4YaIiIgaFYYbIiIialQYboiIiKhRYbghIiKiRoXhhoiIiBoVhhsiIiJqVBhuiIiIqFFhuCGiJk+SJGzatEnrahCRShhuiEhTTzzxBCRJqvAaMmSI1lUjogZKr3UFiIiGDBmCtWvXOmwzGo0a1YaIGjq23BCR5oxGI4KDgx1efn5+AJQuo1WrVmHo0KFwc3NDu3bt8NVXXzkcf+LECfzhD3+Am5sbmjdvjilTpiAvL8+hzJo1a9C9e3cYjUaEhIRg+vTpDvuvXr2KkSNHwt3dHR07dsTmzZvr9ksTUZ1huCGiem/+/PkYNWoUjh8/jrFjx+LPf/4zEhMTAQD5+fmIiYmBn58fDh8+jA0bNmD37t0O4WXVqlWYNm0apkyZghMnTmDz5s3o0KGDwzVeeeUVPPbYY/j555/x4IMPYuzYscjMzHTq9yQilWi9LDkRNW0TJkwQOp1OeHh4OLxee+01IYQQAMTUqVMdjomMjBRPP/20EEKIDz74QPj5+Ym8vDz7/i1btghZlkVaWpoQQogWLVqIefPm3bQOAMTLL79s/5yXlycAiG3btqn2PYnIeTjmhog0d99992HVqlUO25o1a2b/OSoqymFfVFQUjh07BgBITExE79694eHhYd8/cOBAWK1WnD59GpIk4fLlyxg8eHCVdejVq5f9Zw8PD3h7eyMjI6O2X4mINMRwQ0Sa8/DwqNBNpBY3N7dqlXNxcXH4LEkSrFZrXVSJiOoYx9wQUb33ww8/VPjctWtXAEDXrl1x/Phx5Ofn2/fv378fsiyjc+fO8PLyQlhYGOLj451aZyLSDltuiEhzxcXFSEtLc9im1+vh7+8PANiwYQMiIiJw11134bPPPkNCQgI+/vhjAMDYsWOxcOFCTJgwAYsWLcKVK1cwY8YMjBs3DkFBQQCARYsWYerUqQgMDMTQoUORm5uL/fv3Y8aMGc79okTkFAw3RKS57du3IyQkxGFb586d8euvvwJQnmSKi4vDM888g5CQEHzxxRfo1q0bAMDd3R07duzAs88+i379+sHd3R2jRo3C0qVL7eeaMGECioqK8M9//hOzZ8+Gv78/Hn30Ued9QSJyKkkIIbSuBBHRzUiShI0bN2LEiBFaV4WIGgiOuSEiIqJGheGGiIiIGhWOuSGieo0950RUU2y5ISIiokaF4YaIiIgaFYYbIiIialQYboiIiKhRYbghIiKiRoXhhoiIiBoVhhsiIiJqVBhuiIiIqFH5/yafhIwpyg25AAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Difference:\n",
      "[-3094176.0000, -1082880.0000, -391264.0000, 3800224.0000, -32880.0000, -2003492.0000, 4617392.0000, 91136.0000, 5744864.0000, -49973664.0000,\n",
      " 216720.0000, 4086560.0000, 3167200.0000, 1518336.0000, 1509056.0000, 4392704.0000, 2822064.0000, 165808.0000, 2925008.0000, 186208.0000,\n",
      " 3937936.0000, 208304.0000, -929472.0000, 1507552.0000, 202064.0000, -3715648.0000, -374864.0000, -4099440.0000, -1513824.0000, 1808016.0000,\n",
      " 2041024.0000, -2278496.0000, -711424.0000, -271328.0000, -1082016.0000, -2592368.0000, -1698368.0000, -593280.0000, 2101376.0000]\n",
      "\n",
      "E_test_denorm:\n",
      "[105000000.0000, 210000000.0000, 210000000.0000, 210000000.0000, 210000000.0000, 52500000.0000, 210000000.0000, 210000000.0000, 210000000.0000, 105000000.0000,\n",
      " 210000000.0000, 210000000.0000, 210000000.0000, 210000000.0000, 210000000.0000, 210000000.0000, 210000000.0000, 210000000.0000, 210000000.0000, 210000000.0000,\n",
      " 210000000.0000, 210000000.0000, 210000000.0000, 210000000.0000, 210000000.0000, 210000000.0000, 210000000.0000, 210000000.0000, 210000000.0000, 210000000.0000,\n",
      " 105000000.0000, 210000000.0000, 210000000.0000, 210000000.0000, 210000000.0000, 210000000.0000, 210000000.0000, 210000000.0000, 210000000.0000]\n",
      "\n",
      "Pred_test_denorm:\n",
      "[108094176.0000, 211082880.0000, 210391264.0000, 206199776.0000, 210032880.0000, 54503492.0000, 205382608.0000, 209908864.0000, 204255136.0000, 154973664.0000,\n",
      " 209783280.0000, 205913440.0000, 206832800.0000, 208481664.0000, 208490944.0000, 205607296.0000, 207177936.0000, 209834192.0000, 207074992.0000, 209813792.0000,\n",
      " 206062064.0000, 209791696.0000, 210929472.0000, 208492448.0000, 209797936.0000, 213715648.0000, 210374864.0000, 214099440.0000, 211513824.0000, 208191984.0000,\n",
      " 102958976.0000, 212278496.0000, 210711424.0000, 210271328.0000, 211082016.0000, 212592368.0000, 211698368.0000, 210593280.0000, 207898624.0000]\n",
      "\n",
      "\n",
      "MAPE: 2.4321%\n",
      "MdAPE: 1.2539847753942013%\n",
      "Best epoch: 5504\n"
     ]
    }
   ],
   "source": [
    "class NormalizedCombinedRegressionLoss_original(nn.Module):\n",
    "    def __init__(self, alpha=0.4, beta=0.4, gamma=0.2):\n",
    "        super().__init__()\n",
    "        self.alpha = alpha\n",
    "        self.beta = beta\n",
    "        self.gamma = gamma\n",
    "        self.huber_loss = nn.HuberLoss(delta=1.0)\n",
    "        self.mse_loss = nn.MSELoss()\n",
    "        self.mae_loss = nn.L1Loss()\n",
    "\n",
    "    def forward(self, pred, target):\n",
    "        huber = self.huber_loss(pred, target)\n",
    "        mse = self.mse_loss(pred, target)\n",
    "        mae = self.mae_loss(pred, target)\n",
    "\n",
    "        # Scale losses based on their theoretical properties\n",
    "        # Huber loss is already balanced, so we keep it as is\n",
    "        # MSE is quadratic, so we take its square root to make it comparable\n",
    "        # MAE is linear, so we keep it as is\n",
    "        scaled_mse = torch.sqrt(mse)\n",
    "\n",
    "        return self.alpha * huber + self.beta * scaled_mse + self.gamma * mae\n",
    "\n",
    "\n",
    "# Set the seed for reproducibility\n",
    "seed = 42\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "\n",
    "# Define combined loss function, optimizer and scheduler\n",
    "loss_fn = NormalizedCombinedRegressionLoss_original(alpha=0.4, beta=0.4, gamma=0.2)\n",
    "optimizer = optim.AdamW(model.parameters(), lr=1e-4, weight_decay=0.01)\n",
    "scheduler = ReduceLROnPlateau(optimizer, 'min', patience=15, factor=0.99)\n",
    "\n",
    "# Move the model to the device\n",
    "model = model.to(device)\n",
    "E_test = E_test.to(device)\n",
    "\n",
    "# Initialize variables for tracking\n",
    "best_model = None\n",
    "best_val_loss = float('inf')\n",
    "best_epoch = 0\n",
    "epochs_without_improvement = 0\n",
    "patience = 250  # for early stopping\n",
    "\n",
    "# For plotting\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "\n",
    "start_time = time.time()\n",
    "try:\n",
    "    for epoch in range(epochs_dl):\n",
    "        # Training\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        pred_train = model(x_categ[:x_cont_train.shape[0]], x_cont_train)\n",
    "        loss = loss_fn(pred_train, E_train)\n",
    "        loss.backward()\n",
    "        nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        optimizer.step()\n",
    "\n",
    "        # Validation\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            pred_val = model(x_categ[:x_cont_test.shape[0]], x_cont_test)\n",
    "            val_loss = loss_fn(pred_val, E_test)\n",
    "\n",
    "        # Update scheduler\n",
    "        scheduler.step(val_loss)\n",
    "\n",
    "        # Check if validation loss has improved\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            best_epoch = epoch\n",
    "            best_model = copy.deepcopy(model.state_dict())\n",
    "            epochs_without_improvement = 0\n",
    "        else:\n",
    "            epochs_without_improvement += 1\n",
    "\n",
    "        # Early stopping\n",
    "        if epochs_without_improvement >= patience:\n",
    "            print(f'Early stopping at epoch {epoch} due to no improvement in validation loss.')\n",
    "            break\n",
    "\n",
    "        # Store losses for plotting\n",
    "        train_losses.append(loss.item())\n",
    "        val_losses.append(val_loss.item())\n",
    "\n",
    "        # Clear cache to free memory\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "        # Print progress\n",
    "        if epoch % 10 == 0:\n",
    "            print(f'Epoch {epoch}, Train Loss: {loss.item():.4f}, Val Loss: {val_loss.item():.4f}')\n",
    "            print(f'Learning rate: {optimizer.param_groups[0][\"lr\"]:.6f}')\n",
    "except KeyboardInterrupt:\n",
    "    print(\"Training interrupted by user.\")\n",
    "    \n",
    "finally:\n",
    "    print(f\"Time taken: {time.time() - start_time}\")\n",
    "    print(f'Best epoch: {best_epoch}, Best validation loss: {best_val_loss:.4f}')\n",
    "    \n",
    "    # Load the best model\n",
    "    model.load_state_dict(best_model)\n",
    "    \n",
    "    # Plot the loss values of the training set and the validation set\n",
    "    plt.plot(train_losses, label='Training Loss')\n",
    "    plt.plot(val_losses, label='Validation Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    # Test the model\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        pred_test = model(x_categ[:x_cont_test.shape[0]], x_cont_test)\n",
    "    \n",
    "    # Move the predictions back to the CPU for further processing\n",
    "    pred_test = pred_test.to(\"cpu\")\n",
    "    E_test = E_test.to(\"cpu\")\n",
    "    # Denormalize the E_tensor\n",
    "    E_test_denorm = denormalize_data(E_test, E_return, E_return2, scaling_type)\n",
    "    E_test_denorm_np = E_test_denorm.numpy()\n",
    "    # Denormalize the predictions\n",
    "    pred_test_denorm = denormalize_data(pred_test, E_return, E_return2, scaling_type)\n",
    "    pred_test_denorm_np = pred_test_denorm.numpy()\n",
    "    \n",
    "    # MAPE\n",
    "    mape = np.mean(np.abs((E_test_denorm_np - pred_test_denorm_np) / np.abs(E_test_denorm_np))) * 100\n",
    "    # MdAPE\n",
    "    mdape = np.median(np.abs((E_test_denorm_np - pred_test_denorm_np) / np.abs(E_test_denorm_np))) * 100\n",
    "    \n",
    "    # Calculate the difference\n",
    "    difference = E_test_denorm_np - pred_test_denorm_np\n",
    "    \n",
    "    \n",
    "    sample = random.randint(0,len(pred_test_denorm))\n",
    "    print(\"Difference:\\n\" + format_array(difference[sample], 4))\n",
    "    print(\"\\nE_test_denorm:\\n\" + format_array(E_test_denorm_np[sample], 4))\n",
    "    print(\"\\nPred_test_denorm:\\n\" + format_array(pred_test_denorm_np[sample], 4))\n",
    "    print(f\"\\n\\nMAPE: {mape.item():.4f}%\")\n",
    "    print(f'MdAPE: {mdape}%')\n",
    "    print(f'Best epoch: {best_epoch}')\n",
    "\n",
    "    model_name = 'warren_sp25_run15k_edit_normalizedcombinedregression.pth'\n",
    "    torch.save(model.state_dict(), model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-08T15:08:30.145357Z",
     "iopub.status.busy": "2025-02-08T15:08:30.144990Z",
     "iopub.status.idle": "2025-02-08T17:53:52.289494Z",
     "shell.execute_reply": "2025-02-08T17:53:52.288472Z",
     "shell.execute_reply.started": "2025-02-08T15:08:30.145326Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Train Loss: 0.6555, Val Loss: 0.5839\n",
      "Learning rate: 0.000100\n",
      "Epoch 10, Train Loss: 0.3296, Val Loss: 0.5016\n",
      "Learning rate: 0.000100\n",
      "Epoch 20, Train Loss: 0.1414, Val Loss: 0.3512\n",
      "Learning rate: 0.000100\n",
      "Epoch 30, Train Loss: 0.1228, Val Loss: 0.2481\n",
      "Learning rate: 0.000100\n",
      "Epoch 40, Train Loss: 0.1055, Val Loss: 0.2464\n",
      "Learning rate: 0.000100\n",
      "Epoch 50, Train Loss: 0.0988, Val Loss: 0.2744\n",
      "Learning rate: 0.000100\n",
      "Epoch 60, Train Loss: 0.0935, Val Loss: 0.2391\n",
      "Learning rate: 0.000100\n",
      "Epoch 70, Train Loss: 0.0887, Val Loss: 0.1913\n",
      "Learning rate: 0.000100\n",
      "Epoch 80, Train Loss: 0.0848, Val Loss: 0.1461\n",
      "Learning rate: 0.000100\n",
      "Epoch 90, Train Loss: 0.0816, Val Loss: 0.1180\n",
      "Learning rate: 0.000100\n",
      "Epoch 100, Train Loss: 0.0787, Val Loss: 0.1061\n",
      "Learning rate: 0.000100\n",
      "Epoch 110, Train Loss: 0.0762, Val Loss: 0.0929\n",
      "Learning rate: 0.000100\n",
      "Epoch 120, Train Loss: 0.0738, Val Loss: 0.0894\n",
      "Learning rate: 0.000100\n",
      "Epoch 130, Train Loss: 0.0717, Val Loss: 0.0911\n",
      "Learning rate: 0.000100\n",
      "Epoch 140, Train Loss: 0.0696, Val Loss: 0.0867\n",
      "Learning rate: 0.000100\n",
      "Epoch 150, Train Loss: 0.0677, Val Loss: 0.0922\n",
      "Learning rate: 0.000100\n",
      "Epoch 160, Train Loss: 0.0660, Val Loss: 0.0921\n",
      "Learning rate: 0.000099\n",
      "Epoch 170, Train Loss: 0.0645, Val Loss: 0.0923\n",
      "Learning rate: 0.000099\n",
      "Epoch 180, Train Loss: 0.0631, Val Loss: 0.0876\n",
      "Learning rate: 0.000099\n",
      "Epoch 190, Train Loss: 0.0617, Val Loss: 0.0834\n",
      "Learning rate: 0.000099\n",
      "Epoch 200, Train Loss: 0.0606, Val Loss: 0.0921\n",
      "Learning rate: 0.000099\n",
      "Epoch 210, Train Loss: 0.0594, Val Loss: 0.0807\n",
      "Learning rate: 0.000098\n",
      "Epoch 220, Train Loss: 0.0583, Val Loss: 0.0846\n",
      "Learning rate: 0.000098\n",
      "Epoch 230, Train Loss: 0.0572, Val Loss: 0.0790\n",
      "Learning rate: 0.000098\n",
      "Epoch 240, Train Loss: 0.0563, Val Loss: 0.0893\n",
      "Learning rate: 0.000098\n",
      "Epoch 250, Train Loss: 0.0554, Val Loss: 0.0881\n",
      "Learning rate: 0.000097\n",
      "Epoch 260, Train Loss: 0.0544, Val Loss: 0.0844\n",
      "Learning rate: 0.000097\n",
      "Epoch 270, Train Loss: 0.0535, Val Loss: 0.0787\n",
      "Learning rate: 0.000097\n",
      "Epoch 280, Train Loss: 0.0528, Val Loss: 0.0874\n",
      "Learning rate: 0.000097\n",
      "Epoch 290, Train Loss: 0.0521, Val Loss: 0.0801\n",
      "Learning rate: 0.000096\n",
      "Epoch 300, Train Loss: 0.0513, Val Loss: 0.0869\n",
      "Learning rate: 0.000096\n",
      "Epoch 310, Train Loss: 0.0507, Val Loss: 0.0759\n",
      "Learning rate: 0.000096\n",
      "Epoch 320, Train Loss: 0.0499, Val Loss: 0.0786\n",
      "Learning rate: 0.000096\n",
      "Epoch 330, Train Loss: 0.0493, Val Loss: 0.0732\n",
      "Learning rate: 0.000096\n",
      "Epoch 340, Train Loss: 0.0487, Val Loss: 0.0702\n",
      "Learning rate: 0.000096\n",
      "Epoch 350, Train Loss: 0.0481, Val Loss: 0.0646\n",
      "Learning rate: 0.000096\n",
      "Epoch 360, Train Loss: 0.0477, Val Loss: 0.0734\n",
      "Learning rate: 0.000096\n",
      "Epoch 370, Train Loss: 0.0472, Val Loss: 0.0704\n",
      "Learning rate: 0.000095\n",
      "Epoch 380, Train Loss: 0.0467, Val Loss: 0.0689\n",
      "Learning rate: 0.000095\n",
      "Epoch 390, Train Loss: 0.0463, Val Loss: 0.0638\n",
      "Learning rate: 0.000095\n",
      "Epoch 400, Train Loss: 0.0458, Val Loss: 0.0596\n",
      "Learning rate: 0.000095\n",
      "Epoch 410, Train Loss: 0.0454, Val Loss: 0.0584\n",
      "Learning rate: 0.000095\n",
      "Epoch 420, Train Loss: 0.0450, Val Loss: 0.0582\n",
      "Learning rate: 0.000095\n",
      "Epoch 430, Train Loss: 0.0447, Val Loss: 0.0615\n",
      "Learning rate: 0.000095\n",
      "Epoch 440, Train Loss: 0.0443, Val Loss: 0.0713\n",
      "Learning rate: 0.000095\n",
      "Epoch 450, Train Loss: 0.0440, Val Loss: 0.0650\n",
      "Learning rate: 0.000094\n",
      "Epoch 460, Train Loss: 0.0436, Val Loss: 0.0632\n",
      "Learning rate: 0.000094\n",
      "Epoch 470, Train Loss: 0.0433, Val Loss: 0.0642\n",
      "Learning rate: 0.000094\n",
      "Epoch 480, Train Loss: 0.0430, Val Loss: 0.0564\n",
      "Learning rate: 0.000093\n",
      "Epoch 490, Train Loss: 0.0427, Val Loss: 0.0537\n",
      "Learning rate: 0.000093\n",
      "Epoch 500, Train Loss: 0.0424, Val Loss: 0.0679\n",
      "Learning rate: 0.000093\n",
      "Epoch 510, Train Loss: 0.0422, Val Loss: 0.0654\n",
      "Learning rate: 0.000093\n",
      "Epoch 520, Train Loss: 0.0419, Val Loss: 0.0552\n",
      "Learning rate: 0.000092\n",
      "Epoch 530, Train Loss: 0.0416, Val Loss: 0.0531\n",
      "Learning rate: 0.000092\n",
      "Epoch 540, Train Loss: 0.0415, Val Loss: 0.0574\n",
      "Learning rate: 0.000092\n",
      "Epoch 550, Train Loss: 0.0412, Val Loss: 0.0534\n",
      "Learning rate: 0.000091\n",
      "Epoch 560, Train Loss: 0.0409, Val Loss: 0.0503\n",
      "Learning rate: 0.000091\n",
      "Epoch 570, Train Loss: 0.0407, Val Loss: 0.0552\n",
      "Learning rate: 0.000091\n",
      "Epoch 580, Train Loss: 0.0405, Val Loss: 0.0525\n",
      "Learning rate: 0.000091\n",
      "Epoch 590, Train Loss: 0.0402, Val Loss: 0.0639\n",
      "Learning rate: 0.000091\n",
      "Epoch 600, Train Loss: 0.0400, Val Loss: 0.0549\n",
      "Learning rate: 0.000090\n",
      "Epoch 610, Train Loss: 0.0398, Val Loss: 0.0489\n",
      "Learning rate: 0.000090\n",
      "Epoch 620, Train Loss: 0.0397, Val Loss: 0.0494\n",
      "Learning rate: 0.000090\n",
      "Epoch 630, Train Loss: 0.0393, Val Loss: 0.0466\n",
      "Learning rate: 0.000090\n",
      "Epoch 640, Train Loss: 0.0391, Val Loss: 0.0498\n",
      "Learning rate: 0.000090\n",
      "Epoch 650, Train Loss: 0.0390, Val Loss: 0.0466\n",
      "Learning rate: 0.000090\n",
      "Epoch 660, Train Loss: 0.0388, Val Loss: 0.0461\n",
      "Learning rate: 0.000090\n",
      "Epoch 670, Train Loss: 0.0385, Val Loss: 0.0441\n",
      "Learning rate: 0.000090\n",
      "Epoch 680, Train Loss: 0.0385, Val Loss: 0.0507\n",
      "Learning rate: 0.000090\n",
      "Epoch 690, Train Loss: 0.0382, Val Loss: 0.0460\n",
      "Learning rate: 0.000089\n",
      "Epoch 700, Train Loss: 0.0381, Val Loss: 0.0495\n",
      "Learning rate: 0.000089\n",
      "Epoch 710, Train Loss: 0.0378, Val Loss: 0.0451\n",
      "Learning rate: 0.000089\n",
      "Epoch 720, Train Loss: 0.0378, Val Loss: 0.0478\n",
      "Learning rate: 0.000089\n",
      "Epoch 730, Train Loss: 0.0375, Val Loss: 0.0496\n",
      "Learning rate: 0.000089\n",
      "Epoch 740, Train Loss: 0.0374, Val Loss: 0.0476\n",
      "Learning rate: 0.000088\n",
      "Epoch 750, Train Loss: 0.0373, Val Loss: 0.0483\n",
      "Learning rate: 0.000088\n",
      "Epoch 760, Train Loss: 0.0371, Val Loss: 0.0458\n",
      "Learning rate: 0.000088\n",
      "Epoch 770, Train Loss: 0.0369, Val Loss: 0.0469\n",
      "Learning rate: 0.000087\n",
      "Epoch 780, Train Loss: 0.0368, Val Loss: 0.0446\n",
      "Learning rate: 0.000087\n",
      "Epoch 790, Train Loss: 0.0366, Val Loss: 0.0438\n",
      "Learning rate: 0.000087\n",
      "Epoch 800, Train Loss: 0.0365, Val Loss: 0.0442\n",
      "Learning rate: 0.000087\n",
      "Epoch 810, Train Loss: 0.0363, Val Loss: 0.0444\n",
      "Learning rate: 0.000086\n",
      "Epoch 820, Train Loss: 0.0361, Val Loss: 0.0400\n",
      "Learning rate: 0.000086\n",
      "Epoch 830, Train Loss: 0.0362, Val Loss: 0.0476\n",
      "Learning rate: 0.000086\n",
      "Epoch 840, Train Loss: 0.0359, Val Loss: 0.0539\n",
      "Learning rate: 0.000086\n",
      "Epoch 850, Train Loss: 0.0358, Val Loss: 0.0538\n",
      "Learning rate: 0.000086\n",
      "Epoch 860, Train Loss: 0.0359, Val Loss: 0.0514\n",
      "Learning rate: 0.000086\n",
      "Epoch 870, Train Loss: 0.0355, Val Loss: 0.0461\n",
      "Learning rate: 0.000085\n",
      "Epoch 880, Train Loss: 0.0354, Val Loss: 0.0506\n",
      "Learning rate: 0.000085\n",
      "Epoch 890, Train Loss: 0.0353, Val Loss: 0.0478\n",
      "Learning rate: 0.000085\n",
      "Epoch 900, Train Loss: 0.0352, Val Loss: 0.0404\n",
      "Learning rate: 0.000084\n",
      "Epoch 910, Train Loss: 0.0350, Val Loss: 0.0441\n",
      "Learning rate: 0.000084\n",
      "Epoch 920, Train Loss: 0.0350, Val Loss: 0.0453\n",
      "Learning rate: 0.000084\n",
      "Epoch 930, Train Loss: 0.0348, Val Loss: 0.0429\n",
      "Learning rate: 0.000083\n",
      "Epoch 940, Train Loss: 0.0347, Val Loss: 0.0391\n",
      "Learning rate: 0.000083\n",
      "Epoch 950, Train Loss: 0.0346, Val Loss: 0.0417\n",
      "Learning rate: 0.000083\n",
      "Epoch 960, Train Loss: 0.0345, Val Loss: 0.0433\n",
      "Learning rate: 0.000083\n",
      "Epoch 970, Train Loss: 0.0344, Val Loss: 0.0398\n",
      "Learning rate: 0.000083\n",
      "Epoch 980, Train Loss: 0.0343, Val Loss: 0.0485\n",
      "Learning rate: 0.000083\n",
      "Epoch 990, Train Loss: 0.0341, Val Loss: 0.0418\n",
      "Learning rate: 0.000082\n",
      "Epoch 1000, Train Loss: 0.0340, Val Loss: 0.0421\n",
      "Learning rate: 0.000082\n",
      "Epoch 1010, Train Loss: 0.0339, Val Loss: 0.0389\n",
      "Learning rate: 0.000082\n",
      "Epoch 1020, Train Loss: 0.0340, Val Loss: 0.0443\n",
      "Learning rate: 0.000082\n",
      "Epoch 1030, Train Loss: 0.0337, Val Loss: 0.0468\n",
      "Learning rate: 0.000081\n",
      "Epoch 1040, Train Loss: 0.0336, Val Loss: 0.0437\n",
      "Learning rate: 0.000081\n",
      "Epoch 1050, Train Loss: 0.0335, Val Loss: 0.0456\n",
      "Learning rate: 0.000081\n",
      "Epoch 1060, Train Loss: 0.0334, Val Loss: 0.0403\n",
      "Learning rate: 0.000081\n",
      "Epoch 1070, Train Loss: 0.0333, Val Loss: 0.0439\n",
      "Learning rate: 0.000081\n",
      "Epoch 1080, Train Loss: 0.0332, Val Loss: 0.0445\n",
      "Learning rate: 0.000080\n",
      "Epoch 1090, Train Loss: 0.0331, Val Loss: 0.0415\n",
      "Learning rate: 0.000080\n",
      "Epoch 1100, Train Loss: 0.0330, Val Loss: 0.0466\n",
      "Learning rate: 0.000080\n",
      "Epoch 1110, Train Loss: 0.0330, Val Loss: 0.0464\n",
      "Learning rate: 0.000079\n",
      "Epoch 1120, Train Loss: 0.0328, Val Loss: 0.0416\n",
      "Learning rate: 0.000079\n",
      "Epoch 1130, Train Loss: 0.0328, Val Loss: 0.0393\n",
      "Learning rate: 0.000079\n",
      "Epoch 1140, Train Loss: 0.0327, Val Loss: 0.0453\n",
      "Learning rate: 0.000079\n",
      "Epoch 1150, Train Loss: 0.0326, Val Loss: 0.0397\n",
      "Learning rate: 0.000079\n",
      "Epoch 1160, Train Loss: 0.0324, Val Loss: 0.0411\n",
      "Learning rate: 0.000078\n",
      "Epoch 1170, Train Loss: 0.0323, Val Loss: 0.0351\n",
      "Learning rate: 0.000078\n",
      "Epoch 1180, Train Loss: 0.0323, Val Loss: 0.0406\n",
      "Learning rate: 0.000078\n",
      "Epoch 1190, Train Loss: 0.0322, Val Loss: 0.0414\n",
      "Learning rate: 0.000078\n",
      "Epoch 1200, Train Loss: 0.0321, Val Loss: 0.0405\n",
      "Learning rate: 0.000078\n",
      "Epoch 1210, Train Loss: 0.0320, Val Loss: 0.0403\n",
      "Learning rate: 0.000077\n",
      "Epoch 1220, Train Loss: 0.0320, Val Loss: 0.0394\n",
      "Learning rate: 0.000077\n",
      "Epoch 1230, Train Loss: 0.0319, Val Loss: 0.0406\n",
      "Learning rate: 0.000077\n",
      "Epoch 1240, Train Loss: 0.0317, Val Loss: 0.0429\n",
      "Learning rate: 0.000077\n",
      "Epoch 1250, Train Loss: 0.0316, Val Loss: 0.0416\n",
      "Learning rate: 0.000076\n",
      "Epoch 1260, Train Loss: 0.0316, Val Loss: 0.0412\n",
      "Learning rate: 0.000076\n",
      "Epoch 1270, Train Loss: 0.0315, Val Loss: 0.0436\n",
      "Learning rate: 0.000076\n",
      "Epoch 1280, Train Loss: 0.0314, Val Loss: 0.0402\n",
      "Learning rate: 0.000076\n",
      "Epoch 1290, Train Loss: 0.0313, Val Loss: 0.0378\n",
      "Learning rate: 0.000076\n",
      "Epoch 1300, Train Loss: 0.0312, Val Loss: 0.0421\n",
      "Learning rate: 0.000075\n",
      "Epoch 1310, Train Loss: 0.0311, Val Loss: 0.0359\n",
      "Learning rate: 0.000075\n",
      "Epoch 1320, Train Loss: 0.0311, Val Loss: 0.0403\n",
      "Learning rate: 0.000075\n",
      "Epoch 1330, Train Loss: 0.0310, Val Loss: 0.0397\n",
      "Learning rate: 0.000074\n",
      "Epoch 1340, Train Loss: 0.0309, Val Loss: 0.0439\n",
      "Learning rate: 0.000074\n",
      "Epoch 1350, Train Loss: 0.0309, Val Loss: 0.0397\n",
      "Learning rate: 0.000074\n",
      "Epoch 1360, Train Loss: 0.0308, Val Loss: 0.0419\n",
      "Learning rate: 0.000074\n",
      "Epoch 1370, Train Loss: 0.0307, Val Loss: 0.0461\n",
      "Learning rate: 0.000074\n",
      "Epoch 1380, Train Loss: 0.0306, Val Loss: 0.0403\n",
      "Learning rate: 0.000073\n",
      "Epoch 1390, Train Loss: 0.0306, Val Loss: 0.0364\n",
      "Learning rate: 0.000073\n",
      "Epoch 1400, Train Loss: 0.0305, Val Loss: 0.0442\n",
      "Learning rate: 0.000073\n",
      "Epoch 1410, Train Loss: 0.0305, Val Loss: 0.0429\n",
      "Learning rate: 0.000073\n",
      "Epoch 1420, Train Loss: 0.0304, Val Loss: 0.0436\n",
      "Learning rate: 0.000073\n",
      "Epoch 1430, Train Loss: 0.0303, Val Loss: 0.0397\n",
      "Learning rate: 0.000073\n",
      "Epoch 1440, Train Loss: 0.0302, Val Loss: 0.0422\n",
      "Learning rate: 0.000072\n",
      "Epoch 1450, Train Loss: 0.0302, Val Loss: 0.0421\n",
      "Learning rate: 0.000072\n",
      "Epoch 1460, Train Loss: 0.0301, Val Loss: 0.0354\n",
      "Learning rate: 0.000072\n",
      "Epoch 1470, Train Loss: 0.0300, Val Loss: 0.0419\n",
      "Learning rate: 0.000071\n",
      "Epoch 1480, Train Loss: 0.0299, Val Loss: 0.0380\n",
      "Learning rate: 0.000071\n",
      "Epoch 1490, Train Loss: 0.0299, Val Loss: 0.0422\n",
      "Learning rate: 0.000071\n",
      "Epoch 1500, Train Loss: 0.0297, Val Loss: 0.0380\n",
      "Learning rate: 0.000071\n",
      "Epoch 1510, Train Loss: 0.0297, Val Loss: 0.0338\n",
      "Learning rate: 0.000071\n",
      "Epoch 1520, Train Loss: 0.0296, Val Loss: 0.0375\n",
      "Learning rate: 0.000071\n",
      "Epoch 1530, Train Loss: 0.0296, Val Loss: 0.0392\n",
      "Learning rate: 0.000070\n",
      "Epoch 1540, Train Loss: 0.0294, Val Loss: 0.0364\n",
      "Learning rate: 0.000070\n",
      "Epoch 1550, Train Loss: 0.0294, Val Loss: 0.0354\n",
      "Learning rate: 0.000070\n",
      "Epoch 1560, Train Loss: 0.0293, Val Loss: 0.0381\n",
      "Learning rate: 0.000070\n",
      "Epoch 1570, Train Loss: 0.0292, Val Loss: 0.0362\n",
      "Learning rate: 0.000070\n",
      "Epoch 1580, Train Loss: 0.0291, Val Loss: 0.0340\n",
      "Learning rate: 0.000069\n",
      "Epoch 1590, Train Loss: 0.0291, Val Loss: 0.0326\n",
      "Learning rate: 0.000069\n",
      "Epoch 1600, Train Loss: 0.0290, Val Loss: 0.0354\n",
      "Learning rate: 0.000069\n",
      "Epoch 1610, Train Loss: 0.0290, Val Loss: 0.0400\n",
      "Learning rate: 0.000069\n",
      "Epoch 1620, Train Loss: 0.0289, Val Loss: 0.0399\n",
      "Learning rate: 0.000069\n",
      "Epoch 1630, Train Loss: 0.0288, Val Loss: 0.0349\n",
      "Learning rate: 0.000068\n",
      "Epoch 1640, Train Loss: 0.0288, Val Loss: 0.0350\n",
      "Learning rate: 0.000068\n",
      "Epoch 1650, Train Loss: 0.0287, Val Loss: 0.0368\n",
      "Learning rate: 0.000068\n",
      "Epoch 1660, Train Loss: 0.0286, Val Loss: 0.0333\n",
      "Learning rate: 0.000068\n",
      "Epoch 1670, Train Loss: 0.0285, Val Loss: 0.0338\n",
      "Learning rate: 0.000068\n",
      "Epoch 1680, Train Loss: 0.0285, Val Loss: 0.0390\n",
      "Learning rate: 0.000067\n",
      "Epoch 1690, Train Loss: 0.0284, Val Loss: 0.0386\n",
      "Learning rate: 0.000067\n",
      "Epoch 1700, Train Loss: 0.0284, Val Loss: 0.0338\n",
      "Learning rate: 0.000067\n",
      "Epoch 1710, Train Loss: 0.0283, Val Loss: 0.0422\n",
      "Learning rate: 0.000067\n",
      "Epoch 1720, Train Loss: 0.0282, Val Loss: 0.0340\n",
      "Learning rate: 0.000067\n",
      "Epoch 1730, Train Loss: 0.0281, Val Loss: 0.0346\n",
      "Learning rate: 0.000066\n",
      "Epoch 1740, Train Loss: 0.0281, Val Loss: 0.0386\n",
      "Learning rate: 0.000066\n",
      "Epoch 1750, Train Loss: 0.0281, Val Loss: 0.0385\n",
      "Learning rate: 0.000066\n",
      "Epoch 1760, Train Loss: 0.0280, Val Loss: 0.0343\n",
      "Learning rate: 0.000066\n",
      "Epoch 1770, Train Loss: 0.0279, Val Loss: 0.0348\n",
      "Learning rate: 0.000066\n",
      "Epoch 1780, Train Loss: 0.0279, Val Loss: 0.0320\n",
      "Learning rate: 0.000065\n",
      "Epoch 1790, Train Loss: 0.0278, Val Loss: 0.0359\n",
      "Learning rate: 0.000065\n",
      "Epoch 1800, Train Loss: 0.0279, Val Loss: 0.0347\n",
      "Learning rate: 0.000065\n",
      "Epoch 1810, Train Loss: 0.0277, Val Loss: 0.0404\n",
      "Learning rate: 0.000065\n",
      "Epoch 1820, Train Loss: 0.0276, Val Loss: 0.0313\n",
      "Learning rate: 0.000065\n",
      "Epoch 1830, Train Loss: 0.0276, Val Loss: 0.0333\n",
      "Learning rate: 0.000065\n",
      "Epoch 1840, Train Loss: 0.0275, Val Loss: 0.0319\n",
      "Learning rate: 0.000064\n",
      "Epoch 1850, Train Loss: 0.0275, Val Loss: 0.0364\n",
      "Learning rate: 0.000064\n",
      "Epoch 1860, Train Loss: 0.0274, Val Loss: 0.0348\n",
      "Learning rate: 0.000064\n",
      "Epoch 1870, Train Loss: 0.0273, Val Loss: 0.0341\n",
      "Learning rate: 0.000064\n",
      "Epoch 1880, Train Loss: 0.0273, Val Loss: 0.0373\n",
      "Learning rate: 0.000064\n",
      "Epoch 1890, Train Loss: 0.0272, Val Loss: 0.0338\n",
      "Learning rate: 0.000063\n",
      "Epoch 1900, Train Loss: 0.0272, Val Loss: 0.0342\n",
      "Learning rate: 0.000063\n",
      "Epoch 1910, Train Loss: 0.0271, Val Loss: 0.0322\n",
      "Learning rate: 0.000063\n",
      "Epoch 1920, Train Loss: 0.0271, Val Loss: 0.0343\n",
      "Learning rate: 0.000063\n",
      "Epoch 1930, Train Loss: 0.0270, Val Loss: 0.0364\n",
      "Learning rate: 0.000063\n",
      "Epoch 1940, Train Loss: 0.0270, Val Loss: 0.0375\n",
      "Learning rate: 0.000062\n",
      "Epoch 1950, Train Loss: 0.0269, Val Loss: 0.0365\n",
      "Learning rate: 0.000062\n",
      "Epoch 1960, Train Loss: 0.0268, Val Loss: 0.0320\n",
      "Learning rate: 0.000062\n",
      "Epoch 1970, Train Loss: 0.0267, Val Loss: 0.0291\n",
      "Learning rate: 0.000062\n",
      "Epoch 1980, Train Loss: 0.0267, Val Loss: 0.0323\n",
      "Learning rate: 0.000062\n",
      "Epoch 1990, Train Loss: 0.0267, Val Loss: 0.0344\n",
      "Learning rate: 0.000062\n",
      "Epoch 2000, Train Loss: 0.0266, Val Loss: 0.0294\n",
      "Learning rate: 0.000061\n",
      "Epoch 2010, Train Loss: 0.0265, Val Loss: 0.0316\n",
      "Learning rate: 0.000061\n",
      "Epoch 2020, Train Loss: 0.0265, Val Loss: 0.0341\n",
      "Learning rate: 0.000061\n",
      "Epoch 2030, Train Loss: 0.0265, Val Loss: 0.0367\n",
      "Learning rate: 0.000061\n",
      "Epoch 2040, Train Loss: 0.0265, Val Loss: 0.0364\n",
      "Learning rate: 0.000061\n",
      "Epoch 2050, Train Loss: 0.0264, Val Loss: 0.0374\n",
      "Learning rate: 0.000061\n",
      "Epoch 2060, Train Loss: 0.0263, Val Loss: 0.0328\n",
      "Learning rate: 0.000061\n",
      "Epoch 2070, Train Loss: 0.0262, Val Loss: 0.0359\n",
      "Learning rate: 0.000060\n",
      "Epoch 2080, Train Loss: 0.0262, Val Loss: 0.0308\n",
      "Learning rate: 0.000060\n",
      "Epoch 2090, Train Loss: 0.0261, Val Loss: 0.0318\n",
      "Learning rate: 0.000060\n",
      "Epoch 2100, Train Loss: 0.0261, Val Loss: 0.0295\n",
      "Learning rate: 0.000060\n",
      "Epoch 2110, Train Loss: 0.0260, Val Loss: 0.0289\n",
      "Learning rate: 0.000059\n",
      "Epoch 2120, Train Loss: 0.0260, Val Loss: 0.0335\n",
      "Learning rate: 0.000059\n",
      "Epoch 2130, Train Loss: 0.0259, Val Loss: 0.0331\n",
      "Learning rate: 0.000059\n",
      "Epoch 2140, Train Loss: 0.0259, Val Loss: 0.0297\n",
      "Learning rate: 0.000059\n",
      "Epoch 2150, Train Loss: 0.0258, Val Loss: 0.0321\n",
      "Learning rate: 0.000059\n",
      "Epoch 2160, Train Loss: 0.0258, Val Loss: 0.0300\n",
      "Learning rate: 0.000058\n",
      "Epoch 2170, Train Loss: 0.0257, Val Loss: 0.0311\n",
      "Learning rate: 0.000058\n",
      "Epoch 2180, Train Loss: 0.0257, Val Loss: 0.0318\n",
      "Learning rate: 0.000058\n",
      "Epoch 2190, Train Loss: 0.0257, Val Loss: 0.0298\n",
      "Learning rate: 0.000058\n",
      "Epoch 2200, Train Loss: 0.0256, Val Loss: 0.0309\n",
      "Learning rate: 0.000058\n",
      "Epoch 2210, Train Loss: 0.0255, Val Loss: 0.0314\n",
      "Learning rate: 0.000058\n",
      "Epoch 2220, Train Loss: 0.0255, Val Loss: 0.0345\n",
      "Learning rate: 0.000058\n",
      "Epoch 2230, Train Loss: 0.0254, Val Loss: 0.0304\n",
      "Learning rate: 0.000057\n",
      "Epoch 2240, Train Loss: 0.0254, Val Loss: 0.0303\n",
      "Learning rate: 0.000057\n",
      "Epoch 2250, Train Loss: 0.0253, Val Loss: 0.0269\n",
      "Learning rate: 0.000057\n",
      "Epoch 2260, Train Loss: 0.0253, Val Loss: 0.0296\n",
      "Learning rate: 0.000057\n",
      "Epoch 2270, Train Loss: 0.0252, Val Loss: 0.0335\n",
      "Learning rate: 0.000057\n",
      "Epoch 2280, Train Loss: 0.0252, Val Loss: 0.0302\n",
      "Learning rate: 0.000057\n",
      "Epoch 2290, Train Loss: 0.0251, Val Loss: 0.0278\n",
      "Learning rate: 0.000056\n",
      "Epoch 2300, Train Loss: 0.0251, Val Loss: 0.0315\n",
      "Learning rate: 0.000056\n",
      "Epoch 2310, Train Loss: 0.0251, Val Loss: 0.0319\n",
      "Learning rate: 0.000056\n",
      "Epoch 2320, Train Loss: 0.0250, Val Loss: 0.0336\n",
      "Learning rate: 0.000056\n",
      "Epoch 2330, Train Loss: 0.0250, Val Loss: 0.0378\n",
      "Learning rate: 0.000056\n",
      "Epoch 2340, Train Loss: 0.0249, Val Loss: 0.0316\n",
      "Learning rate: 0.000056\n",
      "Epoch 2350, Train Loss: 0.0249, Val Loss: 0.0344\n",
      "Learning rate: 0.000056\n",
      "Epoch 2360, Train Loss: 0.0249, Val Loss: 0.0309\n",
      "Learning rate: 0.000055\n",
      "Epoch 2370, Train Loss: 0.0248, Val Loss: 0.0315\n",
      "Learning rate: 0.000055\n",
      "Epoch 2380, Train Loss: 0.0248, Val Loss: 0.0289\n",
      "Learning rate: 0.000055\n",
      "Epoch 2390, Train Loss: 0.0247, Val Loss: 0.0306\n",
      "Learning rate: 0.000055\n",
      "Epoch 2400, Train Loss: 0.0247, Val Loss: 0.0334\n",
      "Learning rate: 0.000055\n",
      "Epoch 2410, Train Loss: 0.0246, Val Loss: 0.0279\n",
      "Learning rate: 0.000055\n",
      "Epoch 2420, Train Loss: 0.0246, Val Loss: 0.0286\n",
      "Learning rate: 0.000055\n",
      "Epoch 2430, Train Loss: 0.0245, Val Loss: 0.0288\n",
      "Learning rate: 0.000055\n",
      "Epoch 2440, Train Loss: 0.0244, Val Loss: 0.0275\n",
      "Learning rate: 0.000054\n",
      "Epoch 2450, Train Loss: 0.0244, Val Loss: 0.0292\n",
      "Learning rate: 0.000054\n",
      "Epoch 2460, Train Loss: 0.0244, Val Loss: 0.0309\n",
      "Learning rate: 0.000054\n",
      "Epoch 2470, Train Loss: 0.0243, Val Loss: 0.0285\n",
      "Learning rate: 0.000054\n",
      "Epoch 2480, Train Loss: 0.0243, Val Loss: 0.0287\n",
      "Learning rate: 0.000054\n",
      "Epoch 2490, Train Loss: 0.0242, Val Loss: 0.0264\n",
      "Learning rate: 0.000053\n",
      "Epoch 2500, Train Loss: 0.0242, Val Loss: 0.0300\n",
      "Learning rate: 0.000053\n",
      "Epoch 2510, Train Loss: 0.0241, Val Loss: 0.0300\n",
      "Learning rate: 0.000053\n",
      "Epoch 2520, Train Loss: 0.0241, Val Loss: 0.0304\n",
      "Learning rate: 0.000053\n",
      "Epoch 2530, Train Loss: 0.0241, Val Loss: 0.0264\n",
      "Learning rate: 0.000053\n",
      "Epoch 2540, Train Loss: 0.0240, Val Loss: 0.0283\n",
      "Learning rate: 0.000053\n",
      "Epoch 2550, Train Loss: 0.0240, Val Loss: 0.0277\n",
      "Learning rate: 0.000052\n",
      "Epoch 2560, Train Loss: 0.0239, Val Loss: 0.0264\n",
      "Learning rate: 0.000052\n",
      "Epoch 2570, Train Loss: 0.0239, Val Loss: 0.0260\n",
      "Learning rate: 0.000052\n",
      "Epoch 2580, Train Loss: 0.0238, Val Loss: 0.0264\n",
      "Learning rate: 0.000052\n",
      "Epoch 2590, Train Loss: 0.0238, Val Loss: 0.0273\n",
      "Learning rate: 0.000052\n",
      "Epoch 2600, Train Loss: 0.0238, Val Loss: 0.0255\n",
      "Learning rate: 0.000052\n",
      "Epoch 2610, Train Loss: 0.0238, Val Loss: 0.0262\n",
      "Learning rate: 0.000052\n",
      "Epoch 2620, Train Loss: 0.0237, Val Loss: 0.0293\n",
      "Learning rate: 0.000052\n",
      "Epoch 2630, Train Loss: 0.0236, Val Loss: 0.0271\n",
      "Learning rate: 0.000051\n",
      "Epoch 2640, Train Loss: 0.0236, Val Loss: 0.0258\n",
      "Learning rate: 0.000051\n",
      "Epoch 2650, Train Loss: 0.0236, Val Loss: 0.0256\n",
      "Learning rate: 0.000051\n",
      "Epoch 2660, Train Loss: 0.0235, Val Loss: 0.0271\n",
      "Learning rate: 0.000051\n",
      "Epoch 2670, Train Loss: 0.0235, Val Loss: 0.0255\n",
      "Learning rate: 0.000051\n",
      "Epoch 2680, Train Loss: 0.0234, Val Loss: 0.0249\n",
      "Learning rate: 0.000051\n",
      "Epoch 2690, Train Loss: 0.0234, Val Loss: 0.0314\n",
      "Learning rate: 0.000051\n",
      "Epoch 2700, Train Loss: 0.0233, Val Loss: 0.0267\n",
      "Learning rate: 0.000050\n",
      "Epoch 2710, Train Loss: 0.0233, Val Loss: 0.0271\n",
      "Learning rate: 0.000050\n",
      "Epoch 2720, Train Loss: 0.0233, Val Loss: 0.0253\n",
      "Learning rate: 0.000050\n",
      "Epoch 2730, Train Loss: 0.0233, Val Loss: 0.0292\n",
      "Learning rate: 0.000050\n",
      "Epoch 2740, Train Loss: 0.0232, Val Loss: 0.0268\n",
      "Learning rate: 0.000050\n",
      "Epoch 2750, Train Loss: 0.0232, Val Loss: 0.0264\n",
      "Learning rate: 0.000050\n",
      "Epoch 2760, Train Loss: 0.0232, Val Loss: 0.0276\n",
      "Learning rate: 0.000049\n",
      "Epoch 2770, Train Loss: 0.0231, Val Loss: 0.0258\n",
      "Learning rate: 0.000049\n",
      "Epoch 2780, Train Loss: 0.0231, Val Loss: 0.0247\n",
      "Learning rate: 0.000049\n",
      "Epoch 2790, Train Loss: 0.0230, Val Loss: 0.0272\n",
      "Learning rate: 0.000049\n",
      "Epoch 2800, Train Loss: 0.0230, Val Loss: 0.0265\n",
      "Learning rate: 0.000049\n",
      "Epoch 2810, Train Loss: 0.0230, Val Loss: 0.0270\n",
      "Learning rate: 0.000049\n",
      "Epoch 2820, Train Loss: 0.0229, Val Loss: 0.0262\n",
      "Learning rate: 0.000049\n",
      "Epoch 2830, Train Loss: 0.0229, Val Loss: 0.0257\n",
      "Learning rate: 0.000049\n",
      "Epoch 2840, Train Loss: 0.0229, Val Loss: 0.0276\n",
      "Learning rate: 0.000048\n",
      "Epoch 2850, Train Loss: 0.0228, Val Loss: 0.0235\n",
      "Learning rate: 0.000048\n",
      "Epoch 2860, Train Loss: 0.0228, Val Loss: 0.0243\n",
      "Learning rate: 0.000048\n",
      "Epoch 2870, Train Loss: 0.0227, Val Loss: 0.0247\n",
      "Learning rate: 0.000048\n",
      "Epoch 2880, Train Loss: 0.0227, Val Loss: 0.0259\n",
      "Learning rate: 0.000048\n",
      "Epoch 2890, Train Loss: 0.0227, Val Loss: 0.0249\n",
      "Learning rate: 0.000048\n",
      "Epoch 2900, Train Loss: 0.0226, Val Loss: 0.0250\n",
      "Learning rate: 0.000048\n",
      "Epoch 2910, Train Loss: 0.0226, Val Loss: 0.0253\n",
      "Learning rate: 0.000047\n",
      "Epoch 2920, Train Loss: 0.0226, Val Loss: 0.0266\n",
      "Learning rate: 0.000047\n",
      "Epoch 2930, Train Loss: 0.0225, Val Loss: 0.0237\n",
      "Learning rate: 0.000047\n",
      "Epoch 2940, Train Loss: 0.0225, Val Loss: 0.0232\n",
      "Learning rate: 0.000047\n",
      "Epoch 2950, Train Loss: 0.0225, Val Loss: 0.0237\n",
      "Learning rate: 0.000047\n",
      "Epoch 2960, Train Loss: 0.0224, Val Loss: 0.0252\n",
      "Learning rate: 0.000047\n",
      "Epoch 2970, Train Loss: 0.0224, Val Loss: 0.0231\n",
      "Learning rate: 0.000047\n",
      "Epoch 2980, Train Loss: 0.0223, Val Loss: 0.0232\n",
      "Learning rate: 0.000047\n",
      "Epoch 2990, Train Loss: 0.0223, Val Loss: 0.0223\n",
      "Learning rate: 0.000047\n",
      "Epoch 3000, Train Loss: 0.0223, Val Loss: 0.0226\n",
      "Learning rate: 0.000047\n",
      "Epoch 3010, Train Loss: 0.0222, Val Loss: 0.0240\n",
      "Learning rate: 0.000047\n",
      "Epoch 3020, Train Loss: 0.0222, Val Loss: 0.0226\n",
      "Learning rate: 0.000046\n",
      "Epoch 3030, Train Loss: 0.0222, Val Loss: 0.0243\n",
      "Learning rate: 0.000046\n",
      "Epoch 3040, Train Loss: 0.0221, Val Loss: 0.0256\n",
      "Learning rate: 0.000046\n",
      "Epoch 3050, Train Loss: 0.0221, Val Loss: 0.0261\n",
      "Learning rate: 0.000046\n",
      "Epoch 3060, Train Loss: 0.0221, Val Loss: 0.0241\n",
      "Learning rate: 0.000046\n",
      "Epoch 3070, Train Loss: 0.0220, Val Loss: 0.0222\n",
      "Learning rate: 0.000046\n",
      "Epoch 3080, Train Loss: 0.0220, Val Loss: 0.0277\n",
      "Learning rate: 0.000046\n",
      "Epoch 3090, Train Loss: 0.0220, Val Loss: 0.0237\n",
      "Learning rate: 0.000046\n",
      "Epoch 3100, Train Loss: 0.0220, Val Loss: 0.0243\n",
      "Learning rate: 0.000046\n",
      "Epoch 3110, Train Loss: 0.0219, Val Loss: 0.0225\n",
      "Learning rate: 0.000045\n",
      "Epoch 3120, Train Loss: 0.0219, Val Loss: 0.0225\n",
      "Learning rate: 0.000045\n",
      "Epoch 3130, Train Loss: 0.0218, Val Loss: 0.0224\n",
      "Learning rate: 0.000045\n",
      "Epoch 3140, Train Loss: 0.0218, Val Loss: 0.0228\n",
      "Learning rate: 0.000045\n",
      "Epoch 3150, Train Loss: 0.0218, Val Loss: 0.0225\n",
      "Learning rate: 0.000045\n",
      "Epoch 3160, Train Loss: 0.0218, Val Loss: 0.0231\n",
      "Learning rate: 0.000045\n",
      "Epoch 3170, Train Loss: 0.0217, Val Loss: 0.0256\n",
      "Learning rate: 0.000044\n",
      "Epoch 3180, Train Loss: 0.0217, Val Loss: 0.0233\n",
      "Learning rate: 0.000044\n",
      "Epoch 3190, Train Loss: 0.0217, Val Loss: 0.0244\n",
      "Learning rate: 0.000044\n",
      "Epoch 3200, Train Loss: 0.0216, Val Loss: 0.0233\n",
      "Learning rate: 0.000044\n",
      "Epoch 3210, Train Loss: 0.0216, Val Loss: 0.0254\n",
      "Learning rate: 0.000044\n",
      "Epoch 3220, Train Loss: 0.0216, Val Loss: 0.0252\n",
      "Learning rate: 0.000044\n",
      "Epoch 3230, Train Loss: 0.0215, Val Loss: 0.0224\n",
      "Learning rate: 0.000044\n",
      "Epoch 3240, Train Loss: 0.0215, Val Loss: 0.0215\n",
      "Learning rate: 0.000044\n",
      "Epoch 3250, Train Loss: 0.0214, Val Loss: 0.0225\n",
      "Learning rate: 0.000044\n",
      "Epoch 3260, Train Loss: 0.0214, Val Loss: 0.0230\n",
      "Learning rate: 0.000043\n",
      "Epoch 3270, Train Loss: 0.0214, Val Loss: 0.0216\n",
      "Learning rate: 0.000043\n",
      "Epoch 3280, Train Loss: 0.0213, Val Loss: 0.0211\n",
      "Learning rate: 0.000043\n",
      "Epoch 3290, Train Loss: 0.0213, Val Loss: 0.0207\n",
      "Learning rate: 0.000043\n",
      "Epoch 3300, Train Loss: 0.0213, Val Loss: 0.0266\n",
      "Learning rate: 0.000043\n",
      "Epoch 3310, Train Loss: 0.0213, Val Loss: 0.0234\n",
      "Learning rate: 0.000043\n",
      "Epoch 3320, Train Loss: 0.0212, Val Loss: 0.0210\n",
      "Learning rate: 0.000043\n",
      "Epoch 3330, Train Loss: 0.0212, Val Loss: 0.0201\n",
      "Learning rate: 0.000042\n",
      "Epoch 3340, Train Loss: 0.0212, Val Loss: 0.0237\n",
      "Learning rate: 0.000042\n",
      "Epoch 3350, Train Loss: 0.0212, Val Loss: 0.0238\n",
      "Learning rate: 0.000042\n",
      "Epoch 3360, Train Loss: 0.0211, Val Loss: 0.0237\n",
      "Learning rate: 0.000042\n",
      "Epoch 3370, Train Loss: 0.0211, Val Loss: 0.0217\n",
      "Learning rate: 0.000042\n",
      "Epoch 3380, Train Loss: 0.0211, Val Loss: 0.0203\n",
      "Learning rate: 0.000042\n",
      "Epoch 3390, Train Loss: 0.0210, Val Loss: 0.0203\n",
      "Learning rate: 0.000042\n",
      "Epoch 3400, Train Loss: 0.0210, Val Loss: 0.0261\n",
      "Learning rate: 0.000042\n",
      "Epoch 3410, Train Loss: 0.0210, Val Loss: 0.0206\n",
      "Learning rate: 0.000042\n",
      "Epoch 3420, Train Loss: 0.0210, Val Loss: 0.0210\n",
      "Learning rate: 0.000041\n",
      "Epoch 3430, Train Loss: 0.0209, Val Loss: 0.0214\n",
      "Learning rate: 0.000041\n",
      "Epoch 3440, Train Loss: 0.0209, Val Loss: 0.0217\n",
      "Learning rate: 0.000041\n",
      "Epoch 3450, Train Loss: 0.0209, Val Loss: 0.0264\n",
      "Learning rate: 0.000041\n",
      "Epoch 3460, Train Loss: 0.0209, Val Loss: 0.0225\n",
      "Learning rate: 0.000041\n",
      "Epoch 3470, Train Loss: 0.0208, Val Loss: 0.0233\n",
      "Learning rate: 0.000041\n",
      "Epoch 3480, Train Loss: 0.0208, Val Loss: 0.0200\n",
      "Learning rate: 0.000041\n",
      "Epoch 3490, Train Loss: 0.0208, Val Loss: 0.0216\n",
      "Learning rate: 0.000041\n",
      "Epoch 3500, Train Loss: 0.0207, Val Loss: 0.0190\n",
      "Learning rate: 0.000040\n",
      "Epoch 3510, Train Loss: 0.0207, Val Loss: 0.0201\n",
      "Learning rate: 0.000040\n",
      "Epoch 3520, Train Loss: 0.0207, Val Loss: 0.0219\n",
      "Learning rate: 0.000040\n",
      "Epoch 3530, Train Loss: 0.0206, Val Loss: 0.0203\n",
      "Learning rate: 0.000040\n",
      "Epoch 3540, Train Loss: 0.0206, Val Loss: 0.0221\n",
      "Learning rate: 0.000040\n",
      "Epoch 3550, Train Loss: 0.0206, Val Loss: 0.0230\n",
      "Learning rate: 0.000040\n",
      "Epoch 3560, Train Loss: 0.0206, Val Loss: 0.0216\n",
      "Learning rate: 0.000040\n",
      "Epoch 3570, Train Loss: 0.0205, Val Loss: 0.0194\n",
      "Learning rate: 0.000040\n",
      "Epoch 3580, Train Loss: 0.0205, Val Loss: 0.0194\n",
      "Learning rate: 0.000040\n",
      "Epoch 3590, Train Loss: 0.0205, Val Loss: 0.0234\n",
      "Learning rate: 0.000040\n",
      "Epoch 3600, Train Loss: 0.0205, Val Loss: 0.0196\n",
      "Learning rate: 0.000039\n",
      "Epoch 3610, Train Loss: 0.0204, Val Loss: 0.0198\n",
      "Learning rate: 0.000039\n",
      "Epoch 3620, Train Loss: 0.0204, Val Loss: 0.0186\n",
      "Learning rate: 0.000039\n",
      "Epoch 3630, Train Loss: 0.0204, Val Loss: 0.0197\n",
      "Learning rate: 0.000039\n",
      "Epoch 3640, Train Loss: 0.0204, Val Loss: 0.0207\n",
      "Learning rate: 0.000039\n",
      "Epoch 3650, Train Loss: 0.0203, Val Loss: 0.0203\n",
      "Learning rate: 0.000039\n",
      "Epoch 3660, Train Loss: 0.0203, Val Loss: 0.0201\n",
      "Learning rate: 0.000039\n",
      "Epoch 3670, Train Loss: 0.0203, Val Loss: 0.0199\n",
      "Learning rate: 0.000039\n",
      "Epoch 3680, Train Loss: 0.0202, Val Loss: 0.0186\n",
      "Learning rate: 0.000039\n",
      "Epoch 3690, Train Loss: 0.0203, Val Loss: 0.0190\n",
      "Learning rate: 0.000039\n",
      "Epoch 3700, Train Loss: 0.0202, Val Loss: 0.0204\n",
      "Learning rate: 0.000038\n",
      "Epoch 3710, Train Loss: 0.0202, Val Loss: 0.0216\n",
      "Learning rate: 0.000038\n",
      "Epoch 3720, Train Loss: 0.0201, Val Loss: 0.0210\n",
      "Learning rate: 0.000038\n",
      "Epoch 3730, Train Loss: 0.0201, Val Loss: 0.0185\n",
      "Learning rate: 0.000038\n",
      "Epoch 3740, Train Loss: 0.0201, Val Loss: 0.0199\n",
      "Learning rate: 0.000038\n",
      "Epoch 3750, Train Loss: 0.0201, Val Loss: 0.0190\n",
      "Learning rate: 0.000038\n",
      "Epoch 3760, Train Loss: 0.0200, Val Loss: 0.0185\n",
      "Learning rate: 0.000038\n",
      "Epoch 3770, Train Loss: 0.0200, Val Loss: 0.0219\n",
      "Learning rate: 0.000038\n",
      "Epoch 3780, Train Loss: 0.0200, Val Loss: 0.0200\n",
      "Learning rate: 0.000038\n",
      "Epoch 3790, Train Loss: 0.0200, Val Loss: 0.0223\n",
      "Learning rate: 0.000038\n",
      "Epoch 3800, Train Loss: 0.0199, Val Loss: 0.0214\n",
      "Learning rate: 0.000037\n",
      "Epoch 3810, Train Loss: 0.0199, Val Loss: 0.0196\n",
      "Learning rate: 0.000037\n",
      "Epoch 3820, Train Loss: 0.0199, Val Loss: 0.0197\n",
      "Learning rate: 0.000037\n",
      "Epoch 3830, Train Loss: 0.0199, Val Loss: 0.0278\n",
      "Learning rate: 0.000037\n",
      "Epoch 3840, Train Loss: 0.0199, Val Loss: 0.0218\n",
      "Learning rate: 0.000037\n",
      "Epoch 3850, Train Loss: 0.0198, Val Loss: 0.0194\n",
      "Learning rate: 0.000037\n",
      "Epoch 3860, Train Loss: 0.0198, Val Loss: 0.0188\n",
      "Learning rate: 0.000037\n",
      "Epoch 3870, Train Loss: 0.0197, Val Loss: 0.0176\n",
      "Learning rate: 0.000037\n",
      "Epoch 3880, Train Loss: 0.0197, Val Loss: 0.0193\n",
      "Learning rate: 0.000037\n",
      "Epoch 3890, Train Loss: 0.0197, Val Loss: 0.0193\n",
      "Learning rate: 0.000037\n",
      "Epoch 3900, Train Loss: 0.0197, Val Loss: 0.0188\n",
      "Learning rate: 0.000037\n",
      "Epoch 3910, Train Loss: 0.0196, Val Loss: 0.0177\n",
      "Learning rate: 0.000036\n",
      "Epoch 3920, Train Loss: 0.0196, Val Loss: 0.0190\n",
      "Learning rate: 0.000036\n",
      "Epoch 3930, Train Loss: 0.0196, Val Loss: 0.0181\n",
      "Learning rate: 0.000036\n",
      "Epoch 3940, Train Loss: 0.0196, Val Loss: 0.0173\n",
      "Learning rate: 0.000036\n",
      "Epoch 3950, Train Loss: 0.0196, Val Loss: 0.0195\n",
      "Learning rate: 0.000036\n",
      "Epoch 3960, Train Loss: 0.0195, Val Loss: 0.0172\n",
      "Learning rate: 0.000036\n",
      "Epoch 3970, Train Loss: 0.0195, Val Loss: 0.0186\n",
      "Learning rate: 0.000036\n",
      "Epoch 3980, Train Loss: 0.0195, Val Loss: 0.0175\n",
      "Learning rate: 0.000036\n",
      "Epoch 3990, Train Loss: 0.0195, Val Loss: 0.0196\n",
      "Learning rate: 0.000036\n",
      "Epoch 4000, Train Loss: 0.0194, Val Loss: 0.0180\n",
      "Learning rate: 0.000035\n",
      "Epoch 4010, Train Loss: 0.0194, Val Loss: 0.0173\n",
      "Learning rate: 0.000035\n",
      "Epoch 4020, Train Loss: 0.0194, Val Loss: 0.0169\n",
      "Learning rate: 0.000035\n",
      "Epoch 4030, Train Loss: 0.0193, Val Loss: 0.0180\n",
      "Learning rate: 0.000035\n",
      "Epoch 4040, Train Loss: 0.0193, Val Loss: 0.0174\n",
      "Learning rate: 0.000035\n",
      "Epoch 4050, Train Loss: 0.0193, Val Loss: 0.0173\n",
      "Learning rate: 0.000035\n",
      "Epoch 4060, Train Loss: 0.0193, Val Loss: 0.0178\n",
      "Learning rate: 0.000035\n",
      "Epoch 4070, Train Loss: 0.0192, Val Loss: 0.0167\n",
      "Learning rate: 0.000035\n",
      "Epoch 4080, Train Loss: 0.0192, Val Loss: 0.0180\n",
      "Learning rate: 0.000035\n",
      "Epoch 4090, Train Loss: 0.0192, Val Loss: 0.0177\n",
      "Learning rate: 0.000035\n",
      "Epoch 4100, Train Loss: 0.0192, Val Loss: 0.0203\n",
      "Learning rate: 0.000035\n",
      "Epoch 4110, Train Loss: 0.0191, Val Loss: 0.0196\n",
      "Learning rate: 0.000034\n",
      "Epoch 4120, Train Loss: 0.0191, Val Loss: 0.0193\n",
      "Learning rate: 0.000034\n",
      "Epoch 4130, Train Loss: 0.0191, Val Loss: 0.0216\n",
      "Learning rate: 0.000034\n",
      "Epoch 4140, Train Loss: 0.0191, Val Loss: 0.0172\n",
      "Learning rate: 0.000034\n",
      "Epoch 4150, Train Loss: 0.0191, Val Loss: 0.0177\n",
      "Learning rate: 0.000034\n",
      "Epoch 4160, Train Loss: 0.0190, Val Loss: 0.0178\n",
      "Learning rate: 0.000034\n",
      "Epoch 4170, Train Loss: 0.0190, Val Loss: 0.0173\n",
      "Learning rate: 0.000034\n",
      "Epoch 4180, Train Loss: 0.0190, Val Loss: 0.0196\n",
      "Learning rate: 0.000034\n",
      "Epoch 4190, Train Loss: 0.0190, Val Loss: 0.0184\n",
      "Learning rate: 0.000034\n",
      "Epoch 4200, Train Loss: 0.0190, Val Loss: 0.0181\n",
      "Learning rate: 0.000034\n",
      "Epoch 4210, Train Loss: 0.0189, Val Loss: 0.0176\n",
      "Learning rate: 0.000034\n",
      "Epoch 4220, Train Loss: 0.0189, Val Loss: 0.0191\n",
      "Learning rate: 0.000034\n",
      "Epoch 4230, Train Loss: 0.0189, Val Loss: 0.0177\n",
      "Learning rate: 0.000033\n",
      "Epoch 4240, Train Loss: 0.0188, Val Loss: 0.0180\n",
      "Learning rate: 0.000033\n",
      "Epoch 4250, Train Loss: 0.0188, Val Loss: 0.0191\n",
      "Learning rate: 0.000033\n",
      "Epoch 4260, Train Loss: 0.0188, Val Loss: 0.0202\n",
      "Learning rate: 0.000033\n",
      "Epoch 4270, Train Loss: 0.0188, Val Loss: 0.0199\n",
      "Learning rate: 0.000033\n",
      "Epoch 4280, Train Loss: 0.0187, Val Loss: 0.0185\n",
      "Learning rate: 0.000033\n",
      "Epoch 4290, Train Loss: 0.0187, Val Loss: 0.0190\n",
      "Learning rate: 0.000033\n",
      "Epoch 4300, Train Loss: 0.0187, Val Loss: 0.0166\n",
      "Learning rate: 0.000033\n",
      "Epoch 4310, Train Loss: 0.0187, Val Loss: 0.0162\n",
      "Learning rate: 0.000033\n",
      "Epoch 4320, Train Loss: 0.0187, Val Loss: 0.0183\n",
      "Learning rate: 0.000033\n",
      "Epoch 4330, Train Loss: 0.0186, Val Loss: 0.0162\n",
      "Learning rate: 0.000033\n",
      "Epoch 4340, Train Loss: 0.0186, Val Loss: 0.0160\n",
      "Learning rate: 0.000033\n",
      "Epoch 4350, Train Loss: 0.0186, Val Loss: 0.0160\n",
      "Learning rate: 0.000033\n",
      "Epoch 4360, Train Loss: 0.0186, Val Loss: 0.0163\n",
      "Learning rate: 0.000033\n",
      "Epoch 4370, Train Loss: 0.0185, Val Loss: 0.0165\n",
      "Learning rate: 0.000032\n",
      "Epoch 4380, Train Loss: 0.0185, Val Loss: 0.0158\n",
      "Learning rate: 0.000032\n",
      "Epoch 4390, Train Loss: 0.0185, Val Loss: 0.0155\n",
      "Learning rate: 0.000032\n",
      "Epoch 4400, Train Loss: 0.0185, Val Loss: 0.0169\n",
      "Learning rate: 0.000032\n",
      "Epoch 4410, Train Loss: 0.0185, Val Loss: 0.0167\n",
      "Learning rate: 0.000032\n",
      "Epoch 4420, Train Loss: 0.0184, Val Loss: 0.0160\n",
      "Learning rate: 0.000032\n",
      "Epoch 4430, Train Loss: 0.0184, Val Loss: 0.0166\n",
      "Learning rate: 0.000032\n",
      "Epoch 4440, Train Loss: 0.0184, Val Loss: 0.0180\n",
      "Learning rate: 0.000032\n",
      "Epoch 4450, Train Loss: 0.0184, Val Loss: 0.0162\n",
      "Learning rate: 0.000032\n",
      "Epoch 4460, Train Loss: 0.0183, Val Loss: 0.0157\n",
      "Learning rate: 0.000032\n",
      "Epoch 4470, Train Loss: 0.0183, Val Loss: 0.0163\n",
      "Learning rate: 0.000031\n",
      "Epoch 4480, Train Loss: 0.0183, Val Loss: 0.0159\n",
      "Learning rate: 0.000031\n",
      "Epoch 4490, Train Loss: 0.0183, Val Loss: 0.0162\n",
      "Learning rate: 0.000031\n",
      "Epoch 4500, Train Loss: 0.0183, Val Loss: 0.0152\n",
      "Learning rate: 0.000031\n",
      "Epoch 4510, Train Loss: 0.0182, Val Loss: 0.0155\n",
      "Learning rate: 0.000031\n",
      "Epoch 4520, Train Loss: 0.0182, Val Loss: 0.0200\n",
      "Learning rate: 0.000031\n",
      "Epoch 4530, Train Loss: 0.0182, Val Loss: 0.0160\n",
      "Learning rate: 0.000031\n",
      "Epoch 4550, Train Loss: 0.0182, Val Loss: 0.0195\n",
      "Learning rate: 0.000031\n",
      "Epoch 4560, Train Loss: 0.0181, Val Loss: 0.0170\n",
      "Learning rate: 0.000031\n",
      "Epoch 4570, Train Loss: 0.0181, Val Loss: 0.0154\n",
      "Learning rate: 0.000030\n",
      "Epoch 4580, Train Loss: 0.0181, Val Loss: 0.0160\n",
      "Learning rate: 0.000030\n",
      "Epoch 4590, Train Loss: 0.0181, Val Loss: 0.0169\n",
      "Learning rate: 0.000030\n",
      "Epoch 4600, Train Loss: 0.0180, Val Loss: 0.0154\n",
      "Learning rate: 0.000030\n",
      "Epoch 4610, Train Loss: 0.0180, Val Loss: 0.0148\n",
      "Learning rate: 0.000030\n",
      "Epoch 4620, Train Loss: 0.0180, Val Loss: 0.0178\n",
      "Learning rate: 0.000030\n",
      "Epoch 4630, Train Loss: 0.0180, Val Loss: 0.0160\n",
      "Learning rate: 0.000030\n",
      "Epoch 4640, Train Loss: 0.0180, Val Loss: 0.0175\n",
      "Learning rate: 0.000030\n",
      "Epoch 4650, Train Loss: 0.0180, Val Loss: 0.0150\n",
      "Learning rate: 0.000030\n",
      "Epoch 4660, Train Loss: 0.0179, Val Loss: 0.0164\n",
      "Learning rate: 0.000030\n",
      "Epoch 4670, Train Loss: 0.0179, Val Loss: 0.0149\n",
      "Learning rate: 0.000030\n",
      "Epoch 4680, Train Loss: 0.0179, Val Loss: 0.0149\n",
      "Learning rate: 0.000030\n",
      "Epoch 4690, Train Loss: 0.0179, Val Loss: 0.0152\n",
      "Learning rate: 0.000030\n",
      "Epoch 4700, Train Loss: 0.0178, Val Loss: 0.0158\n",
      "Learning rate: 0.000029\n",
      "Epoch 4710, Train Loss: 0.0178, Val Loss: 0.0148\n",
      "Learning rate: 0.000029\n",
      "Epoch 4720, Train Loss: 0.0178, Val Loss: 0.0175\n",
      "Learning rate: 0.000029\n",
      "Epoch 4730, Train Loss: 0.0178, Val Loss: 0.0151\n",
      "Learning rate: 0.000029\n",
      "Epoch 4740, Train Loss: 0.0178, Val Loss: 0.0147\n",
      "Learning rate: 0.000029\n",
      "Epoch 4750, Train Loss: 0.0178, Val Loss: 0.0159\n",
      "Learning rate: 0.000029\n",
      "Epoch 4760, Train Loss: 0.0177, Val Loss: 0.0155\n",
      "Learning rate: 0.000029\n",
      "Epoch 4770, Train Loss: 0.0177, Val Loss: 0.0147\n",
      "Learning rate: 0.000029\n",
      "Epoch 4780, Train Loss: 0.0177, Val Loss: 0.0183\n",
      "Learning rate: 0.000029\n",
      "Epoch 4790, Train Loss: 0.0177, Val Loss: 0.0146\n",
      "Learning rate: 0.000029\n",
      "Epoch 4800, Train Loss: 0.0177, Val Loss: 0.0154\n",
      "Learning rate: 0.000029\n",
      "Epoch 4810, Train Loss: 0.0176, Val Loss: 0.0164\n",
      "Learning rate: 0.000028\n",
      "Epoch 4820, Train Loss: 0.0176, Val Loss: 0.0158\n",
      "Learning rate: 0.000028\n",
      "Epoch 4830, Train Loss: 0.0176, Val Loss: 0.0148\n",
      "Learning rate: 0.000028\n",
      "Epoch 4840, Train Loss: 0.0176, Val Loss: 0.0155\n",
      "Learning rate: 0.000028\n",
      "Epoch 4850, Train Loss: 0.0176, Val Loss: 0.0174\n",
      "Learning rate: 0.000028\n",
      "Epoch 4860, Train Loss: 0.0176, Val Loss: 0.0157\n",
      "Learning rate: 0.000028\n",
      "Epoch 4870, Train Loss: 0.0175, Val Loss: 0.0170\n",
      "Learning rate: 0.000028\n",
      "Epoch 4880, Train Loss: 0.0175, Val Loss: 0.0147\n",
      "Learning rate: 0.000028\n",
      "Epoch 4890, Train Loss: 0.0175, Val Loss: 0.0150\n",
      "Learning rate: 0.000028\n",
      "Epoch 4900, Train Loss: 0.0175, Val Loss: 0.0158\n",
      "Learning rate: 0.000028\n",
      "Epoch 4910, Train Loss: 0.0175, Val Loss: 0.0161\n",
      "Learning rate: 0.000028\n",
      "Epoch 4920, Train Loss: 0.0174, Val Loss: 0.0151\n",
      "Learning rate: 0.000027\n",
      "Epoch 4930, Train Loss: 0.0174, Val Loss: 0.0160\n",
      "Learning rate: 0.000027\n",
      "Epoch 4940, Train Loss: 0.0174, Val Loss: 0.0155\n",
      "Learning rate: 0.000027\n",
      "Epoch 4950, Train Loss: 0.0174, Val Loss: 0.0151\n",
      "Learning rate: 0.000027\n",
      "Epoch 4960, Train Loss: 0.0174, Val Loss: 0.0153\n",
      "Learning rate: 0.000027\n",
      "Epoch 4970, Train Loss: 0.0174, Val Loss: 0.0160\n",
      "Learning rate: 0.000027\n",
      "Epoch 4980, Train Loss: 0.0173, Val Loss: 0.0149\n",
      "Learning rate: 0.000027\n",
      "Epoch 4990, Train Loss: 0.0173, Val Loss: 0.0157\n",
      "Learning rate: 0.000027\n",
      "Epoch 5000, Train Loss: 0.0173, Val Loss: 0.0141\n",
      "Learning rate: 0.000027\n",
      "Epoch 5010, Train Loss: 0.0173, Val Loss: 0.0142\n",
      "Learning rate: 0.000027\n",
      "Epoch 5020, Train Loss: 0.0173, Val Loss: 0.0149\n",
      "Learning rate: 0.000027\n",
      "Epoch 5030, Train Loss: 0.0173, Val Loss: 0.0140\n",
      "Learning rate: 0.000027\n",
      "Epoch 5040, Train Loss: 0.0172, Val Loss: 0.0147\n",
      "Learning rate: 0.000027\n",
      "Epoch 5050, Train Loss: 0.0172, Val Loss: 0.0147\n",
      "Learning rate: 0.000027\n",
      "Epoch 5060, Train Loss: 0.0172, Val Loss: 0.0142\n",
      "Learning rate: 0.000026\n",
      "Epoch 5070, Train Loss: 0.0172, Val Loss: 0.0157\n",
      "Learning rate: 0.000026\n",
      "Epoch 5080, Train Loss: 0.0172, Val Loss: 0.0139\n",
      "Learning rate: 0.000026\n",
      "Epoch 5090, Train Loss: 0.0171, Val Loss: 0.0141\n",
      "Learning rate: 0.000026\n",
      "Epoch 5100, Train Loss: 0.0172, Val Loss: 0.0147\n",
      "Learning rate: 0.000026\n",
      "Epoch 5110, Train Loss: 0.0171, Val Loss: 0.0142\n",
      "Learning rate: 0.000026\n",
      "Epoch 5120, Train Loss: 0.0171, Val Loss: 0.0144\n",
      "Learning rate: 0.000026\n",
      "Epoch 5130, Train Loss: 0.0171, Val Loss: 0.0145\n",
      "Learning rate: 0.000026\n",
      "Epoch 5140, Train Loss: 0.0171, Val Loss: 0.0138\n",
      "Learning rate: 0.000026\n",
      "Epoch 5150, Train Loss: 0.0171, Val Loss: 0.0141\n",
      "Learning rate: 0.000026\n",
      "Epoch 5160, Train Loss: 0.0171, Val Loss: 0.0138\n",
      "Learning rate: 0.000026\n",
      "Epoch 5180, Train Loss: 0.0170, Val Loss: 0.0135\n",
      "Learning rate: 0.000026\n",
      "Epoch 5190, Train Loss: 0.0170, Val Loss: 0.0141\n",
      "Learning rate: 0.000025\n",
      "Epoch 5200, Train Loss: 0.0170, Val Loss: 0.0142\n",
      "Learning rate: 0.000025\n",
      "Epoch 5220, Train Loss: 0.0169, Val Loss: 0.0146\n",
      "Learning rate: 0.000025\n",
      "Epoch 5230, Train Loss: 0.0169, Val Loss: 0.0142\n",
      "Learning rate: 0.000025\n",
      "Epoch 5240, Train Loss: 0.0169, Val Loss: 0.0147\n",
      "Learning rate: 0.000025\n",
      "Epoch 5250, Train Loss: 0.0169, Val Loss: 0.0146\n",
      "Learning rate: 0.000025\n",
      "Epoch 5260, Train Loss: 0.0169, Val Loss: 0.0146\n",
      "Learning rate: 0.000025\n",
      "Epoch 5270, Train Loss: 0.0169, Val Loss: 0.0140\n",
      "Learning rate: 0.000025\n",
      "Epoch 5280, Train Loss: 0.0169, Val Loss: 0.0143\n",
      "Learning rate: 0.000025\n",
      "Epoch 5290, Train Loss: 0.0169, Val Loss: 0.0179\n",
      "Learning rate: 0.000025\n",
      "Epoch 5300, Train Loss: 0.0168, Val Loss: 0.0154\n",
      "Learning rate: 0.000025\n",
      "Epoch 5310, Train Loss: 0.0168, Val Loss: 0.0140\n",
      "Learning rate: 0.000025\n",
      "Epoch 5320, Train Loss: 0.0168, Val Loss: 0.0138\n",
      "Learning rate: 0.000025\n",
      "Epoch 5330, Train Loss: 0.0168, Val Loss: 0.0138\n",
      "Learning rate: 0.000025\n",
      "Epoch 5340, Train Loss: 0.0168, Val Loss: 0.0140\n",
      "Learning rate: 0.000024\n",
      "Epoch 5350, Train Loss: 0.0168, Val Loss: 0.0139\n",
      "Learning rate: 0.000024\n",
      "Epoch 5360, Train Loss: 0.0168, Val Loss: 0.0143\n",
      "Learning rate: 0.000024\n",
      "Epoch 5370, Train Loss: 0.0167, Val Loss: 0.0135\n",
      "Learning rate: 0.000024\n",
      "Epoch 5380, Train Loss: 0.0167, Val Loss: 0.0139\n",
      "Learning rate: 0.000024\n",
      "Epoch 5390, Train Loss: 0.0167, Val Loss: 0.0135\n",
      "Learning rate: 0.000024\n",
      "Epoch 5400, Train Loss: 0.0167, Val Loss: 0.0135\n",
      "Learning rate: 0.000024\n",
      "Epoch 5410, Train Loss: 0.0167, Val Loss: 0.0132\n",
      "Learning rate: 0.000024\n",
      "Epoch 5420, Train Loss: 0.0167, Val Loss: 0.0164\n",
      "Learning rate: 0.000024\n",
      "Epoch 5430, Train Loss: 0.0166, Val Loss: 0.0137\n",
      "Learning rate: 0.000024\n",
      "Epoch 5440, Train Loss: 0.0167, Val Loss: 0.0131\n",
      "Learning rate: 0.000024\n",
      "Epoch 5450, Train Loss: 0.0166, Val Loss: 0.0132\n",
      "Learning rate: 0.000024\n",
      "Epoch 5460, Train Loss: 0.0166, Val Loss: 0.0136\n",
      "Learning rate: 0.000024\n",
      "Epoch 5470, Train Loss: 0.0166, Val Loss: 0.0132\n",
      "Learning rate: 0.000024\n",
      "Epoch 5480, Train Loss: 0.0166, Val Loss: 0.0130\n",
      "Learning rate: 0.000024\n",
      "Epoch 5490, Train Loss: 0.0166, Val Loss: 0.0137\n",
      "Learning rate: 0.000023\n",
      "Epoch 5500, Train Loss: 0.0166, Val Loss: 0.0131\n",
      "Learning rate: 0.000023\n",
      "Epoch 5510, Train Loss: 0.0165, Val Loss: 0.0131\n",
      "Learning rate: 0.000023\n",
      "Epoch 5520, Train Loss: 0.0166, Val Loss: 0.0137\n",
      "Learning rate: 0.000023\n",
      "Epoch 5530, Train Loss: 0.0165, Val Loss: 0.0131\n",
      "Learning rate: 0.000023\n",
      "Epoch 5540, Train Loss: 0.0165, Val Loss: 0.0129\n",
      "Learning rate: 0.000023\n",
      "Epoch 5550, Train Loss: 0.0165, Val Loss: 0.0133\n",
      "Learning rate: 0.000023\n",
      "Epoch 5560, Train Loss: 0.0165, Val Loss: 0.0178\n",
      "Learning rate: 0.000023\n",
      "Epoch 5570, Train Loss: 0.0165, Val Loss: 0.0144\n",
      "Learning rate: 0.000023\n",
      "Epoch 5580, Train Loss: 0.0165, Val Loss: 0.0134\n",
      "Learning rate: 0.000023\n",
      "Epoch 5590, Train Loss: 0.0164, Val Loss: 0.0133\n",
      "Learning rate: 0.000023\n",
      "Epoch 5600, Train Loss: 0.0164, Val Loss: 0.0131\n",
      "Learning rate: 0.000023\n",
      "Epoch 5610, Train Loss: 0.0164, Val Loss: 0.0128\n",
      "Learning rate: 0.000023\n",
      "Epoch 5620, Train Loss: 0.0164, Val Loss: 0.0133\n",
      "Learning rate: 0.000023\n",
      "Epoch 5630, Train Loss: 0.0164, Val Loss: 0.0139\n",
      "Learning rate: 0.000023\n",
      "Epoch 5640, Train Loss: 0.0164, Val Loss: 0.0137\n",
      "Learning rate: 0.000023\n",
      "Epoch 5650, Train Loss: 0.0164, Val Loss: 0.0133\n",
      "Learning rate: 0.000022\n",
      "Epoch 5660, Train Loss: 0.0164, Val Loss: 0.0129\n",
      "Learning rate: 0.000022\n",
      "Epoch 5670, Train Loss: 0.0163, Val Loss: 0.0130\n",
      "Learning rate: 0.000022\n",
      "Epoch 5680, Train Loss: 0.0163, Val Loss: 0.0129\n",
      "Learning rate: 0.000022\n",
      "Epoch 5690, Train Loss: 0.0163, Val Loss: 0.0134\n",
      "Learning rate: 0.000022\n",
      "Epoch 5700, Train Loss: 0.0163, Val Loss: 0.0127\n",
      "Learning rate: 0.000022\n",
      "Epoch 5710, Train Loss: 0.0163, Val Loss: 0.0135\n",
      "Learning rate: 0.000022\n",
      "Epoch 5720, Train Loss: 0.0163, Val Loss: 0.0132\n",
      "Learning rate: 0.000022\n",
      "Epoch 5730, Train Loss: 0.0163, Val Loss: 0.0132\n",
      "Learning rate: 0.000022\n",
      "Epoch 5740, Train Loss: 0.0163, Val Loss: 0.0130\n",
      "Learning rate: 0.000022\n",
      "Epoch 5750, Train Loss: 0.0162, Val Loss: 0.0133\n",
      "Learning rate: 0.000022\n",
      "Epoch 5760, Train Loss: 0.0162, Val Loss: 0.0133\n",
      "Learning rate: 0.000022\n",
      "Epoch 5770, Train Loss: 0.0162, Val Loss: 0.0126\n",
      "Learning rate: 0.000022\n",
      "Epoch 5780, Train Loss: 0.0162, Val Loss: 0.0132\n",
      "Learning rate: 0.000022\n",
      "Epoch 5790, Train Loss: 0.0162, Val Loss: 0.0126\n",
      "Learning rate: 0.000022\n",
      "Epoch 5800, Train Loss: 0.0162, Val Loss: 0.0128\n",
      "Learning rate: 0.000022\n",
      "Epoch 5810, Train Loss: 0.0162, Val Loss: 0.0127\n",
      "Learning rate: 0.000022\n",
      "Epoch 5820, Train Loss: 0.0162, Val Loss: 0.0126\n",
      "Learning rate: 0.000021\n",
      "Epoch 5830, Train Loss: 0.0161, Val Loss: 0.0132\n",
      "Learning rate: 0.000021\n",
      "Epoch 5840, Train Loss: 0.0161, Val Loss: 0.0130\n",
      "Learning rate: 0.000021\n",
      "Epoch 5850, Train Loss: 0.0161, Val Loss: 0.0125\n",
      "Learning rate: 0.000021\n",
      "Epoch 5860, Train Loss: 0.0161, Val Loss: 0.0127\n",
      "Learning rate: 0.000021\n",
      "Epoch 5870, Train Loss: 0.0161, Val Loss: 0.0123\n",
      "Learning rate: 0.000021\n",
      "Epoch 5880, Train Loss: 0.0161, Val Loss: 0.0120\n",
      "Learning rate: 0.000021\n",
      "Epoch 5890, Train Loss: 0.0161, Val Loss: 0.0123\n",
      "Learning rate: 0.000021\n",
      "Epoch 5900, Train Loss: 0.0160, Val Loss: 0.0140\n",
      "Learning rate: 0.000021\n",
      "Epoch 5910, Train Loss: 0.0160, Val Loss: 0.0124\n",
      "Learning rate: 0.000021\n",
      "Epoch 5920, Train Loss: 0.0160, Val Loss: 0.0130\n",
      "Learning rate: 0.000021\n",
      "Epoch 5930, Train Loss: 0.0160, Val Loss: 0.0133\n",
      "Learning rate: 0.000021\n",
      "Epoch 5940, Train Loss: 0.0160, Val Loss: 0.0125\n",
      "Learning rate: 0.000021\n",
      "Epoch 5950, Train Loss: 0.0160, Val Loss: 0.0122\n",
      "Learning rate: 0.000021\n",
      "Epoch 5960, Train Loss: 0.0160, Val Loss: 0.0138\n",
      "Learning rate: 0.000021\n",
      "Epoch 5970, Train Loss: 0.0160, Val Loss: 0.0126\n",
      "Learning rate: 0.000021\n",
      "Epoch 5980, Train Loss: 0.0160, Val Loss: 0.0123\n",
      "Learning rate: 0.000021\n",
      "Epoch 5990, Train Loss: 0.0159, Val Loss: 0.0121\n",
      "Learning rate: 0.000021\n",
      "Time taken: 9920.638117313385\n",
      "Best epoch: 5951, Best validation loss: 0.0118\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Difference:\n",
      "[3166868.0000, 4490224.0000, 3585280.0000, 5629552.0000, -929936.0000, -7857680.0000, 1206432.0000, -2317216.0000, 575296.0000, -34388160.0000,\n",
      " -4807472.0000, 6119280.0000, 2568176.0000, 520496.0000, 7529184.0000, -3151360.0000, 864832.0000, 1384240.0000, 1711296.0000, 112048.0000,\n",
      " 1936816.0000, -276976.0000, 636096.0000, -3206512.0000, -3433528.0000, -4694816.0000, 1509008.0000, 1656128.0000, -2504048.0000, -1312384.0000,\n",
      " -3252048.0000, -5663232.0000, 2971776.0000, 33472.0000, -1470080.0000, -1566528.0000, 773424.0000, -3335536.0000, -414768.0000]\n",
      "\n",
      "E_test_denorm:\n",
      "[52500000.0000, 210000000.0000, 210000000.0000, 210000000.0000, 210000000.0000, 105000000.0000, 210000000.0000, 210000000.0000, 210000000.0000, 105000000.0000,\n",
      " 210000000.0000, 210000000.0000, 210000000.0000, 210000000.0000, 210000000.0000, 210000000.0000, 210000000.0000, 210000000.0000, 210000000.0000, 210000000.0000,\n",
      " 210000000.0000, 210000000.0000, 210000000.0000, 210000000.0000, 52500000.0000, 210000000.0000, 210000000.0000, 210000000.0000, 210000000.0000, 210000000.0000,\n",
      " 210000000.0000, 210000000.0000, 210000000.0000, 210000000.0000, 210000000.0000, 210000000.0000, 210000000.0000, 210000000.0000, 210000000.0000]\n",
      "\n",
      "Pred_test_denorm:\n",
      "[49333132.0000, 205509776.0000, 206414720.0000, 204370448.0000, 210929936.0000, 112857680.0000, 208793568.0000, 212317216.0000, 209424704.0000, 139388160.0000,\n",
      " 214807472.0000, 203880720.0000, 207431824.0000, 209479504.0000, 202470816.0000, 213151360.0000, 209135168.0000, 208615760.0000, 208288704.0000, 209887952.0000,\n",
      " 208063184.0000, 210276976.0000, 209363904.0000, 213206512.0000, 55933528.0000, 214694816.0000, 208490992.0000, 208343872.0000, 212504048.0000, 211312384.0000,\n",
      " 213252048.0000, 215663232.0000, 207028224.0000, 209966528.0000, 211470080.0000, 211566528.0000, 209226576.0000, 213335536.0000, 210414768.0000]\n",
      "\n",
      "\n",
      "MAPE: 1.9428%\n",
      "MdAPE: 1.067611388862133%\n",
      "Best epoch: 5951\n"
     ]
    }
   ],
   "source": [
    "class NormalizedCombinedRegressionLoss_modified(nn.Module):\n",
    "    def __init__(self, alpha=0.35, beta=0.3, gamma=0.15, delta_max=0.2, threshold=0.1, epsilon=1e-9, start_epoch=10, ramp_epochs=500):\n",
    "        super().__init__()\n",
    "        self.alpha = alpha\n",
    "        self.beta = beta\n",
    "        self.gamma = gamma\n",
    "        self.delta_max = delta_max\n",
    "        self.threshold = threshold\n",
    "        self.epsilon = epsilon\n",
    "        self.start_epoch = start_epoch\n",
    "        self.ramp_epochs = ramp_epochs\n",
    "        self.huber_loss = nn.HuberLoss(delta=1.0)\n",
    "        self.mse_loss = nn.MSELoss()\n",
    "        self.mae_loss = nn.L1Loss()\n",
    "        self.current_epoch = 0\n",
    "\n",
    "    def forward(self, pred, target):\n",
    "        huber = self.huber_loss(pred, target)\n",
    "        mse = self.mse_loss(pred, target)\n",
    "        mae = self.mae_loss(pred, target)\n",
    "        \n",
    "        # Scale losses based on their theoretical properties\n",
    "        scaled_mse = torch.sqrt(mse)\n",
    "        \n",
    "        total_loss = self.alpha * huber + self.beta * scaled_mse + self.gamma * mae\n",
    "\n",
    "        # Only apply high error penalty after start_epoch\n",
    "        if self.current_epoch >= self.start_epoch:\n",
    "            # Calculate percentage errors with safeguard against division by zero\n",
    "            percentage_errors = torch.abs((pred - target) / (target + self.epsilon))\n",
    "            \n",
    "            # Clip percentage errors to avoid extremely large values\n",
    "            percentage_errors = torch.clamp(percentage_errors, max=10)\n",
    "            \n",
    "            # Calculate the proportion of errors above the threshold\n",
    "            above_threshold = (percentage_errors > self.threshold).float()\n",
    "            proportion_above = torch.mean(above_threshold)\n",
    "            \n",
    "            # Dynamically adjust delta based on the proportion of high errors\n",
    "            delta = self.delta_max * proportion_above\n",
    "            \n",
    "            # Apply additional penalty for errors above the threshold\n",
    "            high_error_penalty = torch.mean(torch.pow(torch.relu(percentage_errors - self.threshold), 2))\n",
    "            \n",
    "            # Normalize high_error_penalty to be on a similar scale as other losses\n",
    "            normalized_high_error_penalty = high_error_penalty / (self.threshold ** 2)\n",
    "            \n",
    "            # Gradually introduce the high error penalty over ramp_epochs\n",
    "            ramp_factor = min(1.0, (self.current_epoch - self.start_epoch + 1) / self.ramp_epochs)\n",
    "            \n",
    "            total_loss += ramp_factor * delta * normalized_high_error_penalty\n",
    "        \n",
    "        # Add a small epsilon to avoid returning exactly zero\n",
    "        return total_loss + self.epsilon\n",
    "\n",
    "    def update_epoch(self, epoch):\n",
    "        self.current_epoch = epoch\n",
    "\n",
    "\n",
    "# Set the seed for reproducibility\n",
    "seed = 42\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "\n",
    "# Define combined loss function, optimizer and scheduler\n",
    "loss_fn = NormalizedCombinedRegressionLoss_modified(alpha=0.35, beta=0.3, gamma=0.15, delta_max=0.2,\n",
    "                                                    threshold=0.1, epsilon=1e-9, start_epoch=10, ramp_epochs=500)\n",
    "optimizer = optim.AdamW(model.parameters(), lr=1e-4, weight_decay=0.005)\n",
    "scheduler = ReduceLROnPlateau(optimizer, 'min', patience=15, factor=0.995)\n",
    "\n",
    "# Move the model to the device\n",
    "model = model.to(device)\n",
    "E_test = E_test.to(device)\n",
    "\n",
    "# Initialize variables for tracking\n",
    "best_model = None\n",
    "best_val_loss = float('inf')\n",
    "best_epoch = 0\n",
    "epochs_without_improvement = 0\n",
    "patience = 350  # for early stopping\n",
    "\n",
    "# For plotting\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "\n",
    "start_time = time.time()\n",
    "try:\n",
    "    for epoch in range(epochs_dl):\n",
    "        # Training\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        pred_train = model(x_categ[:x_cont_train.shape[0]], x_cont_train)\n",
    "        loss = loss_fn(pred_train, E_train)\n",
    "        loss.backward()\n",
    "        nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        optimizer.step()\n",
    "\n",
    "        # Validation\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            pred_val = model(x_categ[:x_cont_test.shape[0]], x_cont_test)\n",
    "            val_loss = loss_fn(pred_val, E_test)\n",
    "\n",
    "        # Update scheduler\n",
    "        scheduler.step(val_loss)\n",
    "\n",
    "        # Check if validation loss has improved\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            best_epoch = epoch\n",
    "            best_model = copy.deepcopy(model.state_dict())\n",
    "            epochs_without_improvement = 0\n",
    "        else:\n",
    "            epochs_without_improvement += 1\n",
    "\n",
    "        # Early stopping\n",
    "        if epochs_without_improvement >= patience:\n",
    "            print(f'Early stopping at epoch {epoch} due to no improvement in validation loss.')\n",
    "            break\n",
    "\n",
    "        # Store losses for plotting\n",
    "        train_losses.append(loss.item())\n",
    "        val_losses.append(val_loss.item())\n",
    "\n",
    "        # Clear cache to free memory\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "        # Print progress\n",
    "        if epoch % 10 == 0:\n",
    "            print(f'Epoch {epoch}, Train Loss: {loss.item():.4f}, Val Loss: {val_loss.item():.4f}')\n",
    "            print(f'Learning rate: {optimizer.param_groups[0][\"lr\"]:.6f}')\n",
    "except KeyboardInterrupt:\n",
    "    print(\"Training interrupted by user.\")\n",
    "    \n",
    "finally:\n",
    "    print(f\"Time taken: {time.time() - start_time}\")\n",
    "    print(f'Best epoch: {best_epoch}, Best validation loss: {best_val_loss:.4f}')\n",
    "    \n",
    "    # Load the best model\n",
    "    model.load_state_dict(best_model)\n",
    "    \n",
    "    # Plot the loss values of the training set and the validation set\n",
    "    plt.plot(train_losses, label='Training Loss')\n",
    "    plt.plot(val_losses, label='Validation Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    # Test the model\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        pred_test = model(x_categ[:x_cont_test.shape[0]], x_cont_test)\n",
    "    \n",
    "    # Move the predictions back to the CPU for further processing\n",
    "    pred_test = pred_test.to(\"cpu\")\n",
    "    E_test = E_test.to(\"cpu\")\n",
    "    # Denormalize the E_tensor\n",
    "    E_test_denorm = denormalize_data(E_test, E_return, E_return2, scaling_type)\n",
    "    E_test_denorm_np = E_test_denorm.numpy()\n",
    "    # Denormalize the predictions\n",
    "    pred_test_denorm = denormalize_data(pred_test, E_return, E_return2, scaling_type)\n",
    "    pred_test_denorm_np = pred_test_denorm.numpy()\n",
    "    \n",
    "    # MAPE\n",
    "    mape = np.mean(np.abs((E_test_denorm_np - pred_test_denorm_np) / np.abs(E_test_denorm_np))) * 100\n",
    "    # MdAPE\n",
    "    mdape = np.median(np.abs((E_test_denorm_np - pred_test_denorm_np) / np.abs(E_test_denorm_np))) * 100\n",
    "    \n",
    "    # Calculate the difference\n",
    "    difference = E_test_denorm_np - pred_test_denorm_np\n",
    "    \n",
    "    \n",
    "    sample = random.randint(0,len(pred_test_denorm))\n",
    "    print(\"Difference:\\n\" + format_array(difference[sample], 4))\n",
    "    print(\"\\nE_test_denorm:\\n\" + format_array(E_test_denorm_np[sample], 4))\n",
    "    print(\"\\nPred_test_denorm:\\n\" + format_array(pred_test_denorm_np[sample], 4))\n",
    "    print(f\"\\n\\nMAPE: {mape.item():.4f}%\")\n",
    "    print(f'MdAPE: {mdape}%')\n",
    "    print(f'Best epoch: {best_epoch}')\n",
    "\n",
    "    model_name = 'warren_sp25_run15k_edit_normalizedcombinedregressionmodified.pth'\n",
    "    torch.save(model.state_dict(), model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Validation of Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-12T01:34:47.661848Z",
     "iopub.status.busy": "2025-02-12T01:34:47.661531Z",
     "iopub.status.idle": "2025-02-12T01:34:56.628235Z",
     "shell.execute_reply": "2025-02-12T01:34:56.626976Z",
     "shell.execute_reply.started": "2025-02-12T01:34:47.661826Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# 12345 or 4567 or 67\n",
    "folder_path = \"/kaggle/working/Outputs\"\n",
    "file_outputs = \"_combo_output5\"\n",
    "E_tensor, E_return, E_return2 = load_and_normalize_data_tensor(folder_path, f\"E{file_outputs}\", max_files=max_files, scaling_type=scaling_type2)\n",
    "A_tensor, _, _ = load_and_normalize_data_tensor(folder_path, f\"A{file_outputs}\", max_files=max_files, scaling_type=scaling_type2)\n",
    "J_tensor, _, _ = load_and_normalize_data_tensor(folder_path, f\"J{file_outputs}\", max_files=max_files, scaling_type=scaling_type2)\n",
    "h_tensor, _, _ = load_and_normalize_data_tensor(folder_path, f\"h{file_outputs}\", max_files=max_files, scaling_type=scaling_type2)\n",
    "X_tensor, _, _ = load_and_normalize_data_tensor(folder_path, f\"X{file_outputs}\", max_files=max_files, scaling_type=scaling_type2)\n",
    "Y_tensor, _, _ = load_and_normalize_data_tensor(folder_path, f\"Y{file_outputs}\", max_files=max_files, scaling_type=scaling_type2)\n",
    "w_tensor, _, _ = load_and_normalize_data_tensor(folder_path, f\"w{file_outputs}\", max_files=max_files, scaling_type=scaling_type2)\n",
    "T_tensor, _, _ = load_and_normalize_data_tensor(folder_path, f\"T{file_outputs}\", max_files=max_files, scaling_type=scaling_type2)\n",
    "\n",
    "n_ele = A_tensor.shape[1]\n",
    "n_ele2 = n_ele * 2\n",
    "n_ele3 = n_ele * 3\n",
    "\n",
    "# Split X_data and w_data into three separate sets of features\n",
    "X1_tensor = X_tensor[:, :n_ele].clone().detach()\n",
    "X2_tensor = X_tensor[:, n_ele:n_ele2].clone().detach()\n",
    "X3_tensor = X_tensor[:, n_ele2:n_ele3].clone().detach()\n",
    "Y1_tensor = Y_tensor[:, :n_ele].clone().detach()\n",
    "Y2_tensor = Y_tensor[:, n_ele:n_ele2].clone().detach()\n",
    "Y3_tensor = Y_tensor[:, n_ele2:n_ele3].clone().detach()\n",
    "w1_tensor = w_tensor[:, :n_ele].clone().detach()\n",
    "w2_tensor = w_tensor[:, n_ele:n_ele2].clone().detach()\n",
    "w3_tensor = w_tensor[:, n_ele2:n_ele3].clone().detach()\n",
    "\n",
    "# Extra 4th set\n",
    "X4_tensor = X_tensor[:, n_ele3:].clone().detach()\n",
    "Y4_tensor = Y_tensor[:, n_ele3:].clone().detach()\n",
    "w4_tensor = w_tensor[:, n_ele3:].clone().detach()\n",
    "\n",
    "# Concatenate continuous features\n",
    "x_cont = torch.cat((A_tensor, J_tensor, h_tensor, w1_tensor, w2_tensor, w3_tensor, w4_tensor, \n",
    "                    X1_tensor, X2_tensor, X3_tensor, X4_tensor, Y1_tensor, Y2_tensor, Y3_tensor, Y4_tensor), dim=1)\n",
    "# There are no categorical features\n",
    "x_categ = torch.empty((x_cont.shape[0], 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-12T01:34:22.673180Z",
     "iopub.status.busy": "2025-02-12T01:34:22.672839Z",
     "iopub.status.idle": "2025-02-12T01:34:23.814256Z",
     "shell.execute_reply": "2025-02-12T01:34:23.813337Z",
     "shell.execute_reply.started": "2025-02-12T01:34:22.673149Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# MODEL\n",
    "model_test = TabTransformer_edit(\n",
    "    categories=(),\n",
    "    num_continuous=x_cont.shape[1],  # continuous features\n",
    "    dim=x_cont.shape[1],             # dimension, paper set at 32\n",
    "    dim_head=16,                     # dimension of each head, paper set at 16\n",
    "    dim_out=E_tensor.shape[1],       # 15 due to the shape of E_data\n",
    "    depth=3,                         # depth, paper recommended 6 (9 worked better by .02%)\n",
    "    heads=15,                        # heads, paper recommends 8\n",
    "    attn_dropout=0.4,                # post-attention dropout\n",
    "    ff_dropout=0.4,                  # feed forward dropout\n",
    "    mlp_hidden_mults=(4,2),          # relative multiples of each hidden dimension of the last mlp to logits\n",
    "    mlp_act=nn.ReLU6(),              # activation for final mlp\n",
    "    mlp_dropout=0.05,\n",
    ")\n",
    "\n",
    "# best_model_curr = 'warren_sp25_run15k_edit_original.pth'\n",
    "# best_model_curr = 'warren_sp25_run15k_edit_combinedregression.pth'\n",
    "# best_model_curr = 'warren_sp25_run15k_edit_normalizedcombinedregression.pth'\n",
    "# best_model_curr = 'warren_sp25_run15k_edit_normalizedcombinedregressionmodified.pth'\n",
    "best_model_curr = 'warren_sp25_run15k_1234_edit_normalizedcombinedregressionmodified.pth'\n",
    "state_dict = torch.load(best_model_curr, weights_only=True)\n",
    "model_test.load_state_dict(state_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-12T01:35:00.169133Z",
     "iopub.status.busy": "2025-02-12T01:35:00.168797Z",
     "iopub.status.idle": "2025-02-12T01:35:00.591878Z",
     "shell.execute_reply": "2025-02-12T01:35:00.591055Z",
     "shell.execute_reply.started": "2025-02-12T01:35:00.169084Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Difference (sample index 123):\n",
      "[-7247936.0000, 7063648.0000, -5517392.0000, -3187296.0000, -6542144.0000, -5402432.0000, -23809568.0000, -7343616.0000, -6717584.0000, -489968.0000,\n",
      " 31625360.0000, -8648736.0000, -8266704.0000, -7353440.0000, 172624.0000, -3316096.0000, -66608.0000, 6261312.0000, 9081040.0000, -14852424.0000,\n",
      " -12877712.0000, -1454832.0000, -1009968.0000, 9637984.0000, -1950960.0000, 5732528.0000, -179360.0000, -1463936.0000, 19029152.0000, -12946968.0000,\n",
      " 15173664.0000, -2927392.0000, 8761904.0000, 1229952.0000, -662960.0000, 22476384.0000, -1349904.0000, 1647152.0000, -6178464.0000]\n",
      "\n",
      "E_all_denorm (sample index 123):\n",
      "[105000000.0000, 210000000.0000, 210000000.0000, 210000000.0000, 210000000.0000, 210000000.0000, 105000000.0000, 210000000.0000, 210000000.0000, 210000000.0000,\n",
      " 210000000.0000, 210000000.0000, 210000000.0000, 210000000.0000, 210000000.0000, 210000000.0000, 210000000.0000, 210000000.0000, 210000000.0000, 52500000.0000,\n",
      " 210000000.0000, 210000000.0000, 210000000.0000, 210000000.0000, 210000000.0000, 210000000.0000, 210000000.0000, 210000000.0000, 210000000.0000, 52500000.0000,\n",
      " 210000000.0000, 210000000.0000, 210000000.0000, 210000000.0000, 210000000.0000, 210000000.0000, 210000000.0000, 210000000.0000, 210000000.0000]\n",
      "\n",
      "Pred_all_denorm (sample index 123):\n",
      "[112247936.0000, 202936352.0000, 215517392.0000, 213187296.0000, 216542144.0000, 215402432.0000, 128809568.0000, 217343616.0000, 216717584.0000, 210489968.0000,\n",
      " 178374640.0000, 218648736.0000, 218266704.0000, 217353440.0000, 209827376.0000, 213316096.0000, 210066608.0000, 203738688.0000, 200918960.0000, 67352424.0000,\n",
      " 222877712.0000, 211454832.0000, 211009968.0000, 200362016.0000, 211950960.0000, 204267472.0000, 210179360.0000, 211463936.0000, 190970848.0000, 65446968.0000,\n",
      " 194826336.0000, 212927392.0000, 201238096.0000, 208770048.0000, 210662960.0000, 187523616.0000, 211349904.0000, 208352848.0000, 216178464.0000]\n",
      "\n",
      "\n",
      "MAPE: 8.2695%\n",
      "MdAPE: 3.9352722465991974%\n",
      "\n",
      "Element-wise minimum error percentages:\n",
      "[0.0001, 0.0000, 0.0001, 0.0005, 0.0002, 0.0007, 0.0001, 0.0000, 0.0000, 0.0001,\n",
      " 0.0001, 0.0000, 0.0003, 0.0000, 0.0005, 0.0002, 0.0002, 0.0000, 0.0000, 0.0002,\n",
      " 0.0001, 0.0001, 0.0001, 0.0000, 0.0000, 0.0001, 0.0000, 0.0002, 0.0000, 0.0003,\n",
      " 0.0000, 0.0000, 0.0001, 0.0001, 0.0000, 0.0002, 0.0000, 0.0001, 0.0000]\n",
      "\n",
      "Element-wise maximum error percentages:\n",
      "[62.9920, 97.5179, 247.7287, 204.1454, 103.8484, 126.5580, 150.4173, 217.2011, 146.9240, 164.1297,\n",
      " 271.7696, 192.5219, 169.5410, 220.8865, 210.8412, 198.8139, 157.7791, 158.6695, 234.0510, 116.8322,\n",
      " 169.8555, 177.1370, 135.3209, 110.7829, 97.0528, 139.8264, 150.7675, 129.7327, 134.2397, 116.3882,\n",
      " 131.9252, 142.4461, 134.8366, 143.9693, 184.2235, 249.4790, 180.9562, 142.8269, 141.6789]\n",
      "\n",
      "Sample with the overall lowest average error: 56756\n",
      "Average relative error for sample 56756: 3.7314%\n"
     ]
    }
   ],
   "source": [
    "# Test the model\n",
    "if not 'state_dict' in locals() or not 'model_test' in locals() or not state_dict:\n",
    "    model_test = model\n",
    "model_test.eval()\n",
    "\n",
    "# Move all the data to the device\n",
    "x_categ = x_categ.to(device)   # Categorical data tensor (empty here)\n",
    "x_cont = x_cont.to(device)\n",
    "E_tensor = E_tensor.to(device)\n",
    "model_test = model_test.to(device)\n",
    "\n",
    "# Test the model on all data\n",
    "with torch.no_grad():\n",
    "    pred_all = model_test(x_categ, x_cont)\n",
    "\n",
    "# Move predictions back to the CPU for further processing\n",
    "pred_all = pred_all.to(\"cpu\")\n",
    "E_all = E_tensor.to(\"cpu\")\n",
    "\n",
    "# Denormalize the E_tensor\n",
    "E_all_denorm = denormalize_data(E_all, E_return, E_return2, scaling_type)\n",
    "E_all_denorm_np = E_all_denorm.numpy()\n",
    "\n",
    "# Denormalize the predictions\n",
    "pred_all_denorm = denormalize_data(pred_all, E_return, E_return2, scaling_type)\n",
    "pred_all_denorm_np = pred_all_denorm.numpy()\n",
    "\n",
    "# Calculate error metrics (MAPE and MdAPE)\n",
    "mape = np.mean(np.abs((E_all_denorm_np - pred_all_denorm_np) / np.abs(E_all_denorm_np))) * 100\n",
    "mdape = np.median(np.abs((E_all_denorm_np - pred_all_denorm_np) / np.abs(E_all_denorm_np))) * 100\n",
    "\n",
    "# Calculate the difference between the predictions and the ground truth for each sample/element\n",
    "difference = E_all_denorm_np - pred_all_denorm_np\n",
    "\n",
    "# For demonstration, select a specific sample to inspect in detail\n",
    "sample = 123\n",
    "print(\"Difference (sample index {}):\\n{}\".format(sample, format_array(difference[sample], 4)))\n",
    "print(\"\\nE_all_denorm (sample index {}):\\n{}\".format(sample, format_array(E_all_denorm_np[sample], 4)))\n",
    "print(\"\\nPred_all_denorm (sample index {}):\\n{}\".format(sample, format_array(pred_all_denorm_np[sample], 4)))\n",
    "print(f\"\\n\\nMAPE: {mape.item():.4f}%\")\n",
    "print(f\"MdAPE: {mdape}%\")\n",
    "\n",
    "# -------------------------------\n",
    "# Compute element-wise minimum and maximum error percentages\n",
    "\n",
    "# Compute the relative error (in percentage) for all samples and all elements\n",
    "relative_error = np.abs((E_all_denorm_np - pred_all_denorm_np) / np.abs(E_all_denorm_np)) * 100\n",
    "\n",
    "# For each element (i.e. for each column), compute the minimum and maximum error percentage across all samples\n",
    "min_error = np.min(relative_error, axis=0)\n",
    "max_error = np.max(relative_error, axis=0)\n",
    "\n",
    "print(\"\\nElement-wise minimum error percentages:\")\n",
    "print(format_array(min_error, 4))\n",
    "\n",
    "print(\"\\nElement-wise maximum error percentages:\")\n",
    "print(format_array(max_error, 4))\n",
    "\n",
    "# -------------------------------\n",
    "# Compute the overall error for each sample by averaging the element-wise relative errors\n",
    "overall_error = np.mean(relative_error, axis=1)\n",
    "\n",
    "# Find the sample index with the lowest overall error\n",
    "best_sample = np.argmin(overall_error)\n",
    "best_sample_error = overall_error[best_sample]\n",
    "\n",
    "print(f\"\\nSample with the overall lowest average error: {best_sample}\")\n",
    "print(f\"Average relative error for sample {best_sample}: {best_sample_error:.4f}%\")\n",
    "\n",
    "# # Optionally, print detailed information for the best sample\n",
    "# print(\"\\nDetailed information for the best sample:\")\n",
    "# print(\"Difference:\\n\" + format_array(difference[best_sample], 4))\n",
    "# print(\"\\nE_all_denorm:\\n\" + format_array(E_all_denorm_np[best_sample], 4))\n",
    "# print(\"\\nPred_all_denorm:\\n\" + format_array(pred_all_denorm_np[best_sample], 4))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-11T21:31:51.211200Z",
     "iopub.status.busy": "2025-02-11T21:31:51.210867Z",
     "iopub.status.idle": "2025-02-11T21:31:51.501051Z",
     "shell.execute_reply": "2025-02-11T21:31:51.500153Z",
     "shell.execute_reply.started": "2025-02-11T21:31:51.211173Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 900x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Define thresholds for different colors\n",
    "threshold_yellow = 0.1  # 10%\n",
    "threshold_red = 0.3     # 30%\n",
    "\n",
    "# Calculate the difference for all samples using the updated variable names\n",
    "difference = E_all_denorm_np - pred_all_denorm_np\n",
    "\n",
    "# Define a sample index to demonstrate the color change (ensuring a valid index)\n",
    "sample = 51745\n",
    "# sample = random.randint(0, len(difference) - 1)\n",
    "\n",
    "# Plot each element using its angle beta\n",
    "plt.figure(figsize=(9, 6))\n",
    "for i in range(len(ele_nod)):\n",
    "    x1, y1 = X[ele_nod[i, 0]], Y[ele_nod[i, 0]]  # Start node\n",
    "    length = h[i]\n",
    "    # Calculate end point based on length and angle beta\n",
    "    x2 = x1 + length * np.cos(beta[i])\n",
    "    y2 = y1 + length * np.sin(beta[i])\n",
    "    \n",
    "    # Determine color based on the relative difference\n",
    "    relative_diff = np.abs(difference[sample, i] / E_all_denorm_np[sample, i])\n",
    "    if relative_diff >= threshold_red:\n",
    "        color = 'red'    # Red for significant difference (>= 30%)\n",
    "    elif relative_diff >= threshold_yellow:\n",
    "        color = 'yellow' # Yellow for moderate difference (10% - 30%)\n",
    "    else:\n",
    "        color = 'green'  # Green for small difference (< 10%)\n",
    "    \n",
    "    plt.plot([x1, x2], [y1, y2], color=color, linewidth=2)\n",
    "    plt.text((x1 + x2) / 2, (y1 + y2) / 2, f\"{i}\\n{relative_diff:.1%}\", \n",
    "             ha='center', va='center', fontsize=8)\n",
    "\n",
    "plt.plot(X, Y, 'ko', markersize=5)  # Plot nodes\n",
    "plt.axis('equal')\n",
    "plt.title(f'Relative Difference of Prediction and Actual Results (Sample {sample})')\n",
    "plt.xlabel('X')\n",
    "plt.ylabel('Y')\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [],
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "37da007d8f0643a193d2fbd398f270d6": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "47ab5ac8b8274450b5d17dd9029dde86": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "6e918d17bbf845d78c04e3f133a304ea": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_a57e53317ddf4544bb639702360ba1fa",
      "placeholder": "",
      "style": "IPY_MODEL_37da007d8f0643a193d2fbd398f270d6",
      "value": ""
     }
    },
    "7c2401086fad4c4fbd20faec97cb21e1": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_ed952f366ad64c578b80ea79f5872096",
      "max": 1,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_b84682b5a9ff4f5aa70b5eef29d1abf5",
      "value": 0
     }
    },
    "a039088fc3f0454f8cef88a3906da568": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_ea9955a73c3f4c0e9bf3637ce2c599fe",
      "placeholder": "",
      "style": "IPY_MODEL_47ab5ac8b8274450b5d17dd9029dde86",
      "value": "0/0[00:00&lt;?,?it/s]"
     }
    },
    "a57e53317ddf4544bb639702360ba1fa": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "a66a273c0b5e45e389f74d024c56f3d3": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "b84682b5a9ff4f5aa70b5eef29d1abf5": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "d74bbe69523d479b845b3d5eff6f81bd": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_6e918d17bbf845d78c04e3f133a304ea",
       "IPY_MODEL_7c2401086fad4c4fbd20faec97cb21e1",
       "IPY_MODEL_a039088fc3f0454f8cef88a3906da568"
      ],
      "layout": "IPY_MODEL_a66a273c0b5e45e389f74d024c56f3d3"
     }
    },
    "ea9955a73c3f4c0e9bf3637ce2c599fe": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "ed952f366ad64c578b80ea79f5872096": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": "20px"
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
